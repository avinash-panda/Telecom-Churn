{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb3fsqEN14w8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "### Business problem overview\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition. \n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the top business goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    "In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn. \n",
    "\n",
    "### Data Description\n",
    "\n",
    "There are several types of data that is collected from customers by a telecomminucation service provider. Some of the information that you have to look for data analysis and EDA is given below:\n",
    "- Recharging of the service: There are several variables that describe the duration, maximum, total amount and average of the recharge price of the service they avail, which include the 2G service, the 3G service, internet packages and call services\n",
    "  - av_rech_amt_data: Average recharge data amount\n",
    "  - count_rech_2g: Number of customers using 2G\n",
    "  - count_rech_3g: Number of customers using 3G\n",
    "  - max_rech_data: Maximum recharge for mobile internet\n",
    "  - total_rech_data: Total recharge for mobile internet\n",
    "  - max_rech_amt: Maximum recharge amount\n",
    "  - total_rech_amt: Total recharge amount\n",
    "  - total_rech_num: Total number of times customer recharged\n",
    "\n",
    "If there are missing values in the columns corresponding to these variables, this is because the customer did not recharge that month.\n",
    "\n",
    "- Call and Internet service: They specify the amount of calls, type of calling service used (STD, ISD, Roaming), type of internet service and amount of internet usage over a specific period of time\n",
    "  - total_calls_mou: Total minutes of voice calls\n",
    "  - total_internet_mb: Total amount of internet usage in MB\n",
    "  - arpu: Average revenue per usage\n",
    "  - onnet_mou: The minutes of usage for all kind of calls within the same operator network\n",
    "  - offnet_mou: The minutes of usage for all kind of calls outside the operator T network\n",
    "  - Minutes of usage for outgoing calls for each type of call service:\n",
    "    - loc_og_mou\n",
    "    - std_og_mou\n",
    "    - isd_og_mou\n",
    "    - spl_og_mou\n",
    "    - roam_og_mou\n",
    "    - total_og_mou\n",
    "  - Minutes of usage for incoming calls for each type of call service:\n",
    "    - loc_ic_mou\n",
    "    - std_ic_mou\n",
    "    - isd_ic_mou\n",
    "    - spl_ic_mou\n",
    "    - roam_ic_mou\n",
    "    - total_ic_mou\n",
    "  - total_rech_num: Total number of recharge\n",
    "  - total_rech_amt: Total amount of recharge\n",
    "  - max_rech_amt: Maximum recharge amount\n",
    "  - total_rech_data: Total recharge for mobile internet\n",
    "  - max_rech_data: Maximum recharge for mobile internet\n",
    "  - av_rech_amt_data: Average recharge amount for mobile internet\n",
    "  - vol_2g_mb: Mobile internet usage volumn for 2G\n",
    "  - vol_3g_mb: Mobile internet usage volumn for 3G\n",
    "\n",
    "If the columns corresponding to some of these variables that have more than 70% of missing values, you can drop those variables from the data set. If not, then you can use the MICE technique to impute the values in those missing entries.\n",
    "\n",
    "The categorical variables present in the data set are given below:\n",
    "  - night_pck_user: Prepaid service schemes for use during specific night hours only\n",
    "  - fb_user: Service scheme to avail services of Facebook and similar social networking sites\n",
    "\n",
    "If there are missing values, this means that there is another scheme that the customer has availed from the telecomminucation service.\n",
    "\n",
    "Most of the variables have their values recorded for 4 different months. The variable names end with the month number as explained below:\n",
    "- *.6: KPI for the month of June\n",
    "- *.7: KPI for the month of July\n",
    "- *.8: KPI for the month of August\n",
    "- *.9: KPI for the month of September\n",
    "\n",
    "The rest of variables have been defined in the detailed data description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc7Vumkt14xA"
   },
   "source": [
    "# Task 1: Importing the required libraries and loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FATks0vWd81X"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K1YfGJ8qd81a"
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "churn = pd.read_csv(\"telecom_churn_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtbG26H714xD"
   },
   "source": [
    "# Task 2: Understanding and exploring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kMmULcO3d81b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>circle_id</th>\n",
       "      <th>loc_og_t2o_mou</th>\n",
       "      <th>std_og_t2o_mou</th>\n",
       "      <th>loc_ic_t2o_mou</th>\n",
       "      <th>last_date_of_month_6</th>\n",
       "      <th>last_date_of_month_7</th>\n",
       "      <th>last_date_of_month_8</th>\n",
       "      <th>last_date_of_month_9</th>\n",
       "      <th>arpu_6</th>\n",
       "      <th>arpu_7</th>\n",
       "      <th>arpu_8</th>\n",
       "      <th>arpu_9</th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>onnet_mou_9</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>offnet_mou_9</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>roam_ic_mou_9</th>\n",
       "      <th>roam_og_mou_6</th>\n",
       "      <th>roam_og_mou_7</th>\n",
       "      <th>roam_og_mou_8</th>\n",
       "      <th>roam_og_mou_9</th>\n",
       "      <th>loc_og_t2t_mou_6</th>\n",
       "      <th>loc_og_t2t_mou_7</th>\n",
       "      <th>loc_og_t2t_mou_8</th>\n",
       "      <th>loc_og_t2t_mou_9</th>\n",
       "      <th>loc_og_t2m_mou_6</th>\n",
       "      <th>loc_og_t2m_mou_7</th>\n",
       "      <th>loc_og_t2m_mou_8</th>\n",
       "      <th>loc_og_t2m_mou_9</th>\n",
       "      <th>loc_og_t2f_mou_6</th>\n",
       "      <th>loc_og_t2f_mou_7</th>\n",
       "      <th>loc_og_t2f_mou_8</th>\n",
       "      <th>loc_og_t2f_mou_9</th>\n",
       "      <th>loc_og_t2c_mou_6</th>\n",
       "      <th>loc_og_t2c_mou_7</th>\n",
       "      <th>loc_og_t2c_mou_8</th>\n",
       "      <th>loc_og_t2c_mou_9</th>\n",
       "      <th>loc_og_mou_6</th>\n",
       "      <th>loc_og_mou_7</th>\n",
       "      <th>loc_og_mou_8</th>\n",
       "      <th>loc_og_mou_9</th>\n",
       "      <th>std_og_t2t_mou_6</th>\n",
       "      <th>std_og_t2t_mou_7</th>\n",
       "      <th>std_og_t2t_mou_8</th>\n",
       "      <th>std_og_t2t_mou_9</th>\n",
       "      <th>std_og_t2m_mou_6</th>\n",
       "      <th>std_og_t2m_mou_7</th>\n",
       "      <th>std_og_t2m_mou_8</th>\n",
       "      <th>std_og_t2m_mou_9</th>\n",
       "      <th>std_og_t2f_mou_6</th>\n",
       "      <th>std_og_t2f_mou_7</th>\n",
       "      <th>std_og_t2f_mou_8</th>\n",
       "      <th>std_og_t2f_mou_9</th>\n",
       "      <th>std_og_t2c_mou_6</th>\n",
       "      <th>std_og_t2c_mou_7</th>\n",
       "      <th>std_og_t2c_mou_8</th>\n",
       "      <th>std_og_t2c_mou_9</th>\n",
       "      <th>std_og_mou_6</th>\n",
       "      <th>std_og_mou_7</th>\n",
       "      <th>std_og_mou_8</th>\n",
       "      <th>std_og_mou_9</th>\n",
       "      <th>isd_og_mou_6</th>\n",
       "      <th>isd_og_mou_7</th>\n",
       "      <th>isd_og_mou_8</th>\n",
       "      <th>isd_og_mou_9</th>\n",
       "      <th>spl_og_mou_6</th>\n",
       "      <th>spl_og_mou_7</th>\n",
       "      <th>spl_og_mou_8</th>\n",
       "      <th>spl_og_mou_9</th>\n",
       "      <th>og_others_6</th>\n",
       "      <th>og_others_7</th>\n",
       "      <th>og_others_8</th>\n",
       "      <th>og_others_9</th>\n",
       "      <th>total_og_mou_6</th>\n",
       "      <th>total_og_mou_7</th>\n",
       "      <th>total_og_mou_8</th>\n",
       "      <th>total_og_mou_9</th>\n",
       "      <th>loc_ic_t2t_mou_6</th>\n",
       "      <th>loc_ic_t2t_mou_7</th>\n",
       "      <th>loc_ic_t2t_mou_8</th>\n",
       "      <th>loc_ic_t2t_mou_9</th>\n",
       "      <th>loc_ic_t2m_mou_6</th>\n",
       "      <th>loc_ic_t2m_mou_7</th>\n",
       "      <th>loc_ic_t2m_mou_8</th>\n",
       "      <th>loc_ic_t2m_mou_9</th>\n",
       "      <th>loc_ic_t2f_mou_6</th>\n",
       "      <th>loc_ic_t2f_mou_7</th>\n",
       "      <th>loc_ic_t2f_mou_8</th>\n",
       "      <th>loc_ic_t2f_mou_9</th>\n",
       "      <th>loc_ic_mou_6</th>\n",
       "      <th>loc_ic_mou_7</th>\n",
       "      <th>loc_ic_mou_8</th>\n",
       "      <th>loc_ic_mou_9</th>\n",
       "      <th>std_ic_t2t_mou_6</th>\n",
       "      <th>std_ic_t2t_mou_7</th>\n",
       "      <th>std_ic_t2t_mou_8</th>\n",
       "      <th>std_ic_t2t_mou_9</th>\n",
       "      <th>std_ic_t2m_mou_6</th>\n",
       "      <th>std_ic_t2m_mou_7</th>\n",
       "      <th>std_ic_t2m_mou_8</th>\n",
       "      <th>std_ic_t2m_mou_9</th>\n",
       "      <th>std_ic_t2f_mou_6</th>\n",
       "      <th>std_ic_t2f_mou_7</th>\n",
       "      <th>std_ic_t2f_mou_8</th>\n",
       "      <th>std_ic_t2f_mou_9</th>\n",
       "      <th>std_ic_t2o_mou_6</th>\n",
       "      <th>std_ic_t2o_mou_7</th>\n",
       "      <th>std_ic_t2o_mou_8</th>\n",
       "      <th>std_ic_t2o_mou_9</th>\n",
       "      <th>std_ic_mou_6</th>\n",
       "      <th>std_ic_mou_7</th>\n",
       "      <th>std_ic_mou_8</th>\n",
       "      <th>std_ic_mou_9</th>\n",
       "      <th>total_ic_mou_6</th>\n",
       "      <th>total_ic_mou_7</th>\n",
       "      <th>total_ic_mou_8</th>\n",
       "      <th>total_ic_mou_9</th>\n",
       "      <th>spl_ic_mou_6</th>\n",
       "      <th>spl_ic_mou_7</th>\n",
       "      <th>spl_ic_mou_8</th>\n",
       "      <th>spl_ic_mou_9</th>\n",
       "      <th>isd_ic_mou_6</th>\n",
       "      <th>isd_ic_mou_7</th>\n",
       "      <th>isd_ic_mou_8</th>\n",
       "      <th>isd_ic_mou_9</th>\n",
       "      <th>ic_others_6</th>\n",
       "      <th>ic_others_7</th>\n",
       "      <th>ic_others_8</th>\n",
       "      <th>ic_others_9</th>\n",
       "      <th>total_rech_num_6</th>\n",
       "      <th>total_rech_num_7</th>\n",
       "      <th>total_rech_num_8</th>\n",
       "      <th>total_rech_num_9</th>\n",
       "      <th>total_rech_amt_6</th>\n",
       "      <th>total_rech_amt_7</th>\n",
       "      <th>total_rech_amt_8</th>\n",
       "      <th>total_rech_amt_9</th>\n",
       "      <th>max_rech_amt_6</th>\n",
       "      <th>max_rech_amt_7</th>\n",
       "      <th>max_rech_amt_8</th>\n",
       "      <th>max_rech_amt_9</th>\n",
       "      <th>date_of_last_rech_6</th>\n",
       "      <th>date_of_last_rech_7</th>\n",
       "      <th>date_of_last_rech_8</th>\n",
       "      <th>date_of_last_rech_9</th>\n",
       "      <th>last_day_rch_amt_6</th>\n",
       "      <th>last_day_rch_amt_7</th>\n",
       "      <th>last_day_rch_amt_8</th>\n",
       "      <th>last_day_rch_amt_9</th>\n",
       "      <th>date_of_last_rech_data_6</th>\n",
       "      <th>date_of_last_rech_data_7</th>\n",
       "      <th>date_of_last_rech_data_8</th>\n",
       "      <th>date_of_last_rech_data_9</th>\n",
       "      <th>total_rech_data_6</th>\n",
       "      <th>total_rech_data_7</th>\n",
       "      <th>total_rech_data_8</th>\n",
       "      <th>total_rech_data_9</th>\n",
       "      <th>max_rech_data_6</th>\n",
       "      <th>max_rech_data_7</th>\n",
       "      <th>max_rech_data_8</th>\n",
       "      <th>max_rech_data_9</th>\n",
       "      <th>count_rech_2g_6</th>\n",
       "      <th>count_rech_2g_7</th>\n",
       "      <th>count_rech_2g_8</th>\n",
       "      <th>count_rech_2g_9</th>\n",
       "      <th>count_rech_3g_6</th>\n",
       "      <th>count_rech_3g_7</th>\n",
       "      <th>count_rech_3g_8</th>\n",
       "      <th>count_rech_3g_9</th>\n",
       "      <th>av_rech_amt_data_6</th>\n",
       "      <th>av_rech_amt_data_7</th>\n",
       "      <th>av_rech_amt_data_8</th>\n",
       "      <th>av_rech_amt_data_9</th>\n",
       "      <th>vol_2g_mb_6</th>\n",
       "      <th>vol_2g_mb_7</th>\n",
       "      <th>vol_2g_mb_8</th>\n",
       "      <th>vol_2g_mb_9</th>\n",
       "      <th>vol_3g_mb_6</th>\n",
       "      <th>vol_3g_mb_7</th>\n",
       "      <th>vol_3g_mb_8</th>\n",
       "      <th>vol_3g_mb_9</th>\n",
       "      <th>arpu_3g_6</th>\n",
       "      <th>arpu_3g_7</th>\n",
       "      <th>arpu_3g_8</th>\n",
       "      <th>arpu_3g_9</th>\n",
       "      <th>arpu_2g_6</th>\n",
       "      <th>arpu_2g_7</th>\n",
       "      <th>arpu_2g_8</th>\n",
       "      <th>arpu_2g_9</th>\n",
       "      <th>night_pck_user_6</th>\n",
       "      <th>night_pck_user_7</th>\n",
       "      <th>night_pck_user_8</th>\n",
       "      <th>night_pck_user_9</th>\n",
       "      <th>monthly_2g_6</th>\n",
       "      <th>monthly_2g_7</th>\n",
       "      <th>monthly_2g_8</th>\n",
       "      <th>monthly_2g_9</th>\n",
       "      <th>sachet_2g_6</th>\n",
       "      <th>sachet_2g_7</th>\n",
       "      <th>sachet_2g_8</th>\n",
       "      <th>sachet_2g_9</th>\n",
       "      <th>monthly_3g_6</th>\n",
       "      <th>monthly_3g_7</th>\n",
       "      <th>monthly_3g_8</th>\n",
       "      <th>monthly_3g_9</th>\n",
       "      <th>sachet_3g_6</th>\n",
       "      <th>sachet_3g_7</th>\n",
       "      <th>sachet_3g_8</th>\n",
       "      <th>sachet_3g_9</th>\n",
       "      <th>fb_user_6</th>\n",
       "      <th>fb_user_7</th>\n",
       "      <th>fb_user_8</th>\n",
       "      <th>fb_user_9</th>\n",
       "      <th>aon</th>\n",
       "      <th>aug_vbc_3g</th>\n",
       "      <th>jul_vbc_3g</th>\n",
       "      <th>jun_vbc_3g</th>\n",
       "      <th>sep_vbc_3g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>197.385</td>\n",
       "      <td>214.816</td>\n",
       "      <td>213.803</td>\n",
       "      <td>21.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>6/21/2014</td>\n",
       "      <td>7/16/2014</td>\n",
       "      <td>08-08-2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>6/21/2014</td>\n",
       "      <td>7/16/2014</td>\n",
       "      <td>08-08-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.13</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.57</td>\n",
       "      <td>150.76</td>\n",
       "      <td>109.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>968</td>\n",
       "      <td>30.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.20</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>34.047</td>\n",
       "      <td>355.074</td>\n",
       "      <td>268.321</td>\n",
       "      <td>86.285</td>\n",
       "      <td>24.11</td>\n",
       "      <td>78.68</td>\n",
       "      <td>7.68</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.74</td>\n",
       "      <td>99.84</td>\n",
       "      <td>304.76</td>\n",
       "      <td>53.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.88</td>\n",
       "      <td>74.56</td>\n",
       "      <td>7.68</td>\n",
       "      <td>18.34</td>\n",
       "      <td>11.51</td>\n",
       "      <td>75.94</td>\n",
       "      <td>291.86</td>\n",
       "      <td>53.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.39</td>\n",
       "      <td>150.51</td>\n",
       "      <td>299.54</td>\n",
       "      <td>72.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>23.43</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.31</td>\n",
       "      <td>178.53</td>\n",
       "      <td>312.44</td>\n",
       "      <td>72.11</td>\n",
       "      <td>1.61</td>\n",
       "      <td>29.91</td>\n",
       "      <td>29.23</td>\n",
       "      <td>116.09</td>\n",
       "      <td>17.48</td>\n",
       "      <td>65.38</td>\n",
       "      <td>375.58</td>\n",
       "      <td>56.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.93</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.09</td>\n",
       "      <td>104.23</td>\n",
       "      <td>408.43</td>\n",
       "      <td>173.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.49</td>\n",
       "      <td>15.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.84</td>\n",
       "      <td>15.01</td>\n",
       "      <td>26.83</td>\n",
       "      <td>104.23</td>\n",
       "      <td>423.28</td>\n",
       "      <td>188.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>384</td>\n",
       "      <td>283</td>\n",
       "      <td>121</td>\n",
       "      <td>44</td>\n",
       "      <td>154</td>\n",
       "      <td>65</td>\n",
       "      <td>50</td>\n",
       "      <td>6/29/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/28/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/25/2014</td>\n",
       "      <td>08-10-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.07</td>\n",
       "      <td>365.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.61</td>\n",
       "      <td>7.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>167.690</td>\n",
       "      <td>189.058</td>\n",
       "      <td>210.226</td>\n",
       "      <td>290.714</td>\n",
       "      <td>11.54</td>\n",
       "      <td>55.24</td>\n",
       "      <td>37.26</td>\n",
       "      <td>74.81</td>\n",
       "      <td>143.33</td>\n",
       "      <td>220.59</td>\n",
       "      <td>208.36</td>\n",
       "      <td>118.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.94</td>\n",
       "      <td>7.19</td>\n",
       "      <td>28.74</td>\n",
       "      <td>13.58</td>\n",
       "      <td>14.39</td>\n",
       "      <td>29.34</td>\n",
       "      <td>16.86</td>\n",
       "      <td>38.46</td>\n",
       "      <td>28.16</td>\n",
       "      <td>24.11</td>\n",
       "      <td>21.79</td>\n",
       "      <td>15.61</td>\n",
       "      <td>22.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>135.54</td>\n",
       "      <td>45.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>60.66</td>\n",
       "      <td>67.41</td>\n",
       "      <td>67.66</td>\n",
       "      <td>64.81</td>\n",
       "      <td>4.34</td>\n",
       "      <td>26.49</td>\n",
       "      <td>22.58</td>\n",
       "      <td>8.76</td>\n",
       "      <td>41.81</td>\n",
       "      <td>67.41</td>\n",
       "      <td>75.53</td>\n",
       "      <td>9.28</td>\n",
       "      <td>1.48</td>\n",
       "      <td>14.76</td>\n",
       "      <td>22.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.64</td>\n",
       "      <td>108.68</td>\n",
       "      <td>120.94</td>\n",
       "      <td>18.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.56</td>\n",
       "      <td>236.84</td>\n",
       "      <td>96.84</td>\n",
       "      <td>42.08</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>155.33</td>\n",
       "      <td>412.94</td>\n",
       "      <td>285.46</td>\n",
       "      <td>124.94</td>\n",
       "      <td>115.69</td>\n",
       "      <td>71.11</td>\n",
       "      <td>67.46</td>\n",
       "      <td>148.23</td>\n",
       "      <td>14.38</td>\n",
       "      <td>15.44</td>\n",
       "      <td>38.89</td>\n",
       "      <td>38.98</td>\n",
       "      <td>99.48</td>\n",
       "      <td>122.29</td>\n",
       "      <td>49.63</td>\n",
       "      <td>158.19</td>\n",
       "      <td>229.56</td>\n",
       "      <td>208.86</td>\n",
       "      <td>155.99</td>\n",
       "      <td>345.41</td>\n",
       "      <td>72.41</td>\n",
       "      <td>71.29</td>\n",
       "      <td>28.69</td>\n",
       "      <td>49.44</td>\n",
       "      <td>45.18</td>\n",
       "      <td>177.01</td>\n",
       "      <td>167.09</td>\n",
       "      <td>118.18</td>\n",
       "      <td>21.73</td>\n",
       "      <td>58.34</td>\n",
       "      <td>43.23</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.33</td>\n",
       "      <td>306.66</td>\n",
       "      <td>239.03</td>\n",
       "      <td>171.49</td>\n",
       "      <td>370.04</td>\n",
       "      <td>519.53</td>\n",
       "      <td>395.03</td>\n",
       "      <td>517.74</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>315</td>\n",
       "      <td>116</td>\n",
       "      <td>358</td>\n",
       "      <td>86</td>\n",
       "      <td>200</td>\n",
       "      <td>86</td>\n",
       "      <td>100</td>\n",
       "      <td>6/17/2014</td>\n",
       "      <td>7/24/2014</td>\n",
       "      <td>8/14/2014</td>\n",
       "      <td>9/29/2014</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/17/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1103</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>221.338</td>\n",
       "      <td>251.102</td>\n",
       "      <td>508.054</td>\n",
       "      <td>389.500</td>\n",
       "      <td>99.91</td>\n",
       "      <td>54.39</td>\n",
       "      <td>310.98</td>\n",
       "      <td>241.71</td>\n",
       "      <td>123.31</td>\n",
       "      <td>109.01</td>\n",
       "      <td>71.68</td>\n",
       "      <td>113.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.86</td>\n",
       "      <td>44.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.09</td>\n",
       "      <td>39.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73.68</td>\n",
       "      <td>34.81</td>\n",
       "      <td>10.61</td>\n",
       "      <td>15.49</td>\n",
       "      <td>107.43</td>\n",
       "      <td>83.21</td>\n",
       "      <td>22.46</td>\n",
       "      <td>65.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.91</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.03</td>\n",
       "      <td>118.68</td>\n",
       "      <td>37.99</td>\n",
       "      <td>83.03</td>\n",
       "      <td>26.23</td>\n",
       "      <td>14.89</td>\n",
       "      <td>289.58</td>\n",
       "      <td>226.21</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.73</td>\n",
       "      <td>6.53</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.23</td>\n",
       "      <td>16.63</td>\n",
       "      <td>296.11</td>\n",
       "      <td>236.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.09</td>\n",
       "      <td>43.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>223.23</td>\n",
       "      <td>135.31</td>\n",
       "      <td>352.21</td>\n",
       "      <td>362.54</td>\n",
       "      <td>62.08</td>\n",
       "      <td>19.98</td>\n",
       "      <td>8.04</td>\n",
       "      <td>41.73</td>\n",
       "      <td>113.96</td>\n",
       "      <td>64.51</td>\n",
       "      <td>20.28</td>\n",
       "      <td>52.86</td>\n",
       "      <td>57.43</td>\n",
       "      <td>27.09</td>\n",
       "      <td>19.84</td>\n",
       "      <td>65.59</td>\n",
       "      <td>233.48</td>\n",
       "      <td>111.59</td>\n",
       "      <td>48.18</td>\n",
       "      <td>160.19</td>\n",
       "      <td>43.48</td>\n",
       "      <td>66.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>129.84</td>\n",
       "      <td>1.33</td>\n",
       "      <td>38.56</td>\n",
       "      <td>4.94</td>\n",
       "      <td>13.98</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.99</td>\n",
       "      <td>105.01</td>\n",
       "      <td>4.94</td>\n",
       "      <td>143.83</td>\n",
       "      <td>280.08</td>\n",
       "      <td>216.61</td>\n",
       "      <td>53.13</td>\n",
       "      <td>305.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>230</td>\n",
       "      <td>310</td>\n",
       "      <td>601</td>\n",
       "      <td>410</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>6/28/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2491</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>261.636</td>\n",
       "      <td>309.876</td>\n",
       "      <td>238.174</td>\n",
       "      <td>163.426</td>\n",
       "      <td>50.31</td>\n",
       "      <td>149.44</td>\n",
       "      <td>83.89</td>\n",
       "      <td>58.78</td>\n",
       "      <td>76.96</td>\n",
       "      <td>91.88</td>\n",
       "      <td>124.26</td>\n",
       "      <td>45.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.31</td>\n",
       "      <td>149.44</td>\n",
       "      <td>83.89</td>\n",
       "      <td>58.78</td>\n",
       "      <td>67.64</td>\n",
       "      <td>91.88</td>\n",
       "      <td>124.26</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>117.96</td>\n",
       "      <td>241.33</td>\n",
       "      <td>208.16</td>\n",
       "      <td>98.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>127.28</td>\n",
       "      <td>241.33</td>\n",
       "      <td>208.16</td>\n",
       "      <td>104.59</td>\n",
       "      <td>105.68</td>\n",
       "      <td>88.49</td>\n",
       "      <td>233.81</td>\n",
       "      <td>154.56</td>\n",
       "      <td>106.84</td>\n",
       "      <td>109.54</td>\n",
       "      <td>104.13</td>\n",
       "      <td>48.24</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>214.03</td>\n",
       "      <td>198.04</td>\n",
       "      <td>337.94</td>\n",
       "      <td>202.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.31</td>\n",
       "      <td>216.44</td>\n",
       "      <td>198.29</td>\n",
       "      <td>338.81</td>\n",
       "      <td>205.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>196</td>\n",
       "      <td>350</td>\n",
       "      <td>287</td>\n",
       "      <td>200</td>\n",
       "      <td>56</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>6/26/2014</td>\n",
       "      <td>7/28/2014</td>\n",
       "      <td>08-09-2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>50</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>06-04-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>50.258</td>\n",
       "      <td>58.810</td>\n",
       "      <td>83.386</td>\n",
       "      <td>170.826</td>\n",
       "      <td>50.16</td>\n",
       "      <td>43.63</td>\n",
       "      <td>85.48</td>\n",
       "      <td>138.79</td>\n",
       "      <td>19.28</td>\n",
       "      <td>13.44</td>\n",
       "      <td>14.46</td>\n",
       "      <td>46.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.16</td>\n",
       "      <td>43.63</td>\n",
       "      <td>85.48</td>\n",
       "      <td>138.79</td>\n",
       "      <td>16.39</td>\n",
       "      <td>8.83</td>\n",
       "      <td>12.38</td>\n",
       "      <td>44.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.56</td>\n",
       "      <td>52.46</td>\n",
       "      <td>97.86</td>\n",
       "      <td>185.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.88</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.88</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>69.44</td>\n",
       "      <td>57.08</td>\n",
       "      <td>99.94</td>\n",
       "      <td>185.71</td>\n",
       "      <td>28.73</td>\n",
       "      <td>30.03</td>\n",
       "      <td>56.26</td>\n",
       "      <td>68.38</td>\n",
       "      <td>49.19</td>\n",
       "      <td>57.44</td>\n",
       "      <td>62.46</td>\n",
       "      <td>84.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.93</td>\n",
       "      <td>87.48</td>\n",
       "      <td>118.73</td>\n",
       "      <td>152.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.03</td>\n",
       "      <td>71.06</td>\n",
       "      <td>37.93</td>\n",
       "      <td>52.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.03</td>\n",
       "      <td>71.06</td>\n",
       "      <td>37.93</td>\n",
       "      <td>52.03</td>\n",
       "      <td>155.39</td>\n",
       "      <td>158.76</td>\n",
       "      <td>157.13</td>\n",
       "      <td>205.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>6/19/2014</td>\n",
       "      <td>7/17/2014</td>\n",
       "      <td>8/24/2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1471</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>429.023</td>\n",
       "      <td>190.704</td>\n",
       "      <td>255.114</td>\n",
       "      <td>114.751</td>\n",
       "      <td>71.03</td>\n",
       "      <td>45.03</td>\n",
       "      <td>76.66</td>\n",
       "      <td>15.23</td>\n",
       "      <td>262.73</td>\n",
       "      <td>49.24</td>\n",
       "      <td>92.08</td>\n",
       "      <td>50.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.03</td>\n",
       "      <td>45.03</td>\n",
       "      <td>76.14</td>\n",
       "      <td>15.23</td>\n",
       "      <td>252.23</td>\n",
       "      <td>48.71</td>\n",
       "      <td>80.63</td>\n",
       "      <td>50.33</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>333.64</td>\n",
       "      <td>93.74</td>\n",
       "      <td>156.78</td>\n",
       "      <td>65.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>333.76</td>\n",
       "      <td>94.81</td>\n",
       "      <td>168.74</td>\n",
       "      <td>65.91</td>\n",
       "      <td>1857.99</td>\n",
       "      <td>1427.04</td>\n",
       "      <td>1896.43</td>\n",
       "      <td>2334.88</td>\n",
       "      <td>248.64</td>\n",
       "      <td>336.96</td>\n",
       "      <td>265.28</td>\n",
       "      <td>231.41</td>\n",
       "      <td>20.24</td>\n",
       "      <td>22.69</td>\n",
       "      <td>2.51</td>\n",
       "      <td>6.19</td>\n",
       "      <td>2126.89</td>\n",
       "      <td>1786.71</td>\n",
       "      <td>2164.23</td>\n",
       "      <td>2572.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2128.41</td>\n",
       "      <td>1788.06</td>\n",
       "      <td>2167.11</td>\n",
       "      <td>2572.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>499</td>\n",
       "      <td>222</td>\n",
       "      <td>294</td>\n",
       "      <td>141</td>\n",
       "      <td>90</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>6/28/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/28/2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1673</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>1069.180</td>\n",
       "      <td>1349.850</td>\n",
       "      <td>3171.480</td>\n",
       "      <td>500.000</td>\n",
       "      <td>57.84</td>\n",
       "      <td>54.68</td>\n",
       "      <td>52.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>453.43</td>\n",
       "      <td>567.16</td>\n",
       "      <td>325.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.23</td>\n",
       "      <td>33.49</td>\n",
       "      <td>31.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.74</td>\n",
       "      <td>12.59</td>\n",
       "      <td>38.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.39</td>\n",
       "      <td>31.38</td>\n",
       "      <td>40.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>308.63</td>\n",
       "      <td>447.38</td>\n",
       "      <td>162.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.13</td>\n",
       "      <td>55.14</td>\n",
       "      <td>53.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>422.16</td>\n",
       "      <td>533.91</td>\n",
       "      <td>255.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.30</td>\n",
       "      <td>23.29</td>\n",
       "      <td>12.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.89</td>\n",
       "      <td>31.76</td>\n",
       "      <td>49.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.66</td>\n",
       "      <td>20.08</td>\n",
       "      <td>16.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.86</td>\n",
       "      <td>75.14</td>\n",
       "      <td>77.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>10.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>487.53</td>\n",
       "      <td>609.24</td>\n",
       "      <td>350.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.14</td>\n",
       "      <td>32.26</td>\n",
       "      <td>27.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217.56</td>\n",
       "      <td>221.49</td>\n",
       "      <td>121.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.16</td>\n",
       "      <td>101.46</td>\n",
       "      <td>39.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>427.88</td>\n",
       "      <td>355.23</td>\n",
       "      <td>188.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.89</td>\n",
       "      <td>11.83</td>\n",
       "      <td>30.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.44</td>\n",
       "      <td>126.99</td>\n",
       "      <td>141.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.19</td>\n",
       "      <td>34.24</td>\n",
       "      <td>22.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.54</td>\n",
       "      <td>173.08</td>\n",
       "      <td>193.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>626.46</td>\n",
       "      <td>558.04</td>\n",
       "      <td>428.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.06</td>\n",
       "      <td>14.53</td>\n",
       "      <td>31.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.74</td>\n",
       "      <td>15.19</td>\n",
       "      <td>15.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1580</td>\n",
       "      <td>790</td>\n",
       "      <td>3638</td>\n",
       "      <td>0</td>\n",
       "      <td>1580</td>\n",
       "      <td>790</td>\n",
       "      <td>1580</td>\n",
       "      <td>0</td>\n",
       "      <td>6/27/2014</td>\n",
       "      <td>7/25/2014</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>779</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>802</td>\n",
       "      <td>57.74</td>\n",
       "      <td>19.38</td>\n",
       "      <td>18.74</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>378.721</td>\n",
       "      <td>492.223</td>\n",
       "      <td>137.362</td>\n",
       "      <td>166.787</td>\n",
       "      <td>413.69</td>\n",
       "      <td>351.03</td>\n",
       "      <td>35.08</td>\n",
       "      <td>33.46</td>\n",
       "      <td>94.66</td>\n",
       "      <td>80.63</td>\n",
       "      <td>136.48</td>\n",
       "      <td>108.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>297.13</td>\n",
       "      <td>217.59</td>\n",
       "      <td>12.49</td>\n",
       "      <td>26.13</td>\n",
       "      <td>80.96</td>\n",
       "      <td>70.58</td>\n",
       "      <td>50.54</td>\n",
       "      <td>34.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>378.09</td>\n",
       "      <td>288.18</td>\n",
       "      <td>63.04</td>\n",
       "      <td>60.71</td>\n",
       "      <td>116.56</td>\n",
       "      <td>133.43</td>\n",
       "      <td>22.58</td>\n",
       "      <td>7.33</td>\n",
       "      <td>13.69</td>\n",
       "      <td>10.04</td>\n",
       "      <td>75.69</td>\n",
       "      <td>74.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.26</td>\n",
       "      <td>143.48</td>\n",
       "      <td>98.28</td>\n",
       "      <td>81.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508.36</td>\n",
       "      <td>431.66</td>\n",
       "      <td>171.56</td>\n",
       "      <td>142.18</td>\n",
       "      <td>23.84</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4.03</td>\n",
       "      <td>57.58</td>\n",
       "      <td>13.98</td>\n",
       "      <td>15.48</td>\n",
       "      <td>17.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.43</td>\n",
       "      <td>23.83</td>\n",
       "      <td>15.79</td>\n",
       "      <td>21.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.43</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.43</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.53</td>\n",
       "      <td>103.86</td>\n",
       "      <td>28.49</td>\n",
       "      <td>16.54</td>\n",
       "      <td>34.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>437</td>\n",
       "      <td>601</td>\n",
       "      <td>120</td>\n",
       "      <td>186</td>\n",
       "      <td>90</td>\n",
       "      <td>154</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>6/25/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/30/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/23/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>356.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>750.95</td>\n",
       "      <td>11.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315</td>\n",
       "      <td>21.03</td>\n",
       "      <td>910.65</td>\n",
       "      <td>122.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>119.518</td>\n",
       "      <td>247.435</td>\n",
       "      <td>170.231</td>\n",
       "      <td>160.042</td>\n",
       "      <td>33.89</td>\n",
       "      <td>30.11</td>\n",
       "      <td>22.43</td>\n",
       "      <td>27.84</td>\n",
       "      <td>63.48</td>\n",
       "      <td>54.16</td>\n",
       "      <td>78.34</td>\n",
       "      <td>123.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.89</td>\n",
       "      <td>30.11</td>\n",
       "      <td>22.43</td>\n",
       "      <td>27.84</td>\n",
       "      <td>38.03</td>\n",
       "      <td>40.06</td>\n",
       "      <td>34.93</td>\n",
       "      <td>37.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.93</td>\n",
       "      <td>70.18</td>\n",
       "      <td>57.36</td>\n",
       "      <td>65.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.45</td>\n",
       "      <td>14.09</td>\n",
       "      <td>43.41</td>\n",
       "      <td>83.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.45</td>\n",
       "      <td>14.09</td>\n",
       "      <td>43.41</td>\n",
       "      <td>86.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>98.04</td>\n",
       "      <td>84.28</td>\n",
       "      <td>100.78</td>\n",
       "      <td>151.33</td>\n",
       "      <td>129.34</td>\n",
       "      <td>124.34</td>\n",
       "      <td>49.93</td>\n",
       "      <td>313.38</td>\n",
       "      <td>132.94</td>\n",
       "      <td>96.24</td>\n",
       "      <td>122.58</td>\n",
       "      <td>65.06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>262.69</td>\n",
       "      <td>220.59</td>\n",
       "      <td>172.51</td>\n",
       "      <td>378.93</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.38</td>\n",
       "      <td>32.86</td>\n",
       "      <td>78.21</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.16</td>\n",
       "      <td>78.21</td>\n",
       "      <td>1.74</td>\n",
       "      <td>5.56</td>\n",
       "      <td>303.98</td>\n",
       "      <td>327.31</td>\n",
       "      <td>219.86</td>\n",
       "      <td>412.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.11</td>\n",
       "      <td>28.49</td>\n",
       "      <td>45.59</td>\n",
       "      <td>28.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>220</td>\n",
       "      <td>195</td>\n",
       "      <td>210</td>\n",
       "      <td>180</td>\n",
       "      <td>110</td>\n",
       "      <td>154</td>\n",
       "      <td>50</td>\n",
       "      <td>130</td>\n",
       "      <td>6/29/2014</td>\n",
       "      <td>7/23/2014</td>\n",
       "      <td>8/29/2014</td>\n",
       "      <td>9/20/2014</td>\n",
       "      <td>110</td>\n",
       "      <td>154</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/23/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>902</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   circle_id  loc_og_t2o_mou  std_og_t2o_mou  loc_ic_t2o_mou  \\\n",
       "0        109             0.0             0.0             0.0   \n",
       "1        109             0.0             0.0             0.0   \n",
       "2        109             0.0             0.0             0.0   \n",
       "3        109             0.0             0.0             0.0   \n",
       "4        109             0.0             0.0             0.0   \n",
       "5        109             0.0             0.0             0.0   \n",
       "6        109             0.0             0.0             0.0   \n",
       "7        109             0.0             0.0             0.0   \n",
       "8        109             0.0             0.0             0.0   \n",
       "9        109             0.0             0.0             0.0   \n",
       "\n",
       "  last_date_of_month_6 last_date_of_month_7 last_date_of_month_8  \\\n",
       "0            6/30/2014            7/31/2014            8/31/2014   \n",
       "1            6/30/2014            7/31/2014            8/31/2014   \n",
       "2            6/30/2014            7/31/2014            8/31/2014   \n",
       "3            6/30/2014            7/31/2014            8/31/2014   \n",
       "4            6/30/2014            7/31/2014            8/31/2014   \n",
       "5            6/30/2014            7/31/2014            8/31/2014   \n",
       "6            6/30/2014            7/31/2014            8/31/2014   \n",
       "7            6/30/2014            7/31/2014            8/31/2014   \n",
       "8            6/30/2014            7/31/2014            8/31/2014   \n",
       "9            6/30/2014            7/31/2014            8/31/2014   \n",
       "\n",
       "  last_date_of_month_9    arpu_6    arpu_7    arpu_8   arpu_9  onnet_mou_6  \\\n",
       "0            9/30/2014   197.385   214.816   213.803   21.100          NaN   \n",
       "1            9/30/2014    34.047   355.074   268.321   86.285        24.11   \n",
       "2            9/30/2014   167.690   189.058   210.226  290.714        11.54   \n",
       "3            9/30/2014   221.338   251.102   508.054  389.500        99.91   \n",
       "4            9/30/2014   261.636   309.876   238.174  163.426        50.31   \n",
       "5            9/30/2014    50.258    58.810    83.386  170.826        50.16   \n",
       "6            9/30/2014   429.023   190.704   255.114  114.751        71.03   \n",
       "7            9/30/2014  1069.180  1349.850  3171.480  500.000        57.84   \n",
       "8            9/30/2014   378.721   492.223   137.362  166.787       413.69   \n",
       "9            9/30/2014   119.518   247.435   170.231  160.042        33.89   \n",
       "\n",
       "   onnet_mou_7  onnet_mou_8  onnet_mou_9  offnet_mou_6  offnet_mou_7  \\\n",
       "0          NaN         0.00          NaN           NaN           NaN   \n",
       "1        78.68         7.68        18.34         15.74         99.84   \n",
       "2        55.24        37.26        74.81        143.33        220.59   \n",
       "3        54.39       310.98       241.71        123.31        109.01   \n",
       "4       149.44        83.89        58.78         76.96         91.88   \n",
       "5        43.63        85.48       138.79         19.28         13.44   \n",
       "6        45.03        76.66        15.23        262.73         49.24   \n",
       "7        54.68        52.29          NaN        453.43        567.16   \n",
       "8       351.03        35.08        33.46         94.66         80.63   \n",
       "9        30.11        22.43        27.84         63.48         54.16   \n",
       "\n",
       "   offnet_mou_8  offnet_mou_9  roam_ic_mou_6  roam_ic_mou_7  roam_ic_mou_8  \\\n",
       "0          0.00           NaN            NaN            NaN           0.00   \n",
       "1        304.76         53.76           0.00           0.00           0.00   \n",
       "2        208.36        118.91           0.00           0.00           0.00   \n",
       "3         71.68        113.54           0.00          54.86          44.38   \n",
       "4        124.26         45.81           0.00           0.00           0.00   \n",
       "5         14.46         46.91           0.00           0.00           0.00   \n",
       "6         92.08         50.33           0.00           0.00           0.00   \n",
       "7        325.91           NaN          16.23          33.49          31.64   \n",
       "8        136.48        108.71           0.00           0.00           0.00   \n",
       "9         78.34        123.48           0.00           0.00           0.00   \n",
       "\n",
       "   roam_ic_mou_9  roam_og_mou_6  roam_og_mou_7  roam_og_mou_8  roam_og_mou_9  \\\n",
       "0            NaN            NaN            NaN           0.00            NaN   \n",
       "1           0.00           0.00           0.00           0.00           0.00   \n",
       "2          38.49           0.00           0.00           0.00          70.94   \n",
       "3           0.00           0.00          28.09          39.04           0.00   \n",
       "4           0.00           0.00           0.00           0.00           0.00   \n",
       "5           0.00           0.00           0.00           0.00           0.00   \n",
       "6           0.00           0.00           0.00           0.00           0.00   \n",
       "7            NaN          23.74          12.59          38.06            NaN   \n",
       "8           0.00           0.00           0.00           0.00           0.00   \n",
       "9           0.00           0.00           0.00           0.00           0.00   \n",
       "\n",
       "   loc_og_t2t_mou_6  loc_og_t2t_mou_7  loc_og_t2t_mou_8  loc_og_t2t_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1             23.88             74.56              7.68             18.34   \n",
       "2              7.19             28.74             13.58             14.39   \n",
       "3             73.68             34.81             10.61             15.49   \n",
       "4             50.31            149.44             83.89             58.78   \n",
       "5             50.16             43.63             85.48            138.79   \n",
       "6             71.03             45.03             76.14             15.23   \n",
       "7             51.39             31.38             40.28               NaN   \n",
       "8            297.13            217.59             12.49             26.13   \n",
       "9             33.89             30.11             22.43             27.84   \n",
       "\n",
       "   loc_og_t2m_mou_6  loc_og_t2m_mou_7  loc_og_t2m_mou_8  loc_og_t2m_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1             11.51             75.94            291.86             53.76   \n",
       "2             29.34             16.86             38.46             28.16   \n",
       "3            107.43             83.21             22.46             65.46   \n",
       "4             67.64             91.88            124.26             37.89   \n",
       "5             16.39              8.83             12.38             44.78   \n",
       "6            252.23             48.71             80.63             50.33   \n",
       "7            308.63            447.38            162.28               NaN   \n",
       "8             80.96             70.58             50.54             34.58   \n",
       "9             38.03             40.06             34.93             37.26   \n",
       "\n",
       "   loc_og_t2f_mou_6  loc_og_t2f_mou_7  loc_og_t2f_mou_8  loc_og_t2f_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1              0.00              0.00              0.00              0.00   \n",
       "2             24.11             21.79             15.61             22.24   \n",
       "3              1.91              0.65              4.91              2.06   \n",
       "4              0.00              0.00              0.00              1.93   \n",
       "5              0.00              0.00              0.00              2.13   \n",
       "6             10.38              0.00              0.00              0.00   \n",
       "7             62.13             55.14             53.23               NaN   \n",
       "8              0.00              0.00              0.00              0.00   \n",
       "9              0.00              0.00              0.00              0.00   \n",
       "\n",
       "   loc_og_t2c_mou_6  loc_og_t2c_mou_7  loc_og_t2c_mou_8  loc_og_t2c_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1              0.00              2.91              0.00              0.00   \n",
       "2              0.00            135.54             45.76              0.48   \n",
       "3              0.00              0.00              0.00              0.00   \n",
       "4              0.00              0.00              0.00              0.00   \n",
       "5              0.00              0.00              0.00              0.00   \n",
       "6              0.11              0.00              0.00              0.00   \n",
       "7              0.00              0.00              0.00               NaN   \n",
       "8              0.00              0.00              7.15              0.00   \n",
       "9              0.00              0.00              0.00              0.00   \n",
       "\n",
       "   loc_og_mou_6  loc_og_mou_7  loc_og_mou_8  loc_og_mou_9  std_og_t2t_mou_6  \\\n",
       "0           NaN           NaN          0.00           NaN               NaN   \n",
       "1         35.39        150.51        299.54         72.11              0.23   \n",
       "2         60.66         67.41         67.66         64.81              4.34   \n",
       "3        183.03        118.68         37.99         83.03             26.23   \n",
       "4        117.96        241.33        208.16         98.61              0.00   \n",
       "5         66.56         52.46         97.86        185.71              0.00   \n",
       "6        333.64         93.74        156.78         65.56              0.00   \n",
       "7        422.16        533.91        255.79           NaN              4.30   \n",
       "8        378.09        288.18         63.04         60.71            116.56   \n",
       "9         71.93         70.18         57.36         65.11              0.00   \n",
       "\n",
       "   std_og_t2t_mou_7  std_og_t2t_mou_8  std_og_t2t_mou_9  std_og_t2m_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              4.11              0.00              0.00              0.00   \n",
       "2             26.49             22.58              8.76             41.81   \n",
       "3             14.89            289.58            226.21              2.99   \n",
       "4              0.00              0.00              0.00              9.31   \n",
       "5              0.00              0.00              0.00              0.00   \n",
       "6              0.00              0.51              0.00              0.00   \n",
       "7             23.29             12.01               NaN             49.89   \n",
       "8            133.43             22.58              7.33             13.69   \n",
       "9              0.00              0.00              0.00             25.45   \n",
       "\n",
       "   std_og_t2m_mou_7  std_og_t2m_mou_8  std_og_t2m_mou_9  std_og_t2f_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.46              0.13              0.00              0.00   \n",
       "2             67.41             75.53              9.28              1.48   \n",
       "3              1.73              6.53              9.99              0.00   \n",
       "4              0.00              0.00              0.00              0.00   \n",
       "5              0.00              0.00              0.00              2.88   \n",
       "6              0.53             11.45              0.00              0.00   \n",
       "7             31.76             49.14               NaN              6.66   \n",
       "8             10.04             75.69             74.13              0.00   \n",
       "9             14.09             43.41             83.26              0.00   \n",
       "\n",
       "   std_og_t2f_mou_7  std_og_t2f_mou_8  std_og_t2f_mou_9  std_og_t2c_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              0.00              0.00               0.0   \n",
       "2             14.76             22.83              0.00               0.0   \n",
       "3              0.00              0.00              0.00               0.0   \n",
       "4              0.00              0.00              0.00               0.0   \n",
       "5              4.61              2.08              0.00               0.0   \n",
       "6              0.00              0.00              0.00               0.0   \n",
       "7             20.08             16.68               NaN               0.0   \n",
       "8              0.00              0.00              0.00               0.0   \n",
       "9              0.00              0.00              2.94               0.0   \n",
       "\n",
       "   std_og_t2c_mou_7  std_og_t2c_mou_8  std_og_t2c_mou_9  std_og_mou_6  \\\n",
       "0               NaN               0.0               NaN           NaN   \n",
       "1               0.0               0.0               0.0          0.23   \n",
       "2               0.0               0.0               0.0         47.64   \n",
       "3               0.0               0.0               0.0         29.23   \n",
       "4               0.0               0.0               0.0          9.31   \n",
       "5               0.0               0.0               0.0          2.88   \n",
       "6               0.0               0.0               0.0          0.00   \n",
       "7               0.0               0.0               NaN         60.86   \n",
       "8               0.0               0.0               0.0        130.26   \n",
       "9               0.0               0.0               0.0         25.45   \n",
       "\n",
       "   std_og_mou_7  std_og_mou_8  std_og_mou_9  isd_og_mou_6  isd_og_mou_7  \\\n",
       "0           NaN          0.00           NaN           NaN           NaN   \n",
       "1          4.58          0.13          0.00           0.0          0.00   \n",
       "2        108.68        120.94         18.04           0.0          0.00   \n",
       "3         16.63        296.11        236.21           0.0          0.00   \n",
       "4          0.00          0.00          0.00           0.0          0.00   \n",
       "5          4.61          2.08          0.00           0.0          0.00   \n",
       "6          0.53         11.96          0.00           0.0          0.00   \n",
       "7         75.14         77.84           NaN           0.0          0.18   \n",
       "8        143.48         98.28         81.46           0.0          0.00   \n",
       "9         14.09         43.41         86.21           0.0          0.00   \n",
       "\n",
       "   isd_og_mou_8  isd_og_mou_9  spl_og_mou_6  spl_og_mou_7  spl_og_mou_8  \\\n",
       "0          0.00           NaN           NaN           NaN          0.00   \n",
       "1          0.00           0.0          4.68         23.43         12.76   \n",
       "2          0.00           0.0         46.56        236.84         96.84   \n",
       "3          0.00           0.0         10.96          0.00         18.09   \n",
       "4          0.00           0.0          0.00          0.00          0.00   \n",
       "5          0.00           0.0          0.00          0.00          0.00   \n",
       "6          0.00           0.0          0.11          0.53          0.00   \n",
       "7         10.01           NaN          4.50          0.00          6.50   \n",
       "8          0.00           0.0          0.00          0.00         10.23   \n",
       "9          0.00           0.0          0.66          0.00          0.00   \n",
       "\n",
       "   spl_og_mou_9  og_others_6  og_others_7  og_others_8  og_others_9  \\\n",
       "0           NaN          NaN          NaN          0.0          NaN   \n",
       "1          0.00         0.00          0.0          0.0         0.00   \n",
       "2         42.08         0.45          0.0          0.0         0.00   \n",
       "3         43.29         0.00          0.0          0.0         0.00   \n",
       "4          5.98         0.00          0.0          0.0         0.00   \n",
       "5          0.00         0.00          0.0          0.0         0.00   \n",
       "6          0.00         0.00          0.0          0.0         0.35   \n",
       "7           NaN         0.00          0.0          0.0          NaN   \n",
       "8          0.00         0.00          0.0          0.0         0.00   \n",
       "9          0.00         0.00          0.0          0.0         0.00   \n",
       "\n",
       "   total_og_mou_6  total_og_mou_7  total_og_mou_8  total_og_mou_9  \\\n",
       "0            0.00            0.00            0.00            0.00   \n",
       "1           40.31          178.53          312.44           72.11   \n",
       "2          155.33          412.94          285.46          124.94   \n",
       "3          223.23          135.31          352.21          362.54   \n",
       "4          127.28          241.33          208.16          104.59   \n",
       "5           69.44           57.08           99.94          185.71   \n",
       "6          333.76           94.81          168.74           65.91   \n",
       "7          487.53          609.24          350.16            0.00   \n",
       "8          508.36          431.66          171.56          142.18   \n",
       "9           98.04           84.28          100.78          151.33   \n",
       "\n",
       "   loc_ic_t2t_mou_6  loc_ic_t2t_mou_7  loc_ic_t2t_mou_8  loc_ic_t2t_mou_9  \\\n",
       "0               NaN               NaN              0.16               NaN   \n",
       "1              1.61             29.91             29.23            116.09   \n",
       "2            115.69             71.11             67.46            148.23   \n",
       "3             62.08             19.98              8.04             41.73   \n",
       "4            105.68             88.49            233.81            154.56   \n",
       "5             28.73             30.03             56.26             68.38   \n",
       "6           1857.99           1427.04           1896.43           2334.88   \n",
       "7             58.14             32.26             27.31               NaN   \n",
       "8             23.84              9.84              0.31              4.03   \n",
       "9            129.34            124.34             49.93            313.38   \n",
       "\n",
       "   loc_ic_t2m_mou_6  loc_ic_t2m_mou_7  loc_ic_t2m_mou_8  loc_ic_t2m_mou_9  \\\n",
       "0               NaN               NaN              4.13               NaN   \n",
       "1             17.48             65.38            375.58             56.93   \n",
       "2             14.38             15.44             38.89             38.98   \n",
       "3            113.96             64.51             20.28             52.86   \n",
       "4            106.84            109.54            104.13             48.24   \n",
       "5             49.19             57.44             62.46             84.01   \n",
       "6            248.64            336.96            265.28            231.41   \n",
       "7            217.56            221.49            121.19               NaN   \n",
       "8             57.58             13.98             15.48             17.34   \n",
       "9            132.94             96.24            122.58             65.06   \n",
       "\n",
       "   loc_ic_t2f_mou_6  loc_ic_t2f_mou_7  loc_ic_t2f_mou_8  loc_ic_t2f_mou_9  \\\n",
       "0               NaN               NaN              1.15               NaN   \n",
       "1              0.00              8.93              3.61              0.00   \n",
       "2             99.48            122.29             49.63            158.19   \n",
       "3             57.43             27.09             19.84             65.59   \n",
       "4              1.50              0.00              0.00              0.00   \n",
       "5              0.00              0.00              0.00              0.00   \n",
       "6             20.24             22.69              2.51              6.19   \n",
       "7            152.16            101.46             39.53               NaN   \n",
       "8              0.00              0.00              0.00              0.00   \n",
       "9              0.40              0.00              0.00              0.48   \n",
       "\n",
       "   loc_ic_mou_6  loc_ic_mou_7  loc_ic_mou_8  loc_ic_mou_9  std_ic_t2t_mou_6  \\\n",
       "0           NaN           NaN          5.44           NaN               NaN   \n",
       "1         19.09        104.23        408.43        173.03              0.00   \n",
       "2        229.56        208.86        155.99        345.41             72.41   \n",
       "3        233.48        111.59         48.18        160.19             43.48   \n",
       "4        214.03        198.04        337.94        202.81              0.00   \n",
       "5         77.93         87.48        118.73        152.39              0.00   \n",
       "6       2126.89       1786.71       2164.23       2572.49              0.00   \n",
       "7        427.88        355.23        188.04           NaN             36.89   \n",
       "8         81.43         23.83         15.79         21.38              0.00   \n",
       "9        262.69        220.59        172.51        378.93              0.30   \n",
       "\n",
       "   std_ic_t2t_mou_7  std_ic_t2t_mou_8  std_ic_t2t_mou_9  std_ic_t2m_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              2.35              0.00              5.90   \n",
       "2             71.29             28.69             49.44             45.18   \n",
       "3             66.44              0.00            129.84              1.33   \n",
       "4              0.00              0.86              2.31              1.93   \n",
       "5              0.00              0.00              0.00              0.00   \n",
       "6              0.00              0.00              0.00              1.39   \n",
       "7             11.83             30.39               NaN             91.44   \n",
       "8              0.58              0.10              0.00             22.43   \n",
       "9              0.00              0.00              4.38             32.86   \n",
       "\n",
       "   std_ic_t2m_mou_7  std_ic_t2m_mou_8  std_ic_t2m_mou_9  std_ic_t2f_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00             12.49             15.01              0.00   \n",
       "2            177.01            167.09            118.18             21.73   \n",
       "3             38.56              4.94             13.98              1.18   \n",
       "4              0.25              0.00              0.00              0.00   \n",
       "5              0.00              0.00              0.00             77.03   \n",
       "6              0.76              2.60              0.00              0.00   \n",
       "7            126.99            141.33               NaN             52.19   \n",
       "8              4.08              0.65             13.53              0.00   \n",
       "9             78.21              1.74              1.18              0.00   \n",
       "\n",
       "   std_ic_t2f_mou_7  std_ic_t2f_mou_8  std_ic_t2f_mou_9  std_ic_t2o_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              0.00              0.00               0.0   \n",
       "2             58.34             43.23              3.86               0.0   \n",
       "3              0.00              0.00              0.00               0.0   \n",
       "4              0.00              0.00              0.00               0.0   \n",
       "5             71.06             37.93             52.03               0.0   \n",
       "6              0.00              0.00              0.00               0.0   \n",
       "7             34.24             22.21               NaN               0.0   \n",
       "8              0.00              0.00              0.00               0.0   \n",
       "9              0.00              0.00              0.00               0.0   \n",
       "\n",
       "   std_ic_t2o_mou_7  std_ic_t2o_mou_8  std_ic_t2o_mou_9  std_ic_mou_6  \\\n",
       "0               NaN               0.0               NaN           NaN   \n",
       "1               0.0               0.0               0.0          5.90   \n",
       "2               0.0               0.0               0.0        139.33   \n",
       "3               0.0               0.0               0.0         45.99   \n",
       "4               0.0               0.0               0.0          1.93   \n",
       "5               0.0               0.0               0.0         77.03   \n",
       "6               0.0               0.0               0.0          1.39   \n",
       "7               0.0               0.0               NaN        180.54   \n",
       "8               0.0               0.0               0.0         22.43   \n",
       "9               0.0               0.0               0.0         33.16   \n",
       "\n",
       "   std_ic_mou_7  std_ic_mou_8  std_ic_mou_9  total_ic_mou_6  total_ic_mou_7  \\\n",
       "0           NaN          0.00           NaN            0.00            0.00   \n",
       "1          0.00         14.84         15.01           26.83          104.23   \n",
       "2        306.66        239.03        171.49          370.04          519.53   \n",
       "3        105.01          4.94        143.83          280.08          216.61   \n",
       "4          0.25          0.86          2.31          216.44          198.29   \n",
       "5         71.06         37.93         52.03          155.39          158.76   \n",
       "6          0.76          2.60          0.00         2128.41         1788.06   \n",
       "7        173.08        193.94           NaN          626.46          558.04   \n",
       "8          4.66          0.75         13.53          103.86           28.49   \n",
       "9         78.21          1.74          5.56          303.98          327.31   \n",
       "\n",
       "   total_ic_mou_8  total_ic_mou_9  spl_ic_mou_6  spl_ic_mou_7  spl_ic_mou_8  \\\n",
       "0            5.44            0.00           NaN           NaN          0.00   \n",
       "1          423.28          188.04          0.00          0.00          0.00   \n",
       "2          395.03          517.74          0.21          0.00          0.00   \n",
       "3           53.13          305.38          0.59          0.00          0.00   \n",
       "4          338.81          205.31          0.00          0.00          0.00   \n",
       "5          157.13          205.39          0.43          0.21          0.23   \n",
       "6         2167.11         2572.49          0.00          0.00          0.00   \n",
       "7          428.74            0.00          0.21          0.00          0.00   \n",
       "8           16.54           34.91          0.00          0.00          0.00   \n",
       "9          219.86          412.63          0.00          0.00          0.00   \n",
       "\n",
       "   spl_ic_mou_9  isd_ic_mou_6  isd_ic_mou_7  isd_ic_mou_8  isd_ic_mou_9  \\\n",
       "0           NaN           NaN           NaN          0.00           NaN   \n",
       "1          0.00          1.83          0.00          0.00          0.00   \n",
       "2          0.45          0.00          0.85          0.00          0.01   \n",
       "3          0.55          0.00          0.00          0.00          0.00   \n",
       "4          0.18          0.00          0.00          0.00          0.00   \n",
       "5          0.53          0.00          0.00          0.00          0.00   \n",
       "6          0.00          0.00          0.00          0.00          0.00   \n",
       "7           NaN          2.06         14.53         31.59           NaN   \n",
       "8          0.00          0.00          0.00          0.00          0.00   \n",
       "9          0.00          8.11         28.49         45.59         28.13   \n",
       "\n",
       "   ic_others_6  ic_others_7  ic_others_8  ic_others_9  total_rech_num_6  \\\n",
       "0          NaN          NaN         0.00          NaN                 4   \n",
       "1         0.00         0.00         0.00         0.00                 4   \n",
       "2         0.93         3.14         0.00         0.36                 5   \n",
       "3         0.00         0.00         0.00         0.80                10   \n",
       "4         0.48         0.00         0.00         0.00                 5   \n",
       "5         0.00         0.00         0.23         0.43                 2   \n",
       "6         0.11         0.58         0.28         0.00                15   \n",
       "7        15.74        15.19        15.14          NaN                 5   \n",
       "8         0.00         0.00         0.00         0.00                19   \n",
       "9         0.00         0.00         0.00         0.00                 4   \n",
       "\n",
       "   total_rech_num_7  total_rech_num_8  total_rech_num_9  total_rech_amt_6  \\\n",
       "0                 3                 2                 6               362   \n",
       "1                 9                11                 5                74   \n",
       "2                 4                 2                 7               168   \n",
       "3                11                18                14               230   \n",
       "4                 6                 3                 4               196   \n",
       "5                 2                 3                 3               120   \n",
       "6                10                11                 7               499   \n",
       "7                 5                 7                 3              1580   \n",
       "8                21                14                15               437   \n",
       "9                 2                 5                 3               220   \n",
       "\n",
       "   total_rech_amt_7  total_rech_amt_8  total_rech_amt_9  max_rech_amt_6  \\\n",
       "0               252               252                 0             252   \n",
       "1               384               283               121              44   \n",
       "2               315               116               358              86   \n",
       "3               310               601               410              60   \n",
       "4               350               287               200              56   \n",
       "5                 0               130               130             120   \n",
       "6               222               294               141              90   \n",
       "7               790              3638                 0            1580   \n",
       "8               601               120               186              90   \n",
       "9               195               210               180             110   \n",
       "\n",
       "   max_rech_amt_7  max_rech_amt_8  max_rech_amt_9 date_of_last_rech_6  \\\n",
       "0             252             252               0           6/21/2014   \n",
       "1             154              65              50           6/29/2014   \n",
       "2             200              86             100           6/17/2014   \n",
       "3              50              50              50           6/28/2014   \n",
       "4             110             110              50           6/26/2014   \n",
       "5               0             130             130           6/19/2014   \n",
       "6              37              50              30           6/28/2014   \n",
       "7             790            1580               0           6/27/2014   \n",
       "8             154              30              36           6/25/2014   \n",
       "9             154              50             130           6/29/2014   \n",
       "\n",
       "  date_of_last_rech_7 date_of_last_rech_8 date_of_last_rech_9  \\\n",
       "0           7/16/2014          08-08-2014           9/28/2014   \n",
       "1           7/31/2014           8/28/2014           9/30/2014   \n",
       "2           7/24/2014           8/14/2014           9/29/2014   \n",
       "3           7/31/2014           8/31/2014           9/30/2014   \n",
       "4           7/28/2014          08-09-2014           9/28/2014   \n",
       "5           7/17/2014           8/24/2014           9/28/2014   \n",
       "6           7/31/2014           8/28/2014           9/28/2014   \n",
       "7           7/25/2014           8/26/2014           9/30/2014   \n",
       "8           7/31/2014           8/30/2014           9/30/2014   \n",
       "9           7/23/2014           8/29/2014           9/20/2014   \n",
       "\n",
       "   last_day_rch_amt_6  last_day_rch_amt_7  last_day_rch_amt_8  \\\n",
       "0                 252                 252                 252   \n",
       "1                  44                  23                  30   \n",
       "2                   0                 200                  86   \n",
       "3                  30                  50                  50   \n",
       "4                  50                 110                 110   \n",
       "5                 120                   0                   0   \n",
       "6                  37                  24                  10   \n",
       "7                   0                   0                 779   \n",
       "8                  50                   0                  10   \n",
       "9                 110                 154                  30   \n",
       "\n",
       "   last_day_rch_amt_9 date_of_last_rech_data_6 date_of_last_rech_data_7  \\\n",
       "0                   0                6/21/2014                7/16/2014   \n",
       "1                   0                      NaN                7/25/2014   \n",
       "2                   0                      NaN                      NaN   \n",
       "3                  30                      NaN                      NaN   \n",
       "4                  50               06-04-2014                      NaN   \n",
       "5                   0                      NaN                      NaN   \n",
       "6                  24                      NaN                      NaN   \n",
       "7                   0                      NaN                      NaN   \n",
       "8                   0                      NaN                7/31/2014   \n",
       "9                  50                      NaN                7/23/2014   \n",
       "\n",
       "  date_of_last_rech_data_8 date_of_last_rech_data_9  total_rech_data_6  \\\n",
       "0               08-08-2014                      NaN                1.0   \n",
       "1               08-10-2014                      NaN                NaN   \n",
       "2                      NaN                9/17/2014                NaN   \n",
       "3                      NaN                      NaN                NaN   \n",
       "4                      NaN                      NaN                1.0   \n",
       "5                      NaN                      NaN                NaN   \n",
       "6                      NaN                      NaN                NaN   \n",
       "7                      NaN                      NaN                NaN   \n",
       "8                8/23/2014                      NaN                NaN   \n",
       "9                      NaN                      NaN                NaN   \n",
       "\n",
       "   total_rech_data_7  total_rech_data_8  total_rech_data_9  max_rech_data_6  \\\n",
       "0                1.0                1.0                NaN            252.0   \n",
       "1                1.0                2.0                NaN              NaN   \n",
       "2                NaN                NaN                1.0              NaN   \n",
       "3                NaN                NaN                NaN              NaN   \n",
       "4                NaN                NaN                NaN             56.0   \n",
       "5                NaN                NaN                NaN              NaN   \n",
       "6                NaN                NaN                NaN              NaN   \n",
       "7                NaN                NaN                NaN              NaN   \n",
       "8                2.0                3.0                NaN              NaN   \n",
       "9                1.0                NaN                NaN              NaN   \n",
       "\n",
       "   max_rech_data_7  max_rech_data_8  max_rech_data_9  count_rech_2g_6  \\\n",
       "0            252.0            252.0              NaN              0.0   \n",
       "1            154.0             25.0              NaN              NaN   \n",
       "2              NaN              NaN             46.0              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              1.0   \n",
       "5              NaN              NaN              NaN              NaN   \n",
       "6              NaN              NaN              NaN              NaN   \n",
       "7              NaN              NaN              NaN              NaN   \n",
       "8            154.0             23.0              NaN              NaN   \n",
       "9            154.0              NaN              NaN              NaN   \n",
       "\n",
       "   count_rech_2g_7  count_rech_2g_8  count_rech_2g_9  count_rech_3g_6  \\\n",
       "0              0.0              0.0              NaN              1.0   \n",
       "1              1.0              2.0              NaN              NaN   \n",
       "2              NaN              NaN              1.0              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              0.0   \n",
       "5              NaN              NaN              NaN              NaN   \n",
       "6              NaN              NaN              NaN              NaN   \n",
       "7              NaN              NaN              NaN              NaN   \n",
       "8              2.0              3.0              NaN              NaN   \n",
       "9              1.0              NaN              NaN              NaN   \n",
       "\n",
       "   count_rech_3g_7  count_rech_3g_8  count_rech_3g_9  av_rech_amt_data_6  \\\n",
       "0              1.0              1.0              NaN               252.0   \n",
       "1              0.0              0.0              NaN                 NaN   \n",
       "2              NaN              NaN              0.0                 NaN   \n",
       "3              NaN              NaN              NaN                 NaN   \n",
       "4              NaN              NaN              NaN                56.0   \n",
       "5              NaN              NaN              NaN                 NaN   \n",
       "6              NaN              NaN              NaN                 NaN   \n",
       "7              NaN              NaN              NaN                 NaN   \n",
       "8              0.0              0.0              NaN                 NaN   \n",
       "9              0.0              NaN              NaN                 NaN   \n",
       "\n",
       "   av_rech_amt_data_7  av_rech_amt_data_8  av_rech_amt_data_9  vol_2g_mb_6  \\\n",
       "0               252.0               252.0                 NaN        30.13   \n",
       "1               154.0                50.0                 NaN         0.00   \n",
       "2                 NaN                 NaN                46.0         0.00   \n",
       "3                 NaN                 NaN                 NaN         0.00   \n",
       "4                 NaN                 NaN                 NaN         0.00   \n",
       "5                 NaN                 NaN                 NaN         0.00   \n",
       "6                 NaN                 NaN                 NaN         0.00   \n",
       "7                 NaN                 NaN                 NaN         0.00   \n",
       "8               177.0                69.0                 NaN         0.00   \n",
       "9               154.0                 NaN                 NaN         0.00   \n",
       "\n",
       "   vol_2g_mb_7  vol_2g_mb_8  vol_2g_mb_9  vol_3g_mb_6  vol_3g_mb_7  \\\n",
       "0         1.32         5.75          0.0        83.57       150.76   \n",
       "1       108.07       365.47          0.0         0.00         0.00   \n",
       "2         0.00         0.00          0.0         0.00         0.00   \n",
       "3         0.00         0.00          0.0         0.00         0.00   \n",
       "4         0.00         0.00          0.0         0.00         0.00   \n",
       "5         0.00         0.00          0.0         0.00         0.00   \n",
       "6         0.00         0.00          0.0         0.00         0.00   \n",
       "7         0.00         0.00          0.0         0.00         0.00   \n",
       "8       356.00         0.03          0.0         0.00       750.95   \n",
       "9         7.37         0.00          0.0         0.00         0.00   \n",
       "\n",
       "   vol_3g_mb_8  vol_3g_mb_9  arpu_3g_6  arpu_3g_7  arpu_3g_8  arpu_3g_9  \\\n",
       "0       109.61         0.00     212.17     212.17     212.17        NaN   \n",
       "1         0.00         0.00        NaN       0.00       0.00        NaN   \n",
       "2         0.00         8.42        NaN        NaN        NaN       2.84   \n",
       "3         0.00         0.00        NaN        NaN        NaN        NaN   \n",
       "4         0.00         0.00       0.00        NaN        NaN        NaN   \n",
       "5         0.00         0.00        NaN        NaN        NaN        NaN   \n",
       "6         0.00         0.00        NaN        NaN        NaN        NaN   \n",
       "7         0.00         0.00        NaN        NaN        NaN        NaN   \n",
       "8        11.94         0.00        NaN       0.00      19.83        NaN   \n",
       "9         0.00         0.00        NaN       0.00        NaN        NaN   \n",
       "\n",
       "   arpu_2g_6  arpu_2g_7  arpu_2g_8  arpu_2g_9  night_pck_user_6  \\\n",
       "0     212.17     212.17     212.17        NaN               0.0   \n",
       "1        NaN      28.61       7.60        NaN               NaN   \n",
       "2        NaN        NaN        NaN        0.0               NaN   \n",
       "3        NaN        NaN        NaN        NaN               NaN   \n",
       "4       0.00        NaN        NaN        NaN               0.0   \n",
       "5        NaN        NaN        NaN        NaN               NaN   \n",
       "6        NaN        NaN        NaN        NaN               NaN   \n",
       "7        NaN        NaN        NaN        NaN               NaN   \n",
       "8        NaN       0.00       0.00        NaN               NaN   \n",
       "9        NaN       0.00        NaN        NaN               NaN   \n",
       "\n",
       "   night_pck_user_7  night_pck_user_8  night_pck_user_9  monthly_2g_6  \\\n",
       "0               0.0               0.0               NaN             0   \n",
       "1               0.0               0.0               NaN             0   \n",
       "2               NaN               NaN               0.0             0   \n",
       "3               NaN               NaN               NaN             0   \n",
       "4               NaN               NaN               NaN             0   \n",
       "5               NaN               NaN               NaN             0   \n",
       "6               NaN               NaN               NaN             0   \n",
       "7               NaN               NaN               NaN             0   \n",
       "8               0.0               0.0               NaN             0   \n",
       "9               0.0               NaN               NaN             0   \n",
       "\n",
       "   monthly_2g_7  monthly_2g_8  monthly_2g_9  sachet_2g_6  sachet_2g_7  \\\n",
       "0             0             0             0            0            0   \n",
       "1             1             0             0            0            0   \n",
       "2             0             0             0            0            0   \n",
       "3             0             0             0            0            0   \n",
       "4             0             0             0            1            0   \n",
       "5             0             0             0            0            0   \n",
       "6             0             0             0            0            0   \n",
       "7             0             0             0            0            0   \n",
       "8             1             0             0            0            1   \n",
       "9             1             0             0            0            0   \n",
       "\n",
       "   sachet_2g_8  sachet_2g_9  monthly_3g_6  monthly_3g_7  monthly_3g_8  \\\n",
       "0            0            0             1             1             1   \n",
       "1            2            0             0             0             0   \n",
       "2            0            1             0             0             0   \n",
       "3            0            0             0             0             0   \n",
       "4            0            0             0             0             0   \n",
       "5            0            0             0             0             0   \n",
       "6            0            0             0             0             0   \n",
       "7            0            0             0             0             0   \n",
       "8            3            0             0             0             0   \n",
       "9            0            0             0             0             0   \n",
       "\n",
       "   monthly_3g_9  sachet_3g_6  sachet_3g_7  sachet_3g_8  sachet_3g_9  \\\n",
       "0             0            0            0            0            0   \n",
       "1             0            0            0            0            0   \n",
       "2             0            0            0            0            0   \n",
       "3             0            0            0            0            0   \n",
       "4             0            0            0            0            0   \n",
       "5             0            0            0            0            0   \n",
       "6             0            0            0            0            0   \n",
       "7             0            0            0            0            0   \n",
       "8             0            0            0            0            0   \n",
       "9             0            0            0            0            0   \n",
       "\n",
       "   fb_user_6  fb_user_7  fb_user_8  fb_user_9   aon  aug_vbc_3g  jul_vbc_3g  \\\n",
       "0        1.0        1.0        1.0        NaN   968       30.40        0.00   \n",
       "1        NaN        1.0        1.0        NaN  1006        0.00        0.00   \n",
       "2        NaN        NaN        NaN        1.0  1103        0.00        0.00   \n",
       "3        NaN        NaN        NaN        NaN  2491        0.00        0.00   \n",
       "4        0.0        NaN        NaN        NaN  1526        0.00        0.00   \n",
       "5        NaN        NaN        NaN        NaN  1471        0.00        0.00   \n",
       "6        NaN        NaN        NaN        NaN  1673        0.00        0.00   \n",
       "7        NaN        NaN        NaN        NaN   802       57.74       19.38   \n",
       "8        NaN        1.0        1.0        NaN   315       21.03      910.65   \n",
       "9        NaN        1.0        NaN        NaN   902        0.00        0.00   \n",
       "\n",
       "   jun_vbc_3g  sep_vbc_3g  \n",
       "0      101.20        3.58  \n",
       "1        0.00        0.00  \n",
       "2        4.17        0.00  \n",
       "3        0.00        0.00  \n",
       "4        0.00        0.00  \n",
       "5        0.00        0.00  \n",
       "6        0.00        0.00  \n",
       "7       18.74        0.00  \n",
       "8      122.16        0.00  \n",
       "9        0.00        0.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at initial rows of the data\n",
    "churn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RUt5O3eZd81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99999 entries, 0 to 99998\n",
      "Data columns (total 225 columns):\n",
      " #    Column                    Dtype  \n",
      "---   ------                    -----  \n",
      " 0    circle_id                 int64  \n",
      " 1    loc_og_t2o_mou            float64\n",
      " 2    std_og_t2o_mou            float64\n",
      " 3    loc_ic_t2o_mou            float64\n",
      " 4    last_date_of_month_6      object \n",
      " 5    last_date_of_month_7      object \n",
      " 6    last_date_of_month_8      object \n",
      " 7    last_date_of_month_9      object \n",
      " 8    arpu_6                    float64\n",
      " 9    arpu_7                    float64\n",
      " 10   arpu_8                    float64\n",
      " 11   arpu_9                    float64\n",
      " 12   onnet_mou_6               float64\n",
      " 13   onnet_mou_7               float64\n",
      " 14   onnet_mou_8               float64\n",
      " 15   onnet_mou_9               float64\n",
      " 16   offnet_mou_6              float64\n",
      " 17   offnet_mou_7              float64\n",
      " 18   offnet_mou_8              float64\n",
      " 19   offnet_mou_9              float64\n",
      " 20   roam_ic_mou_6             float64\n",
      " 21   roam_ic_mou_7             float64\n",
      " 22   roam_ic_mou_8             float64\n",
      " 23   roam_ic_mou_9             float64\n",
      " 24   roam_og_mou_6             float64\n",
      " 25   roam_og_mou_7             float64\n",
      " 26   roam_og_mou_8             float64\n",
      " 27   roam_og_mou_9             float64\n",
      " 28   loc_og_t2t_mou_6          float64\n",
      " 29   loc_og_t2t_mou_7          float64\n",
      " 30   loc_og_t2t_mou_8          float64\n",
      " 31   loc_og_t2t_mou_9          float64\n",
      " 32   loc_og_t2m_mou_6          float64\n",
      " 33   loc_og_t2m_mou_7          float64\n",
      " 34   loc_og_t2m_mou_8          float64\n",
      " 35   loc_og_t2m_mou_9          float64\n",
      " 36   loc_og_t2f_mou_6          float64\n",
      " 37   loc_og_t2f_mou_7          float64\n",
      " 38   loc_og_t2f_mou_8          float64\n",
      " 39   loc_og_t2f_mou_9          float64\n",
      " 40   loc_og_t2c_mou_6          float64\n",
      " 41   loc_og_t2c_mou_7          float64\n",
      " 42   loc_og_t2c_mou_8          float64\n",
      " 43   loc_og_t2c_mou_9          float64\n",
      " 44   loc_og_mou_6              float64\n",
      " 45   loc_og_mou_7              float64\n",
      " 46   loc_og_mou_8              float64\n",
      " 47   loc_og_mou_9              float64\n",
      " 48   std_og_t2t_mou_6          float64\n",
      " 49   std_og_t2t_mou_7          float64\n",
      " 50   std_og_t2t_mou_8          float64\n",
      " 51   std_og_t2t_mou_9          float64\n",
      " 52   std_og_t2m_mou_6          float64\n",
      " 53   std_og_t2m_mou_7          float64\n",
      " 54   std_og_t2m_mou_8          float64\n",
      " 55   std_og_t2m_mou_9          float64\n",
      " 56   std_og_t2f_mou_6          float64\n",
      " 57   std_og_t2f_mou_7          float64\n",
      " 58   std_og_t2f_mou_8          float64\n",
      " 59   std_og_t2f_mou_9          float64\n",
      " 60   std_og_t2c_mou_6          float64\n",
      " 61   std_og_t2c_mou_7          float64\n",
      " 62   std_og_t2c_mou_8          float64\n",
      " 63   std_og_t2c_mou_9          float64\n",
      " 64   std_og_mou_6              float64\n",
      " 65   std_og_mou_7              float64\n",
      " 66   std_og_mou_8              float64\n",
      " 67   std_og_mou_9              float64\n",
      " 68   isd_og_mou_6              float64\n",
      " 69   isd_og_mou_7              float64\n",
      " 70   isd_og_mou_8              float64\n",
      " 71   isd_og_mou_9              float64\n",
      " 72   spl_og_mou_6              float64\n",
      " 73   spl_og_mou_7              float64\n",
      " 74   spl_og_mou_8              float64\n",
      " 75   spl_og_mou_9              float64\n",
      " 76   og_others_6               float64\n",
      " 77   og_others_7               float64\n",
      " 78   og_others_8               float64\n",
      " 79   og_others_9               float64\n",
      " 80   total_og_mou_6            float64\n",
      " 81   total_og_mou_7            float64\n",
      " 82   total_og_mou_8            float64\n",
      " 83   total_og_mou_9            float64\n",
      " 84   loc_ic_t2t_mou_6          float64\n",
      " 85   loc_ic_t2t_mou_7          float64\n",
      " 86   loc_ic_t2t_mou_8          float64\n",
      " 87   loc_ic_t2t_mou_9          float64\n",
      " 88   loc_ic_t2m_mou_6          float64\n",
      " 89   loc_ic_t2m_mou_7          float64\n",
      " 90   loc_ic_t2m_mou_8          float64\n",
      " 91   loc_ic_t2m_mou_9          float64\n",
      " 92   loc_ic_t2f_mou_6          float64\n",
      " 93   loc_ic_t2f_mou_7          float64\n",
      " 94   loc_ic_t2f_mou_8          float64\n",
      " 95   loc_ic_t2f_mou_9          float64\n",
      " 96   loc_ic_mou_6              float64\n",
      " 97   loc_ic_mou_7              float64\n",
      " 98   loc_ic_mou_8              float64\n",
      " 99   loc_ic_mou_9              float64\n",
      " 100  std_ic_t2t_mou_6          float64\n",
      " 101  std_ic_t2t_mou_7          float64\n",
      " 102  std_ic_t2t_mou_8          float64\n",
      " 103  std_ic_t2t_mou_9          float64\n",
      " 104  std_ic_t2m_mou_6          float64\n",
      " 105  std_ic_t2m_mou_7          float64\n",
      " 106  std_ic_t2m_mou_8          float64\n",
      " 107  std_ic_t2m_mou_9          float64\n",
      " 108  std_ic_t2f_mou_6          float64\n",
      " 109  std_ic_t2f_mou_7          float64\n",
      " 110  std_ic_t2f_mou_8          float64\n",
      " 111  std_ic_t2f_mou_9          float64\n",
      " 112  std_ic_t2o_mou_6          float64\n",
      " 113  std_ic_t2o_mou_7          float64\n",
      " 114  std_ic_t2o_mou_8          float64\n",
      " 115  std_ic_t2o_mou_9          float64\n",
      " 116  std_ic_mou_6              float64\n",
      " 117  std_ic_mou_7              float64\n",
      " 118  std_ic_mou_8              float64\n",
      " 119  std_ic_mou_9              float64\n",
      " 120  total_ic_mou_6            float64\n",
      " 121  total_ic_mou_7            float64\n",
      " 122  total_ic_mou_8            float64\n",
      " 123  total_ic_mou_9            float64\n",
      " 124  spl_ic_mou_6              float64\n",
      " 125  spl_ic_mou_7              float64\n",
      " 126  spl_ic_mou_8              float64\n",
      " 127  spl_ic_mou_9              float64\n",
      " 128  isd_ic_mou_6              float64\n",
      " 129  isd_ic_mou_7              float64\n",
      " 130  isd_ic_mou_8              float64\n",
      " 131  isd_ic_mou_9              float64\n",
      " 132  ic_others_6               float64\n",
      " 133  ic_others_7               float64\n",
      " 134  ic_others_8               float64\n",
      " 135  ic_others_9               float64\n",
      " 136  total_rech_num_6          int64  \n",
      " 137  total_rech_num_7          int64  \n",
      " 138  total_rech_num_8          int64  \n",
      " 139  total_rech_num_9          int64  \n",
      " 140  total_rech_amt_6          int64  \n",
      " 141  total_rech_amt_7          int64  \n",
      " 142  total_rech_amt_8          int64  \n",
      " 143  total_rech_amt_9          int64  \n",
      " 144  max_rech_amt_6            int64  \n",
      " 145  max_rech_amt_7            int64  \n",
      " 146  max_rech_amt_8            int64  \n",
      " 147  max_rech_amt_9            int64  \n",
      " 148  date_of_last_rech_6       object \n",
      " 149  date_of_last_rech_7       object \n",
      " 150  date_of_last_rech_8       object \n",
      " 151  date_of_last_rech_9       object \n",
      " 152  last_day_rch_amt_6        int64  \n",
      " 153  last_day_rch_amt_7        int64  \n",
      " 154  last_day_rch_amt_8        int64  \n",
      " 155  last_day_rch_amt_9        int64  \n",
      " 156  date_of_last_rech_data_6  object \n",
      " 157  date_of_last_rech_data_7  object \n",
      " 158  date_of_last_rech_data_8  object \n",
      " 159  date_of_last_rech_data_9  object \n",
      " 160  total_rech_data_6         float64\n",
      " 161  total_rech_data_7         float64\n",
      " 162  total_rech_data_8         float64\n",
      " 163  total_rech_data_9         float64\n",
      " 164  max_rech_data_6           float64\n",
      " 165  max_rech_data_7           float64\n",
      " 166  max_rech_data_8           float64\n",
      " 167  max_rech_data_9           float64\n",
      " 168  count_rech_2g_6           float64\n",
      " 169  count_rech_2g_7           float64\n",
      " 170  count_rech_2g_8           float64\n",
      " 171  count_rech_2g_9           float64\n",
      " 172  count_rech_3g_6           float64\n",
      " 173  count_rech_3g_7           float64\n",
      " 174  count_rech_3g_8           float64\n",
      " 175  count_rech_3g_9           float64\n",
      " 176  av_rech_amt_data_6        float64\n",
      " 177  av_rech_amt_data_7        float64\n",
      " 178  av_rech_amt_data_8        float64\n",
      " 179  av_rech_amt_data_9        float64\n",
      " 180  vol_2g_mb_6               float64\n",
      " 181  vol_2g_mb_7               float64\n",
      " 182  vol_2g_mb_8               float64\n",
      " 183  vol_2g_mb_9               float64\n",
      " 184  vol_3g_mb_6               float64\n",
      " 185  vol_3g_mb_7               float64\n",
      " 186  vol_3g_mb_8               float64\n",
      " 187  vol_3g_mb_9               float64\n",
      " 188  arpu_3g_6                 float64\n",
      " 189  arpu_3g_7                 float64\n",
      " 190  arpu_3g_8                 float64\n",
      " 191  arpu_3g_9                 float64\n",
      " 192  arpu_2g_6                 float64\n",
      " 193  arpu_2g_7                 float64\n",
      " 194  arpu_2g_8                 float64\n",
      " 195  arpu_2g_9                 float64\n",
      " 196  night_pck_user_6          float64\n",
      " 197  night_pck_user_7          float64\n",
      " 198  night_pck_user_8          float64\n",
      " 199  night_pck_user_9          float64\n",
      " 200  monthly_2g_6              int64  \n",
      " 201  monthly_2g_7              int64  \n",
      " 202  monthly_2g_8              int64  \n",
      " 203  monthly_2g_9              int64  \n",
      " 204  sachet_2g_6               int64  \n",
      " 205  sachet_2g_7               int64  \n",
      " 206  sachet_2g_8               int64  \n",
      " 207  sachet_2g_9               int64  \n",
      " 208  monthly_3g_6              int64  \n",
      " 209  monthly_3g_7              int64  \n",
      " 210  monthly_3g_8              int64  \n",
      " 211  monthly_3g_9              int64  \n",
      " 212  sachet_3g_6               int64  \n",
      " 213  sachet_3g_7               int64  \n",
      " 214  sachet_3g_8               int64  \n",
      " 215  sachet_3g_9               int64  \n",
      " 216  fb_user_6                 float64\n",
      " 217  fb_user_7                 float64\n",
      " 218  fb_user_8                 float64\n",
      " 219  fb_user_9                 float64\n",
      " 220  aon                       int64  \n",
      " 221  aug_vbc_3g                float64\n",
      " 222  jul_vbc_3g                float64\n",
      " 223  jun_vbc_3g                float64\n",
      " 224  sep_vbc_3g                float64\n",
      "dtypes: float64(179), int64(34), object(12)\n",
      "memory usage: 171.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# summary of different feature types\n",
    "churn.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HcEuQeWkd81c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>circle_id</th>\n",
       "      <th>loc_og_t2o_mou</th>\n",
       "      <th>std_og_t2o_mou</th>\n",
       "      <th>loc_ic_t2o_mou</th>\n",
       "      <th>last_date_of_month_6</th>\n",
       "      <th>last_date_of_month_7</th>\n",
       "      <th>last_date_of_month_8</th>\n",
       "      <th>last_date_of_month_9</th>\n",
       "      <th>arpu_6</th>\n",
       "      <th>arpu_7</th>\n",
       "      <th>arpu_8</th>\n",
       "      <th>arpu_9</th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>onnet_mou_9</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>offnet_mou_9</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>roam_ic_mou_9</th>\n",
       "      <th>roam_og_mou_6</th>\n",
       "      <th>roam_og_mou_7</th>\n",
       "      <th>roam_og_mou_8</th>\n",
       "      <th>roam_og_mou_9</th>\n",
       "      <th>loc_og_t2t_mou_6</th>\n",
       "      <th>loc_og_t2t_mou_7</th>\n",
       "      <th>loc_og_t2t_mou_8</th>\n",
       "      <th>loc_og_t2t_mou_9</th>\n",
       "      <th>loc_og_t2m_mou_6</th>\n",
       "      <th>loc_og_t2m_mou_7</th>\n",
       "      <th>loc_og_t2m_mou_8</th>\n",
       "      <th>loc_og_t2m_mou_9</th>\n",
       "      <th>loc_og_t2f_mou_6</th>\n",
       "      <th>loc_og_t2f_mou_7</th>\n",
       "      <th>loc_og_t2f_mou_8</th>\n",
       "      <th>loc_og_t2f_mou_9</th>\n",
       "      <th>loc_og_t2c_mou_6</th>\n",
       "      <th>loc_og_t2c_mou_7</th>\n",
       "      <th>loc_og_t2c_mou_8</th>\n",
       "      <th>loc_og_t2c_mou_9</th>\n",
       "      <th>loc_og_mou_6</th>\n",
       "      <th>loc_og_mou_7</th>\n",
       "      <th>loc_og_mou_8</th>\n",
       "      <th>loc_og_mou_9</th>\n",
       "      <th>std_og_t2t_mou_6</th>\n",
       "      <th>std_og_t2t_mou_7</th>\n",
       "      <th>std_og_t2t_mou_8</th>\n",
       "      <th>std_og_t2t_mou_9</th>\n",
       "      <th>std_og_t2m_mou_6</th>\n",
       "      <th>std_og_t2m_mou_7</th>\n",
       "      <th>std_og_t2m_mou_8</th>\n",
       "      <th>std_og_t2m_mou_9</th>\n",
       "      <th>std_og_t2f_mou_6</th>\n",
       "      <th>std_og_t2f_mou_7</th>\n",
       "      <th>std_og_t2f_mou_8</th>\n",
       "      <th>std_og_t2f_mou_9</th>\n",
       "      <th>std_og_t2c_mou_6</th>\n",
       "      <th>std_og_t2c_mou_7</th>\n",
       "      <th>std_og_t2c_mou_8</th>\n",
       "      <th>std_og_t2c_mou_9</th>\n",
       "      <th>std_og_mou_6</th>\n",
       "      <th>std_og_mou_7</th>\n",
       "      <th>std_og_mou_8</th>\n",
       "      <th>std_og_mou_9</th>\n",
       "      <th>isd_og_mou_6</th>\n",
       "      <th>isd_og_mou_7</th>\n",
       "      <th>isd_og_mou_8</th>\n",
       "      <th>isd_og_mou_9</th>\n",
       "      <th>spl_og_mou_6</th>\n",
       "      <th>spl_og_mou_7</th>\n",
       "      <th>spl_og_mou_8</th>\n",
       "      <th>spl_og_mou_9</th>\n",
       "      <th>og_others_6</th>\n",
       "      <th>og_others_7</th>\n",
       "      <th>og_others_8</th>\n",
       "      <th>og_others_9</th>\n",
       "      <th>total_og_mou_6</th>\n",
       "      <th>total_og_mou_7</th>\n",
       "      <th>total_og_mou_8</th>\n",
       "      <th>total_og_mou_9</th>\n",
       "      <th>loc_ic_t2t_mou_6</th>\n",
       "      <th>loc_ic_t2t_mou_7</th>\n",
       "      <th>loc_ic_t2t_mou_8</th>\n",
       "      <th>loc_ic_t2t_mou_9</th>\n",
       "      <th>loc_ic_t2m_mou_6</th>\n",
       "      <th>loc_ic_t2m_mou_7</th>\n",
       "      <th>loc_ic_t2m_mou_8</th>\n",
       "      <th>loc_ic_t2m_mou_9</th>\n",
       "      <th>loc_ic_t2f_mou_6</th>\n",
       "      <th>loc_ic_t2f_mou_7</th>\n",
       "      <th>loc_ic_t2f_mou_8</th>\n",
       "      <th>loc_ic_t2f_mou_9</th>\n",
       "      <th>loc_ic_mou_6</th>\n",
       "      <th>loc_ic_mou_7</th>\n",
       "      <th>loc_ic_mou_8</th>\n",
       "      <th>loc_ic_mou_9</th>\n",
       "      <th>std_ic_t2t_mou_6</th>\n",
       "      <th>std_ic_t2t_mou_7</th>\n",
       "      <th>std_ic_t2t_mou_8</th>\n",
       "      <th>std_ic_t2t_mou_9</th>\n",
       "      <th>std_ic_t2m_mou_6</th>\n",
       "      <th>std_ic_t2m_mou_7</th>\n",
       "      <th>std_ic_t2m_mou_8</th>\n",
       "      <th>std_ic_t2m_mou_9</th>\n",
       "      <th>std_ic_t2f_mou_6</th>\n",
       "      <th>std_ic_t2f_mou_7</th>\n",
       "      <th>std_ic_t2f_mou_8</th>\n",
       "      <th>std_ic_t2f_mou_9</th>\n",
       "      <th>std_ic_t2o_mou_6</th>\n",
       "      <th>std_ic_t2o_mou_7</th>\n",
       "      <th>std_ic_t2o_mou_8</th>\n",
       "      <th>std_ic_t2o_mou_9</th>\n",
       "      <th>std_ic_mou_6</th>\n",
       "      <th>std_ic_mou_7</th>\n",
       "      <th>std_ic_mou_8</th>\n",
       "      <th>std_ic_mou_9</th>\n",
       "      <th>total_ic_mou_6</th>\n",
       "      <th>total_ic_mou_7</th>\n",
       "      <th>total_ic_mou_8</th>\n",
       "      <th>total_ic_mou_9</th>\n",
       "      <th>spl_ic_mou_6</th>\n",
       "      <th>spl_ic_mou_7</th>\n",
       "      <th>spl_ic_mou_8</th>\n",
       "      <th>spl_ic_mou_9</th>\n",
       "      <th>isd_ic_mou_6</th>\n",
       "      <th>isd_ic_mou_7</th>\n",
       "      <th>isd_ic_mou_8</th>\n",
       "      <th>isd_ic_mou_9</th>\n",
       "      <th>ic_others_6</th>\n",
       "      <th>ic_others_7</th>\n",
       "      <th>ic_others_8</th>\n",
       "      <th>ic_others_9</th>\n",
       "      <th>total_rech_num_6</th>\n",
       "      <th>total_rech_num_7</th>\n",
       "      <th>total_rech_num_8</th>\n",
       "      <th>total_rech_num_9</th>\n",
       "      <th>total_rech_amt_6</th>\n",
       "      <th>total_rech_amt_7</th>\n",
       "      <th>total_rech_amt_8</th>\n",
       "      <th>total_rech_amt_9</th>\n",
       "      <th>max_rech_amt_6</th>\n",
       "      <th>max_rech_amt_7</th>\n",
       "      <th>max_rech_amt_8</th>\n",
       "      <th>max_rech_amt_9</th>\n",
       "      <th>date_of_last_rech_6</th>\n",
       "      <th>date_of_last_rech_7</th>\n",
       "      <th>date_of_last_rech_8</th>\n",
       "      <th>date_of_last_rech_9</th>\n",
       "      <th>last_day_rch_amt_6</th>\n",
       "      <th>last_day_rch_amt_7</th>\n",
       "      <th>last_day_rch_amt_8</th>\n",
       "      <th>last_day_rch_amt_9</th>\n",
       "      <th>date_of_last_rech_data_6</th>\n",
       "      <th>date_of_last_rech_data_7</th>\n",
       "      <th>date_of_last_rech_data_8</th>\n",
       "      <th>date_of_last_rech_data_9</th>\n",
       "      <th>total_rech_data_6</th>\n",
       "      <th>total_rech_data_7</th>\n",
       "      <th>total_rech_data_8</th>\n",
       "      <th>total_rech_data_9</th>\n",
       "      <th>max_rech_data_6</th>\n",
       "      <th>max_rech_data_7</th>\n",
       "      <th>max_rech_data_8</th>\n",
       "      <th>max_rech_data_9</th>\n",
       "      <th>count_rech_2g_6</th>\n",
       "      <th>count_rech_2g_7</th>\n",
       "      <th>count_rech_2g_8</th>\n",
       "      <th>count_rech_2g_9</th>\n",
       "      <th>count_rech_3g_6</th>\n",
       "      <th>count_rech_3g_7</th>\n",
       "      <th>count_rech_3g_8</th>\n",
       "      <th>count_rech_3g_9</th>\n",
       "      <th>av_rech_amt_data_6</th>\n",
       "      <th>av_rech_amt_data_7</th>\n",
       "      <th>av_rech_amt_data_8</th>\n",
       "      <th>av_rech_amt_data_9</th>\n",
       "      <th>vol_2g_mb_6</th>\n",
       "      <th>vol_2g_mb_7</th>\n",
       "      <th>vol_2g_mb_8</th>\n",
       "      <th>vol_2g_mb_9</th>\n",
       "      <th>vol_3g_mb_6</th>\n",
       "      <th>vol_3g_mb_7</th>\n",
       "      <th>vol_3g_mb_8</th>\n",
       "      <th>vol_3g_mb_9</th>\n",
       "      <th>arpu_3g_6</th>\n",
       "      <th>arpu_3g_7</th>\n",
       "      <th>arpu_3g_8</th>\n",
       "      <th>arpu_3g_9</th>\n",
       "      <th>arpu_2g_6</th>\n",
       "      <th>arpu_2g_7</th>\n",
       "      <th>arpu_2g_8</th>\n",
       "      <th>arpu_2g_9</th>\n",
       "      <th>night_pck_user_6</th>\n",
       "      <th>night_pck_user_7</th>\n",
       "      <th>night_pck_user_8</th>\n",
       "      <th>night_pck_user_9</th>\n",
       "      <th>monthly_2g_6</th>\n",
       "      <th>monthly_2g_7</th>\n",
       "      <th>monthly_2g_8</th>\n",
       "      <th>monthly_2g_9</th>\n",
       "      <th>sachet_2g_6</th>\n",
       "      <th>sachet_2g_7</th>\n",
       "      <th>sachet_2g_8</th>\n",
       "      <th>sachet_2g_9</th>\n",
       "      <th>monthly_3g_6</th>\n",
       "      <th>monthly_3g_7</th>\n",
       "      <th>monthly_3g_8</th>\n",
       "      <th>monthly_3g_9</th>\n",
       "      <th>sachet_3g_6</th>\n",
       "      <th>sachet_3g_7</th>\n",
       "      <th>sachet_3g_8</th>\n",
       "      <th>sachet_3g_9</th>\n",
       "      <th>fb_user_6</th>\n",
       "      <th>fb_user_7</th>\n",
       "      <th>fb_user_8</th>\n",
       "      <th>fb_user_9</th>\n",
       "      <th>aon</th>\n",
       "      <th>aug_vbc_3g</th>\n",
       "      <th>jul_vbc_3g</th>\n",
       "      <th>jun_vbc_3g</th>\n",
       "      <th>sep_vbc_3g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>98981.0</td>\n",
       "      <td>98981.0</td>\n",
       "      <td>98981.0</td>\n",
       "      <td>99999</td>\n",
       "      <td>99398</td>\n",
       "      <td>98899</td>\n",
       "      <td>98340</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.0</td>\n",
       "      <td>96140.0</td>\n",
       "      <td>94621.0</td>\n",
       "      <td>92254.0</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.0</td>\n",
       "      <td>96140.0</td>\n",
       "      <td>94621.0</td>\n",
       "      <td>92254.0</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>96062.000000</td>\n",
       "      <td>96140.000000</td>\n",
       "      <td>94621.000000</td>\n",
       "      <td>92254.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>98392</td>\n",
       "      <td>98232</td>\n",
       "      <td>96377</td>\n",
       "      <td>95239</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>25153</td>\n",
       "      <td>25571</td>\n",
       "      <td>26339</td>\n",
       "      <td>25922</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.00000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/29/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/29/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99999</td>\n",
       "      <td>99398</td>\n",
       "      <td>98899</td>\n",
       "      <td>98340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16960</td>\n",
       "      <td>17288</td>\n",
       "      <td>14706</td>\n",
       "      <td>22623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1888</td>\n",
       "      <td>1813</td>\n",
       "      <td>1998</td>\n",
       "      <td>2329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282.987358</td>\n",
       "      <td>278.536648</td>\n",
       "      <td>279.154731</td>\n",
       "      <td>261.645069</td>\n",
       "      <td>132.395875</td>\n",
       "      <td>133.670805</td>\n",
       "      <td>133.018098</td>\n",
       "      <td>130.302327</td>\n",
       "      <td>197.935577</td>\n",
       "      <td>197.045133</td>\n",
       "      <td>196.574803</td>\n",
       "      <td>190.337222</td>\n",
       "      <td>9.950013</td>\n",
       "      <td>7.149898</td>\n",
       "      <td>7.292981</td>\n",
       "      <td>6.343841</td>\n",
       "      <td>13.911337</td>\n",
       "      <td>9.818732</td>\n",
       "      <td>9.971890</td>\n",
       "      <td>8.555519</td>\n",
       "      <td>47.100763</td>\n",
       "      <td>46.473010</td>\n",
       "      <td>45.887806</td>\n",
       "      <td>44.584446</td>\n",
       "      <td>93.342088</td>\n",
       "      <td>91.397131</td>\n",
       "      <td>91.755128</td>\n",
       "      <td>90.463192</td>\n",
       "      <td>3.751013</td>\n",
       "      <td>3.792985</td>\n",
       "      <td>3.677991</td>\n",
       "      <td>3.655123</td>\n",
       "      <td>1.123056</td>\n",
       "      <td>1.368500</td>\n",
       "      <td>1.433821</td>\n",
       "      <td>1.232726</td>\n",
       "      <td>144.201175</td>\n",
       "      <td>141.670476</td>\n",
       "      <td>141.328209</td>\n",
       "      <td>138.709970</td>\n",
       "      <td>79.829870</td>\n",
       "      <td>83.299598</td>\n",
       "      <td>83.282673</td>\n",
       "      <td>82.342919</td>\n",
       "      <td>87.299624</td>\n",
       "      <td>90.804137</td>\n",
       "      <td>89.838390</td>\n",
       "      <td>86.276622</td>\n",
       "      <td>1.129011</td>\n",
       "      <td>1.115010</td>\n",
       "      <td>1.067792</td>\n",
       "      <td>1.042362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.261218</td>\n",
       "      <td>175.221436</td>\n",
       "      <td>174.191498</td>\n",
       "      <td>169.664466</td>\n",
       "      <td>0.798277</td>\n",
       "      <td>0.776572</td>\n",
       "      <td>0.791247</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>3.916811</td>\n",
       "      <td>4.978279</td>\n",
       "      <td>5.053769</td>\n",
       "      <td>4.412767</td>\n",
       "      <td>0.454157</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.047456</td>\n",
       "      <td>305.133424</td>\n",
       "      <td>310.231175</td>\n",
       "      <td>304.119513</td>\n",
       "      <td>289.279198</td>\n",
       "      <td>47.922365</td>\n",
       "      <td>47.990520</td>\n",
       "      <td>47.211362</td>\n",
       "      <td>46.281794</td>\n",
       "      <td>107.475650</td>\n",
       "      <td>107.120493</td>\n",
       "      <td>108.460515</td>\n",
       "      <td>106.155471</td>\n",
       "      <td>12.084305</td>\n",
       "      <td>12.599697</td>\n",
       "      <td>11.751834</td>\n",
       "      <td>12.173105</td>\n",
       "      <td>167.491059</td>\n",
       "      <td>167.719540</td>\n",
       "      <td>167.432575</td>\n",
       "      <td>164.619293</td>\n",
       "      <td>9.575993</td>\n",
       "      <td>10.011904</td>\n",
       "      <td>9.883921</td>\n",
       "      <td>9.432479</td>\n",
       "      <td>20.722240</td>\n",
       "      <td>21.656415</td>\n",
       "      <td>21.183211</td>\n",
       "      <td>19.620913</td>\n",
       "      <td>2.156397</td>\n",
       "      <td>2.216923</td>\n",
       "      <td>2.085004</td>\n",
       "      <td>2.173419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.457179</td>\n",
       "      <td>33.887833</td>\n",
       "      <td>33.154735</td>\n",
       "      <td>31.229344</td>\n",
       "      <td>200.130037</td>\n",
       "      <td>202.853055</td>\n",
       "      <td>198.750783</td>\n",
       "      <td>189.214260</td>\n",
       "      <td>0.061557</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>0.040361</td>\n",
       "      <td>0.163137</td>\n",
       "      <td>7.460608</td>\n",
       "      <td>8.334936</td>\n",
       "      <td>8.442001</td>\n",
       "      <td>8.063003</td>\n",
       "      <td>0.854656</td>\n",
       "      <td>1.012960</td>\n",
       "      <td>0.970800</td>\n",
       "      <td>1.017162</td>\n",
       "      <td>7.558806</td>\n",
       "      <td>7.700367</td>\n",
       "      <td>7.212912</td>\n",
       "      <td>6.893019</td>\n",
       "      <td>327.514615</td>\n",
       "      <td>322.962970</td>\n",
       "      <td>324.157122</td>\n",
       "      <td>303.345673</td>\n",
       "      <td>104.637486</td>\n",
       "      <td>104.752398</td>\n",
       "      <td>107.728207</td>\n",
       "      <td>101.943889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.156252</td>\n",
       "      <td>59.385804</td>\n",
       "      <td>62.641716</td>\n",
       "      <td>43.901249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.463802</td>\n",
       "      <td>2.666419</td>\n",
       "      <td>2.651999</td>\n",
       "      <td>2.441170</td>\n",
       "      <td>126.393392</td>\n",
       "      <td>126.729459</td>\n",
       "      <td>125.717301</td>\n",
       "      <td>124.94144</td>\n",
       "      <td>1.864668</td>\n",
       "      <td>2.044699</td>\n",
       "      <td>2.016288</td>\n",
       "      <td>1.781807</td>\n",
       "      <td>0.599133</td>\n",
       "      <td>0.621720</td>\n",
       "      <td>0.635711</td>\n",
       "      <td>0.659363</td>\n",
       "      <td>192.600982</td>\n",
       "      <td>200.981292</td>\n",
       "      <td>197.526489</td>\n",
       "      <td>192.734315</td>\n",
       "      <td>51.904956</td>\n",
       "      <td>51.229937</td>\n",
       "      <td>50.170154</td>\n",
       "      <td>44.719701</td>\n",
       "      <td>121.396219</td>\n",
       "      <td>128.995847</td>\n",
       "      <td>135.410689</td>\n",
       "      <td>136.056613</td>\n",
       "      <td>89.555057</td>\n",
       "      <td>89.384120</td>\n",
       "      <td>91.173849</td>\n",
       "      <td>100.264116</td>\n",
       "      <td>86.398003</td>\n",
       "      <td>85.914450</td>\n",
       "      <td>86.599478</td>\n",
       "      <td>93.712026</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>0.020844</td>\n",
       "      <td>0.015971</td>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.083221</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>0.068781</td>\n",
       "      <td>0.389384</td>\n",
       "      <td>0.439634</td>\n",
       "      <td>0.450075</td>\n",
       "      <td>0.393104</td>\n",
       "      <td>0.075921</td>\n",
       "      <td>0.078581</td>\n",
       "      <td>0.082941</td>\n",
       "      <td>0.086341</td>\n",
       "      <td>0.074781</td>\n",
       "      <td>0.080401</td>\n",
       "      <td>0.084501</td>\n",
       "      <td>0.084581</td>\n",
       "      <td>0.914404</td>\n",
       "      <td>0.908764</td>\n",
       "      <td>0.890808</td>\n",
       "      <td>0.860968</td>\n",
       "      <td>1219.854749</td>\n",
       "      <td>68.170248</td>\n",
       "      <td>66.839062</td>\n",
       "      <td>60.021204</td>\n",
       "      <td>3.299373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328.439770</td>\n",
       "      <td>338.156291</td>\n",
       "      <td>344.474791</td>\n",
       "      <td>341.998630</td>\n",
       "      <td>297.207406</td>\n",
       "      <td>308.794148</td>\n",
       "      <td>308.951589</td>\n",
       "      <td>308.477668</td>\n",
       "      <td>316.851613</td>\n",
       "      <td>325.862803</td>\n",
       "      <td>327.170662</td>\n",
       "      <td>319.396092</td>\n",
       "      <td>72.825411</td>\n",
       "      <td>73.447948</td>\n",
       "      <td>68.402466</td>\n",
       "      <td>57.137537</td>\n",
       "      <td>71.443196</td>\n",
       "      <td>58.455762</td>\n",
       "      <td>64.713221</td>\n",
       "      <td>58.438186</td>\n",
       "      <td>150.856393</td>\n",
       "      <td>155.318705</td>\n",
       "      <td>151.184830</td>\n",
       "      <td>147.995390</td>\n",
       "      <td>162.780544</td>\n",
       "      <td>157.492308</td>\n",
       "      <td>156.537048</td>\n",
       "      <td>158.681454</td>\n",
       "      <td>14.230438</td>\n",
       "      <td>14.264986</td>\n",
       "      <td>13.270996</td>\n",
       "      <td>13.457549</td>\n",
       "      <td>5.448946</td>\n",
       "      <td>7.533445</td>\n",
       "      <td>6.783335</td>\n",
       "      <td>5.619021</td>\n",
       "      <td>251.751489</td>\n",
       "      <td>248.731086</td>\n",
       "      <td>245.914311</td>\n",
       "      <td>245.934517</td>\n",
       "      <td>252.476533</td>\n",
       "      <td>263.631042</td>\n",
       "      <td>265.486090</td>\n",
       "      <td>267.184991</td>\n",
       "      <td>255.617850</td>\n",
       "      <td>269.347911</td>\n",
       "      <td>271.757783</td>\n",
       "      <td>261.407396</td>\n",
       "      <td>7.984970</td>\n",
       "      <td>8.599406</td>\n",
       "      <td>7.905971</td>\n",
       "      <td>8.261770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>389.948499</td>\n",
       "      <td>408.922934</td>\n",
       "      <td>411.633049</td>\n",
       "      <td>405.138658</td>\n",
       "      <td>25.765248</td>\n",
       "      <td>25.603052</td>\n",
       "      <td>25.544471</td>\n",
       "      <td>21.310751</td>\n",
       "      <td>14.936449</td>\n",
       "      <td>20.661570</td>\n",
       "      <td>17.855111</td>\n",
       "      <td>16.328227</td>\n",
       "      <td>4.125911</td>\n",
       "      <td>2.161717</td>\n",
       "      <td>2.323464</td>\n",
       "      <td>3.635466</td>\n",
       "      <td>463.419481</td>\n",
       "      <td>480.031178</td>\n",
       "      <td>478.150031</td>\n",
       "      <td>468.980002</td>\n",
       "      <td>140.258485</td>\n",
       "      <td>145.795055</td>\n",
       "      <td>137.239552</td>\n",
       "      <td>140.130610</td>\n",
       "      <td>171.713903</td>\n",
       "      <td>169.423620</td>\n",
       "      <td>169.723759</td>\n",
       "      <td>165.492803</td>\n",
       "      <td>40.140895</td>\n",
       "      <td>42.977442</td>\n",
       "      <td>39.125379</td>\n",
       "      <td>43.840776</td>\n",
       "      <td>254.124029</td>\n",
       "      <td>256.242707</td>\n",
       "      <td>250.025523</td>\n",
       "      <td>249.845070</td>\n",
       "      <td>54.330607</td>\n",
       "      <td>57.411971</td>\n",
       "      <td>55.073186</td>\n",
       "      <td>53.376273</td>\n",
       "      <td>80.793414</td>\n",
       "      <td>86.521393</td>\n",
       "      <td>83.683565</td>\n",
       "      <td>74.913050</td>\n",
       "      <td>16.495594</td>\n",
       "      <td>16.454061</td>\n",
       "      <td>15.812580</td>\n",
       "      <td>15.978601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.283386</td>\n",
       "      <td>113.720168</td>\n",
       "      <td>110.127008</td>\n",
       "      <td>101.982303</td>\n",
       "      <td>291.651671</td>\n",
       "      <td>298.124954</td>\n",
       "      <td>289.321094</td>\n",
       "      <td>284.823024</td>\n",
       "      <td>0.160920</td>\n",
       "      <td>0.155725</td>\n",
       "      <td>0.146147</td>\n",
       "      <td>0.527860</td>\n",
       "      <td>59.722948</td>\n",
       "      <td>65.219829</td>\n",
       "      <td>63.813098</td>\n",
       "      <td>63.505379</td>\n",
       "      <td>11.955164</td>\n",
       "      <td>12.673099</td>\n",
       "      <td>13.284348</td>\n",
       "      <td>12.381172</td>\n",
       "      <td>7.078405</td>\n",
       "      <td>7.070422</td>\n",
       "      <td>7.203753</td>\n",
       "      <td>7.096261</td>\n",
       "      <td>398.019701</td>\n",
       "      <td>408.114237</td>\n",
       "      <td>416.540455</td>\n",
       "      <td>404.588583</td>\n",
       "      <td>120.614894</td>\n",
       "      <td>124.523970</td>\n",
       "      <td>126.902505</td>\n",
       "      <td>125.375109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.356649</td>\n",
       "      <td>95.915385</td>\n",
       "      <td>104.431816</td>\n",
       "      <td>90.809712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.789128</td>\n",
       "      <td>3.031593</td>\n",
       "      <td>3.074987</td>\n",
       "      <td>2.516339</td>\n",
       "      <td>108.477235</td>\n",
       "      <td>109.765267</td>\n",
       "      <td>109.437851</td>\n",
       "      <td>111.36376</td>\n",
       "      <td>2.570254</td>\n",
       "      <td>2.768332</td>\n",
       "      <td>2.720132</td>\n",
       "      <td>2.214701</td>\n",
       "      <td>1.274428</td>\n",
       "      <td>1.394524</td>\n",
       "      <td>1.422827</td>\n",
       "      <td>1.411513</td>\n",
       "      <td>192.646318</td>\n",
       "      <td>196.791224</td>\n",
       "      <td>191.301305</td>\n",
       "      <td>188.400286</td>\n",
       "      <td>213.356445</td>\n",
       "      <td>212.302217</td>\n",
       "      <td>212.347892</td>\n",
       "      <td>198.653570</td>\n",
       "      <td>544.247227</td>\n",
       "      <td>541.494013</td>\n",
       "      <td>558.775335</td>\n",
       "      <td>577.394194</td>\n",
       "      <td>193.124653</td>\n",
       "      <td>195.893924</td>\n",
       "      <td>188.180936</td>\n",
       "      <td>216.291992</td>\n",
       "      <td>172.767523</td>\n",
       "      <td>176.379871</td>\n",
       "      <td>168.247852</td>\n",
       "      <td>171.384224</td>\n",
       "      <td>0.156391</td>\n",
       "      <td>0.150014</td>\n",
       "      <td>0.142863</td>\n",
       "      <td>0.125366</td>\n",
       "      <td>0.295058</td>\n",
       "      <td>0.304395</td>\n",
       "      <td>0.299568</td>\n",
       "      <td>0.278120</td>\n",
       "      <td>1.497320</td>\n",
       "      <td>1.636230</td>\n",
       "      <td>1.630263</td>\n",
       "      <td>1.347140</td>\n",
       "      <td>0.363371</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.384947</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.568344</td>\n",
       "      <td>0.628334</td>\n",
       "      <td>0.660234</td>\n",
       "      <td>0.650457</td>\n",
       "      <td>0.279772</td>\n",
       "      <td>0.287950</td>\n",
       "      <td>0.311885</td>\n",
       "      <td>0.345987</td>\n",
       "      <td>954.733842</td>\n",
       "      <td>267.580450</td>\n",
       "      <td>271.201856</td>\n",
       "      <td>253.938223</td>\n",
       "      <td>32.408353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2258.709000</td>\n",
       "      <td>-2014.045000</td>\n",
       "      <td>-945.808000</td>\n",
       "      <td>-1899.505000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-30.820000</td>\n",
       "      <td>-26.040000</td>\n",
       "      <td>-24.490000</td>\n",
       "      <td>-71.090000</td>\n",
       "      <td>-35.830000</td>\n",
       "      <td>-15.480000</td>\n",
       "      <td>-55.830000</td>\n",
       "      <td>-45.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.411500</td>\n",
       "      <td>86.980500</td>\n",
       "      <td>84.126000</td>\n",
       "      <td>62.685000</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.460000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>32.190000</td>\n",
       "      <td>31.630000</td>\n",
       "      <td>27.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>10.025000</td>\n",
       "      <td>9.810000</td>\n",
       "      <td>8.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>15.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.740000</td>\n",
       "      <td>43.010000</td>\n",
       "      <td>38.580000</td>\n",
       "      <td>25.510000</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>3.290000</td>\n",
       "      <td>17.290000</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>18.930000</td>\n",
       "      <td>18.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.390000</td>\n",
       "      <td>32.460000</td>\n",
       "      <td>32.740000</td>\n",
       "      <td>32.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.530000</td>\n",
       "      <td>41.190000</td>\n",
       "      <td>38.290000</td>\n",
       "      <td>32.370000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.704000</td>\n",
       "      <td>191.640000</td>\n",
       "      <td>192.080000</td>\n",
       "      <td>176.849000</td>\n",
       "      <td>34.310000</td>\n",
       "      <td>32.330000</td>\n",
       "      <td>32.360000</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>96.310000</td>\n",
       "      <td>91.735000</td>\n",
       "      <td>92.140000</td>\n",
       "      <td>87.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.910000</td>\n",
       "      <td>11.610000</td>\n",
       "      <td>11.730000</td>\n",
       "      <td>11.260000</td>\n",
       "      <td>41.030000</td>\n",
       "      <td>40.430000</td>\n",
       "      <td>40.360000</td>\n",
       "      <td>39.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.110000</td>\n",
       "      <td>63.685000</td>\n",
       "      <td>63.730000</td>\n",
       "      <td>61.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>3.635000</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.640000</td>\n",
       "      <td>11.090000</td>\n",
       "      <td>10.410000</td>\n",
       "      <td>8.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.140000</td>\n",
       "      <td>141.530000</td>\n",
       "      <td>138.610000</td>\n",
       "      <td>125.460000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.740000</td>\n",
       "      <td>16.030000</td>\n",
       "      <td>15.660000</td>\n",
       "      <td>56.490000</td>\n",
       "      <td>57.080000</td>\n",
       "      <td>58.240000</td>\n",
       "      <td>56.610000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>92.160000</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>93.830000</td>\n",
       "      <td>91.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.890000</td>\n",
       "      <td>5.960000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>5.380000</td>\n",
       "      <td>114.740000</td>\n",
       "      <td>116.340000</td>\n",
       "      <td>114.660000</td>\n",
       "      <td>105.890000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>2.605000</td>\n",
       "      <td>10.830000</td>\n",
       "      <td>8.810000</td>\n",
       "      <td>9.270000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>863.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>371.060000</td>\n",
       "      <td>365.344500</td>\n",
       "      <td>369.370500</td>\n",
       "      <td>353.466500</td>\n",
       "      <td>118.740000</td>\n",
       "      <td>115.595000</td>\n",
       "      <td>115.860000</td>\n",
       "      <td>112.130000</td>\n",
       "      <td>231.860000</td>\n",
       "      <td>226.815000</td>\n",
       "      <td>228.260000</td>\n",
       "      <td>220.505000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.960000</td>\n",
       "      <td>39.910000</td>\n",
       "      <td>40.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>110.390000</td>\n",
       "      <td>107.560000</td>\n",
       "      <td>109.090000</td>\n",
       "      <td>106.810000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.270000</td>\n",
       "      <td>164.382500</td>\n",
       "      <td>166.110000</td>\n",
       "      <td>162.225000</td>\n",
       "      <td>30.807500</td>\n",
       "      <td>31.132500</td>\n",
       "      <td>30.580000</td>\n",
       "      <td>28.230000</td>\n",
       "      <td>53.290000</td>\n",
       "      <td>54.040000</td>\n",
       "      <td>52.490000</td>\n",
       "      <td>48.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.837500</td>\n",
       "      <td>150.615000</td>\n",
       "      <td>147.940000</td>\n",
       "      <td>142.105000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>3.990000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>372.860000</td>\n",
       "      <td>378.570000</td>\n",
       "      <td>369.900000</td>\n",
       "      <td>353.480000</td>\n",
       "      <td>46.840000</td>\n",
       "      <td>45.810000</td>\n",
       "      <td>46.290000</td>\n",
       "      <td>45.180000</td>\n",
       "      <td>132.387500</td>\n",
       "      <td>130.960000</td>\n",
       "      <td>133.930000</td>\n",
       "      <td>130.490000</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>8.282500</td>\n",
       "      <td>8.110000</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>208.075000</td>\n",
       "      <td>205.837500</td>\n",
       "      <td>207.280000</td>\n",
       "      <td>202.737500</td>\n",
       "      <td>4.060000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>15.030000</td>\n",
       "      <td>15.740000</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>14.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.930000</td>\n",
       "      <td>28.310000</td>\n",
       "      <td>27.710000</td>\n",
       "      <td>25.690000</td>\n",
       "      <td>251.670000</td>\n",
       "      <td>250.660000</td>\n",
       "      <td>248.990000</td>\n",
       "      <td>236.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>437.500000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>434.500000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>179.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.070000</td>\n",
       "      <td>119.560000</td>\n",
       "      <td>122.070000</td>\n",
       "      <td>140.010000</td>\n",
       "      <td>122.070000</td>\n",
       "      <td>122.070000</td>\n",
       "      <td>122.070000</td>\n",
       "      <td>140.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1807.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27731.088000</td>\n",
       "      <td>35145.834000</td>\n",
       "      <td>33543.624000</td>\n",
       "      <td>38805.617000</td>\n",
       "      <td>7376.710000</td>\n",
       "      <td>8157.780000</td>\n",
       "      <td>10752.560000</td>\n",
       "      <td>10427.460000</td>\n",
       "      <td>8362.360000</td>\n",
       "      <td>9667.130000</td>\n",
       "      <td>14007.340000</td>\n",
       "      <td>10310.760000</td>\n",
       "      <td>13724.380000</td>\n",
       "      <td>15371.040000</td>\n",
       "      <td>13095.360000</td>\n",
       "      <td>8464.030000</td>\n",
       "      <td>3775.110000</td>\n",
       "      <td>2812.040000</td>\n",
       "      <td>5337.040000</td>\n",
       "      <td>4428.460000</td>\n",
       "      <td>6431.330000</td>\n",
       "      <td>7400.660000</td>\n",
       "      <td>10752.560000</td>\n",
       "      <td>10389.240000</td>\n",
       "      <td>4729.740000</td>\n",
       "      <td>4557.140000</td>\n",
       "      <td>4961.330000</td>\n",
       "      <td>4429.880000</td>\n",
       "      <td>1466.030000</td>\n",
       "      <td>1196.430000</td>\n",
       "      <td>928.490000</td>\n",
       "      <td>927.410000</td>\n",
       "      <td>342.860000</td>\n",
       "      <td>916.240000</td>\n",
       "      <td>502.090000</td>\n",
       "      <td>339.840000</td>\n",
       "      <td>10643.380000</td>\n",
       "      <td>7674.780000</td>\n",
       "      <td>11039.910000</td>\n",
       "      <td>11099.260000</td>\n",
       "      <td>7366.580000</td>\n",
       "      <td>8133.660000</td>\n",
       "      <td>8014.430000</td>\n",
       "      <td>9382.580000</td>\n",
       "      <td>8314.760000</td>\n",
       "      <td>9284.740000</td>\n",
       "      <td>13950.040000</td>\n",
       "      <td>10223.430000</td>\n",
       "      <td>628.560000</td>\n",
       "      <td>544.630000</td>\n",
       "      <td>516.910000</td>\n",
       "      <td>808.490000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8432.990000</td>\n",
       "      <td>10936.730000</td>\n",
       "      <td>13980.060000</td>\n",
       "      <td>11495.310000</td>\n",
       "      <td>5900.660000</td>\n",
       "      <td>5490.280000</td>\n",
       "      <td>5681.540000</td>\n",
       "      <td>4244.530000</td>\n",
       "      <td>1023.210000</td>\n",
       "      <td>2372.510000</td>\n",
       "      <td>1390.880000</td>\n",
       "      <td>1635.710000</td>\n",
       "      <td>800.890000</td>\n",
       "      <td>370.130000</td>\n",
       "      <td>394.930000</td>\n",
       "      <td>787.790000</td>\n",
       "      <td>10674.030000</td>\n",
       "      <td>11365.310000</td>\n",
       "      <td>14043.060000</td>\n",
       "      <td>11517.730000</td>\n",
       "      <td>6626.930000</td>\n",
       "      <td>9324.660000</td>\n",
       "      <td>10696.230000</td>\n",
       "      <td>10598.830000</td>\n",
       "      <td>4693.860000</td>\n",
       "      <td>4455.830000</td>\n",
       "      <td>6274.190000</td>\n",
       "      <td>5463.780000</td>\n",
       "      <td>1872.340000</td>\n",
       "      <td>1983.010000</td>\n",
       "      <td>2433.060000</td>\n",
       "      <td>4318.280000</td>\n",
       "      <td>7454.630000</td>\n",
       "      <td>9669.910000</td>\n",
       "      <td>10830.160000</td>\n",
       "      <td>10796.290000</td>\n",
       "      <td>5459.560000</td>\n",
       "      <td>5800.930000</td>\n",
       "      <td>4309.290000</td>\n",
       "      <td>3819.830000</td>\n",
       "      <td>5647.160000</td>\n",
       "      <td>6141.880000</td>\n",
       "      <td>5645.860000</td>\n",
       "      <td>5689.760000</td>\n",
       "      <td>1351.110000</td>\n",
       "      <td>1136.080000</td>\n",
       "      <td>1394.890000</td>\n",
       "      <td>1431.960000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5712.110000</td>\n",
       "      <td>6745.760000</td>\n",
       "      <td>5957.140000</td>\n",
       "      <td>5956.660000</td>\n",
       "      <td>7716.140000</td>\n",
       "      <td>9699.010000</td>\n",
       "      <td>10830.380000</td>\n",
       "      <td>10796.590000</td>\n",
       "      <td>19.760000</td>\n",
       "      <td>21.330000</td>\n",
       "      <td>16.860000</td>\n",
       "      <td>62.380000</td>\n",
       "      <td>6789.410000</td>\n",
       "      <td>5289.540000</td>\n",
       "      <td>4127.010000</td>\n",
       "      <td>5057.740000</td>\n",
       "      <td>1362.940000</td>\n",
       "      <td>1495.940000</td>\n",
       "      <td>2327.510000</td>\n",
       "      <td>1005.230000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>35190.000000</td>\n",
       "      <td>40335.000000</td>\n",
       "      <td>45320.000000</td>\n",
       "      <td>37235.000000</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>4449.000000</td>\n",
       "      <td>3399.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>4449.000000</td>\n",
       "      <td>3399.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>7546.000000</td>\n",
       "      <td>4365.000000</td>\n",
       "      <td>4076.000000</td>\n",
       "      <td>4061.000000</td>\n",
       "      <td>10285.900000</td>\n",
       "      <td>7873.550000</td>\n",
       "      <td>11117.610000</td>\n",
       "      <td>8993.950000</td>\n",
       "      <td>45735.400000</td>\n",
       "      <td>28144.120000</td>\n",
       "      <td>30036.060000</td>\n",
       "      <td>39221.270000</td>\n",
       "      <td>6362.280000</td>\n",
       "      <td>4980.900000</td>\n",
       "      <td>3716.900000</td>\n",
       "      <td>13884.310000</td>\n",
       "      <td>6433.760000</td>\n",
       "      <td>4809.360000</td>\n",
       "      <td>3483.170000</td>\n",
       "      <td>3467.170000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4337.000000</td>\n",
       "      <td>12916.220000</td>\n",
       "      <td>9165.600000</td>\n",
       "      <td>11166.210000</td>\n",
       "      <td>2618.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        circle_id  loc_og_t2o_mou  std_og_t2o_mou  loc_ic_t2o_mou  \\\n",
       "count     99999.0         98981.0         98981.0         98981.0   \n",
       "unique        NaN             NaN             NaN             NaN   \n",
       "top           NaN             NaN             NaN             NaN   \n",
       "freq          NaN             NaN             NaN             NaN   \n",
       "mean        109.0             0.0             0.0             0.0   \n",
       "std           0.0             0.0             0.0             0.0   \n",
       "min         109.0             0.0             0.0             0.0   \n",
       "25%         109.0             0.0             0.0             0.0   \n",
       "50%         109.0             0.0             0.0             0.0   \n",
       "75%         109.0             0.0             0.0             0.0   \n",
       "max         109.0             0.0             0.0             0.0   \n",
       "\n",
       "       last_date_of_month_6 last_date_of_month_7 last_date_of_month_8  \\\n",
       "count                 99999                99398                98899   \n",
       "unique                    1                    1                    1   \n",
       "top               6/30/2014            7/31/2014            8/31/2014   \n",
       "freq                  99999                99398                98899   \n",
       "mean                    NaN                  NaN                  NaN   \n",
       "std                     NaN                  NaN                  NaN   \n",
       "min                     NaN                  NaN                  NaN   \n",
       "25%                     NaN                  NaN                  NaN   \n",
       "50%                     NaN                  NaN                  NaN   \n",
       "75%                     NaN                  NaN                  NaN   \n",
       "max                     NaN                  NaN                  NaN   \n",
       "\n",
       "       last_date_of_month_9        arpu_6        arpu_7        arpu_8  \\\n",
       "count                 98340  99999.000000  99999.000000  99999.000000   \n",
       "unique                    1           NaN           NaN           NaN   \n",
       "top               9/30/2014           NaN           NaN           NaN   \n",
       "freq                  98340           NaN           NaN           NaN   \n",
       "mean                    NaN    282.987358    278.536648    279.154731   \n",
       "std                     NaN    328.439770    338.156291    344.474791   \n",
       "min                     NaN  -2258.709000  -2014.045000   -945.808000   \n",
       "25%                     NaN     93.411500     86.980500     84.126000   \n",
       "50%                     NaN    197.704000    191.640000    192.080000   \n",
       "75%                     NaN    371.060000    365.344500    369.370500   \n",
       "max                     NaN  27731.088000  35145.834000  33543.624000   \n",
       "\n",
       "              arpu_9   onnet_mou_6   onnet_mou_7   onnet_mou_8   onnet_mou_9  \\\n",
       "count   99999.000000  96062.000000  96140.000000  94621.000000  92254.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean      261.645069    132.395875    133.670805    133.018098    130.302327   \n",
       "std       341.998630    297.207406    308.794148    308.951589    308.477668   \n",
       "min     -1899.505000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        62.685000      7.380000      6.660000      6.460000      5.330000   \n",
       "50%       176.849000     34.310000     32.330000     32.360000     29.840000   \n",
       "75%       353.466500    118.740000    115.595000    115.860000    112.130000   \n",
       "max     38805.617000   7376.710000   8157.780000  10752.560000  10427.460000   \n",
       "\n",
       "        offnet_mou_6  offnet_mou_7  offnet_mou_8  offnet_mou_9  roam_ic_mou_6  \\\n",
       "count   96062.000000  96140.000000  94621.000000  92254.000000   96062.000000   \n",
       "unique           NaN           NaN           NaN           NaN            NaN   \n",
       "top              NaN           NaN           NaN           NaN            NaN   \n",
       "freq             NaN           NaN           NaN           NaN            NaN   \n",
       "mean      197.935577    197.045133    196.574803    190.337222       9.950013   \n",
       "std       316.851613    325.862803    327.170662    319.396092      72.825411   \n",
       "min         0.000000      0.000000      0.000000      0.000000       0.000000   \n",
       "25%        34.730000     32.190000     31.630000     27.130000       0.000000   \n",
       "50%        96.310000     91.735000     92.140000     87.290000       0.000000   \n",
       "75%       231.860000    226.815000    228.260000    220.505000       0.000000   \n",
       "max      8362.360000   9667.130000  14007.340000  10310.760000   13724.380000   \n",
       "\n",
       "        roam_ic_mou_7  roam_ic_mou_8  roam_ic_mou_9  roam_og_mou_6  \\\n",
       "count    96140.000000   94621.000000   92254.000000   96062.000000   \n",
       "unique            NaN            NaN            NaN            NaN   \n",
       "top               NaN            NaN            NaN            NaN   \n",
       "freq              NaN            NaN            NaN            NaN   \n",
       "mean         7.149898       7.292981       6.343841      13.911337   \n",
       "std         73.447948      68.402466      57.137537      71.443196   \n",
       "min          0.000000       0.000000       0.000000       0.000000   \n",
       "25%          0.000000       0.000000       0.000000       0.000000   \n",
       "50%          0.000000       0.000000       0.000000       0.000000   \n",
       "75%          0.000000       0.000000       0.000000       0.000000   \n",
       "max      15371.040000   13095.360000    8464.030000    3775.110000   \n",
       "\n",
       "        roam_og_mou_7  roam_og_mou_8  roam_og_mou_9  loc_og_t2t_mou_6  \\\n",
       "count    96140.000000   94621.000000   92254.000000      96062.000000   \n",
       "unique            NaN            NaN            NaN               NaN   \n",
       "top               NaN            NaN            NaN               NaN   \n",
       "freq              NaN            NaN            NaN               NaN   \n",
       "mean         9.818732       9.971890       8.555519         47.100763   \n",
       "std         58.455762      64.713221      58.438186        150.856393   \n",
       "min          0.000000       0.000000       0.000000          0.000000   \n",
       "25%          0.000000       0.000000       0.000000          1.660000   \n",
       "50%          0.000000       0.000000       0.000000         11.910000   \n",
       "75%          0.000000       0.000000       0.000000         40.960000   \n",
       "max       2812.040000    5337.040000    4428.460000       6431.330000   \n",
       "\n",
       "        loc_og_t2t_mou_7  loc_og_t2t_mou_8  loc_og_t2t_mou_9  \\\n",
       "count       96140.000000      94621.000000      92254.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           46.473010         45.887806         44.584446   \n",
       "std           155.318705        151.184830        147.995390   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             1.630000          1.600000          1.360000   \n",
       "50%            11.610000         11.730000         11.260000   \n",
       "75%            39.910000         40.110000         39.280000   \n",
       "max          7400.660000      10752.560000      10389.240000   \n",
       "\n",
       "        loc_og_t2m_mou_6  loc_og_t2m_mou_7  loc_og_t2m_mou_8  \\\n",
       "count       96062.000000      96140.000000      94621.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           93.342088         91.397131         91.755128   \n",
       "std           162.780544        157.492308        156.537048   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             9.880000         10.025000          9.810000   \n",
       "50%            41.030000         40.430000         40.360000   \n",
       "75%           110.390000        107.560000        109.090000   \n",
       "max          4729.740000       4557.140000       4961.330000   \n",
       "\n",
       "        loc_og_t2m_mou_9  loc_og_t2f_mou_6  loc_og_t2f_mou_7  \\\n",
       "count       92254.000000      96062.000000      96140.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           90.463192          3.751013          3.792985   \n",
       "std           158.681454         14.230438         14.264986   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             8.810000          0.000000          0.000000   \n",
       "50%            39.120000          0.000000          0.000000   \n",
       "75%           106.810000          2.080000          2.090000   \n",
       "max          4429.880000       1466.030000       1196.430000   \n",
       "\n",
       "        loc_og_t2f_mou_8  loc_og_t2f_mou_9  loc_og_t2c_mou_6  \\\n",
       "count       94621.000000      92254.000000      96062.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean            3.677991          3.655123          1.123056   \n",
       "std            13.270996         13.457549          5.448946   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             0.000000          0.000000          0.000000   \n",
       "75%             2.040000          1.940000          0.000000   \n",
       "max           928.490000        927.410000        342.860000   \n",
       "\n",
       "        loc_og_t2c_mou_7  loc_og_t2c_mou_8  loc_og_t2c_mou_9  loc_og_mou_6  \\\n",
       "count       96140.000000      94621.000000      92254.000000  96062.000000   \n",
       "unique               NaN               NaN               NaN           NaN   \n",
       "top                  NaN               NaN               NaN           NaN   \n",
       "freq                 NaN               NaN               NaN           NaN   \n",
       "mean            1.368500          1.433821          1.232726    144.201175   \n",
       "std             7.533445          6.783335          5.619021    251.751489   \n",
       "min             0.000000          0.000000          0.000000      0.000000   \n",
       "25%             0.000000          0.000000          0.000000     17.110000   \n",
       "50%             0.000000          0.000000          0.000000     65.110000   \n",
       "75%             0.000000          0.000000          0.000000    168.270000   \n",
       "max           916.240000        502.090000        339.840000  10643.380000   \n",
       "\n",
       "        loc_og_mou_7  loc_og_mou_8  loc_og_mou_9  std_og_t2t_mou_6  \\\n",
       "count   96140.000000  94621.000000  92254.000000      96062.000000   \n",
       "unique           NaN           NaN           NaN               NaN   \n",
       "top              NaN           NaN           NaN               NaN   \n",
       "freq             NaN           NaN           NaN               NaN   \n",
       "mean      141.670476    141.328209    138.709970         79.829870   \n",
       "std       248.731086    245.914311    245.934517        252.476533   \n",
       "min         0.000000      0.000000      0.000000          0.000000   \n",
       "25%        17.480000     17.110000     15.560000          0.000000   \n",
       "50%        63.685000     63.730000     61.840000          0.000000   \n",
       "75%       164.382500    166.110000    162.225000         30.807500   \n",
       "max      7674.780000  11039.910000  11099.260000       7366.580000   \n",
       "\n",
       "        std_og_t2t_mou_7  std_og_t2t_mou_8  std_og_t2t_mou_9  \\\n",
       "count       96140.000000      94621.000000      92254.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           83.299598         83.282673         82.342919   \n",
       "std           263.631042        265.486090        267.184991   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             0.000000          0.000000          0.000000   \n",
       "75%            31.132500         30.580000         28.230000   \n",
       "max          8133.660000       8014.430000       9382.580000   \n",
       "\n",
       "        std_og_t2m_mou_6  std_og_t2m_mou_7  std_og_t2m_mou_8  \\\n",
       "count       96062.000000      96140.000000      94621.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           87.299624         90.804137         89.838390   \n",
       "std           255.617850        269.347911        271.757783   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             3.950000          3.635000          3.310000   \n",
       "75%            53.290000         54.040000         52.490000   \n",
       "max          8314.760000       9284.740000      13950.040000   \n",
       "\n",
       "        std_og_t2m_mou_9  std_og_t2f_mou_6  std_og_t2f_mou_7  \\\n",
       "count       92254.000000      96062.000000      96140.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           86.276622          1.129011          1.115010   \n",
       "std           261.407396          7.984970          8.599406   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             2.500000          0.000000          0.000000   \n",
       "75%            48.560000          0.000000          0.000000   \n",
       "max         10223.430000        628.560000        544.630000   \n",
       "\n",
       "        std_og_t2f_mou_8  std_og_t2f_mou_9  std_og_t2c_mou_6  \\\n",
       "count       94621.000000      92254.000000           96062.0   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean            1.067792          1.042362               0.0   \n",
       "std             7.905971          8.261770               0.0   \n",
       "min             0.000000          0.000000               0.0   \n",
       "25%             0.000000          0.000000               0.0   \n",
       "50%             0.000000          0.000000               0.0   \n",
       "75%             0.000000          0.000000               0.0   \n",
       "max           516.910000        808.490000               0.0   \n",
       "\n",
       "        std_og_t2c_mou_7  std_og_t2c_mou_8  std_og_t2c_mou_9  std_og_mou_6  \\\n",
       "count            96140.0           94621.0           92254.0  96062.000000   \n",
       "unique               NaN               NaN               NaN           NaN   \n",
       "top                  NaN               NaN               NaN           NaN   \n",
       "freq                 NaN               NaN               NaN           NaN   \n",
       "mean                 0.0               0.0               0.0    168.261218   \n",
       "std                  0.0               0.0               0.0    389.948499   \n",
       "min                  0.0               0.0               0.0      0.000000   \n",
       "25%                  0.0               0.0               0.0      0.000000   \n",
       "50%                  0.0               0.0               0.0     11.640000   \n",
       "75%                  0.0               0.0               0.0    144.837500   \n",
       "max                  0.0               0.0               0.0   8432.990000   \n",
       "\n",
       "        std_og_mou_7  std_og_mou_8  std_og_mou_9  isd_og_mou_6  isd_og_mou_7  \\\n",
       "count   96140.000000  94621.000000  92254.000000  96062.000000  96140.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean      175.221436    174.191498    169.664466      0.798277      0.776572   \n",
       "std       408.922934    411.633049    405.138658     25.765248     25.603052   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        11.090000     10.410000      8.410000      0.000000      0.000000   \n",
       "75%       150.615000    147.940000    142.105000      0.000000      0.000000   \n",
       "max     10936.730000  13980.060000  11495.310000   5900.660000   5490.280000   \n",
       "\n",
       "        isd_og_mou_8  isd_og_mou_9  spl_og_mou_6  spl_og_mou_7  spl_og_mou_8  \\\n",
       "count   94621.000000  92254.000000  96062.000000  96140.000000  94621.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.791247      0.723892      3.916811      4.978279      5.053769   \n",
       "std        25.544471     21.310751     14.936449     20.661570     17.855111   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      2.430000      3.710000      3.990000   \n",
       "max      5681.540000   4244.530000   1023.210000   2372.510000   1390.880000   \n",
       "\n",
       "        spl_og_mou_9   og_others_6   og_others_7   og_others_8   og_others_9  \\\n",
       "count   92254.000000  96062.000000  96140.000000  94621.000000  92254.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        4.412767      0.454157      0.030235      0.033372      0.047456   \n",
       "std        16.328227      4.125911      2.161717      2.323464      3.635466   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         3.230000      0.000000      0.000000      0.000000      0.000000   \n",
       "max      1635.710000    800.890000    370.130000    394.930000    787.790000   \n",
       "\n",
       "        total_og_mou_6  total_og_mou_7  total_og_mou_8  total_og_mou_9  \\\n",
       "count     99999.000000    99999.000000    99999.000000    99999.000000   \n",
       "unique             NaN             NaN             NaN             NaN   \n",
       "top                NaN             NaN             NaN             NaN   \n",
       "freq               NaN             NaN             NaN             NaN   \n",
       "mean        305.133424      310.231175      304.119513      289.279198   \n",
       "std         463.419481      480.031178      478.150031      468.980002   \n",
       "min           0.000000        0.000000        0.000000        0.000000   \n",
       "25%          44.740000       43.010000       38.580000       25.510000   \n",
       "50%         145.140000      141.530000      138.610000      125.460000   \n",
       "75%         372.860000      378.570000      369.900000      353.480000   \n",
       "max       10674.030000    11365.310000    14043.060000    11517.730000   \n",
       "\n",
       "        loc_ic_t2t_mou_6  loc_ic_t2t_mou_7  loc_ic_t2t_mou_8  \\\n",
       "count       96062.000000      96140.000000      94621.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           47.922365         47.990520         47.211362   \n",
       "std           140.258485        145.795055        137.239552   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             2.990000          3.230000          3.280000   \n",
       "50%            15.690000         15.740000         16.030000   \n",
       "75%            46.840000         45.810000         46.290000   \n",
       "max          6626.930000       9324.660000      10696.230000   \n",
       "\n",
       "        loc_ic_t2t_mou_9  loc_ic_t2m_mou_6  loc_ic_t2m_mou_7  \\\n",
       "count       92254.000000      96062.000000      96140.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           46.281794        107.475650        107.120493   \n",
       "std           140.130610        171.713903        169.423620   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             3.290000         17.290000         18.590000   \n",
       "50%            15.660000         56.490000         57.080000   \n",
       "75%            45.180000        132.387500        130.960000   \n",
       "max         10598.830000       4693.860000       4455.830000   \n",
       "\n",
       "        loc_ic_t2m_mou_8  loc_ic_t2m_mou_9  loc_ic_t2f_mou_6  \\\n",
       "count       94621.000000      92254.000000      96062.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean          108.460515        106.155471         12.084305   \n",
       "std           169.723759        165.492803         40.140895   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%            18.930000         18.560000          0.000000   \n",
       "50%            58.240000         56.610000          0.880000   \n",
       "75%           133.930000        130.490000          8.140000   \n",
       "max          6274.190000       5463.780000       1872.340000   \n",
       "\n",
       "        loc_ic_t2f_mou_7  loc_ic_t2f_mou_8  loc_ic_t2f_mou_9  loc_ic_mou_6  \\\n",
       "count       96140.000000      94621.000000      92254.000000  96062.000000   \n",
       "unique               NaN               NaN               NaN           NaN   \n",
       "top                  NaN               NaN               NaN           NaN   \n",
       "freq                 NaN               NaN               NaN           NaN   \n",
       "mean           12.599697         11.751834         12.173105    167.491059   \n",
       "std            42.977442         39.125379         43.840776    254.124029   \n",
       "min             0.000000          0.000000          0.000000      0.000000   \n",
       "25%             0.000000          0.000000          0.000000     30.390000   \n",
       "50%             0.930000          0.930000          0.960000     92.160000   \n",
       "75%             8.282500          8.110000          8.140000    208.075000   \n",
       "max          1983.010000       2433.060000       4318.280000   7454.630000   \n",
       "\n",
       "        loc_ic_mou_7  loc_ic_mou_8  loc_ic_mou_9  std_ic_t2t_mou_6  \\\n",
       "count   96140.000000  94621.000000  92254.000000      96062.000000   \n",
       "unique           NaN           NaN           NaN               NaN   \n",
       "top              NaN           NaN           NaN               NaN   \n",
       "freq             NaN           NaN           NaN               NaN   \n",
       "mean      167.719540    167.432575    164.619293          9.575993   \n",
       "std       256.242707    250.025523    249.845070         54.330607   \n",
       "min         0.000000      0.000000      0.000000          0.000000   \n",
       "25%        32.460000     32.740000     32.290000          0.000000   \n",
       "50%        92.550000     93.830000     91.640000          0.000000   \n",
       "75%       205.837500    207.280000    202.737500          4.060000   \n",
       "max      9669.910000  10830.160000  10796.290000       5459.560000   \n",
       "\n",
       "        std_ic_t2t_mou_7  std_ic_t2t_mou_8  std_ic_t2t_mou_9  \\\n",
       "count       96140.000000      94621.000000      92254.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           10.011904          9.883921          9.432479   \n",
       "std            57.411971         55.073186         53.376273   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             0.000000          0.000000          0.000000   \n",
       "75%             4.230000          4.080000          3.510000   \n",
       "max          5800.930000       4309.290000       3819.830000   \n",
       "\n",
       "        std_ic_t2m_mou_6  std_ic_t2m_mou_7  std_ic_t2m_mou_8  \\\n",
       "count       96062.000000      96140.000000      94621.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           20.722240         21.656415         21.183211   \n",
       "std            80.793414         86.521393         83.683565   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             2.030000          2.040000          2.030000   \n",
       "75%            15.030000         15.740000         15.360000   \n",
       "max          5647.160000       6141.880000       5645.860000   \n",
       "\n",
       "        std_ic_t2m_mou_9  std_ic_t2f_mou_6  std_ic_t2f_mou_7  \\\n",
       "count       92254.000000      96062.000000      96140.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean           19.620913          2.156397          2.216923   \n",
       "std            74.913050         16.495594         16.454061   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             1.740000          0.000000          0.000000   \n",
       "75%            14.260000          0.000000          0.000000   \n",
       "max          5689.760000       1351.110000       1136.080000   \n",
       "\n",
       "        std_ic_t2f_mou_8  std_ic_t2f_mou_9  std_ic_t2o_mou_6  \\\n",
       "count       94621.000000      92254.000000           96062.0   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean            2.085004          2.173419               0.0   \n",
       "std            15.812580         15.978601               0.0   \n",
       "min             0.000000          0.000000               0.0   \n",
       "25%             0.000000          0.000000               0.0   \n",
       "50%             0.000000          0.000000               0.0   \n",
       "75%             0.000000          0.000000               0.0   \n",
       "max          1394.890000       1431.960000               0.0   \n",
       "\n",
       "        std_ic_t2o_mou_7  std_ic_t2o_mou_8  std_ic_t2o_mou_9  std_ic_mou_6  \\\n",
       "count            96140.0           94621.0           92254.0  96062.000000   \n",
       "unique               NaN               NaN               NaN           NaN   \n",
       "top                  NaN               NaN               NaN           NaN   \n",
       "freq                 NaN               NaN               NaN           NaN   \n",
       "mean                 0.0               0.0               0.0     32.457179   \n",
       "std                  0.0               0.0               0.0    106.283386   \n",
       "min                  0.0               0.0               0.0      0.000000   \n",
       "25%                  0.0               0.0               0.0      0.000000   \n",
       "50%                  0.0               0.0               0.0      5.890000   \n",
       "75%                  0.0               0.0               0.0     26.930000   \n",
       "max                  0.0               0.0               0.0   5712.110000   \n",
       "\n",
       "        std_ic_mou_7  std_ic_mou_8  std_ic_mou_9  total_ic_mou_6  \\\n",
       "count   96140.000000  94621.000000  92254.000000    99999.000000   \n",
       "unique           NaN           NaN           NaN             NaN   \n",
       "top              NaN           NaN           NaN             NaN   \n",
       "freq             NaN           NaN           NaN             NaN   \n",
       "mean       33.887833     33.154735     31.229344      200.130037   \n",
       "std       113.720168    110.127008    101.982303      291.651671   \n",
       "min         0.000000      0.000000      0.000000        0.000000   \n",
       "25%         0.000000      0.010000      0.000000       38.530000   \n",
       "50%         5.960000      5.880000      5.380000      114.740000   \n",
       "75%        28.310000     27.710000     25.690000      251.670000   \n",
       "max      6745.760000   5957.140000   5956.660000     7716.140000   \n",
       "\n",
       "        total_ic_mou_7  total_ic_mou_8  total_ic_mou_9  spl_ic_mou_6  \\\n",
       "count     99999.000000    99999.000000    99999.000000  96062.000000   \n",
       "unique             NaN             NaN             NaN           NaN   \n",
       "top                NaN             NaN             NaN           NaN   \n",
       "freq               NaN             NaN             NaN           NaN   \n",
       "mean        202.853055      198.750783      189.214260      0.061557   \n",
       "std         298.124954      289.321094      284.823024      0.160920   \n",
       "min           0.000000        0.000000        0.000000      0.000000   \n",
       "25%          41.190000       38.290000       32.370000      0.000000   \n",
       "50%         116.340000      114.660000      105.890000      0.000000   \n",
       "75%         250.660000      248.990000      236.320000      0.000000   \n",
       "max        9699.010000    10830.380000    10796.590000     19.760000   \n",
       "\n",
       "        spl_ic_mou_7  spl_ic_mou_8  spl_ic_mou_9  isd_ic_mou_6  isd_ic_mou_7  \\\n",
       "count   96140.000000  94621.000000  92254.000000  96062.000000  96140.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.033585      0.040361      0.163137      7.460608      8.334936   \n",
       "std         0.155725      0.146147      0.527860     59.722948     65.219829   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.060000      0.000000      0.000000   \n",
       "max        21.330000     16.860000     62.380000   6789.410000   5289.540000   \n",
       "\n",
       "        isd_ic_mou_8  isd_ic_mou_9   ic_others_6   ic_others_7   ic_others_8  \\\n",
       "count   94621.000000  92254.000000  96062.000000  96140.000000  94621.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        8.442001      8.063003      0.854656      1.012960      0.970800   \n",
       "std        63.813098     63.505379     11.955164     12.673099     13.284348   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max      4127.010000   5057.740000   1362.940000   1495.940000   2327.510000   \n",
       "\n",
       "         ic_others_9  total_rech_num_6  total_rech_num_7  total_rech_num_8  \\\n",
       "count   92254.000000      99999.000000      99999.000000      99999.000000   \n",
       "unique           NaN               NaN               NaN               NaN   \n",
       "top              NaN               NaN               NaN               NaN   \n",
       "freq             NaN               NaN               NaN               NaN   \n",
       "mean        1.017162          7.558806          7.700367          7.212912   \n",
       "std        12.381172          7.078405          7.070422          7.203753   \n",
       "min         0.000000          0.000000          0.000000          0.000000   \n",
       "25%         0.000000          3.000000          3.000000          3.000000   \n",
       "50%         0.000000          6.000000          6.000000          5.000000   \n",
       "75%         0.000000          9.000000         10.000000          9.000000   \n",
       "max      1005.230000        307.000000        138.000000        196.000000   \n",
       "\n",
       "        total_rech_num_9  total_rech_amt_6  total_rech_amt_7  \\\n",
       "count       99999.000000      99999.000000      99999.000000   \n",
       "unique               NaN               NaN               NaN   \n",
       "top                  NaN               NaN               NaN   \n",
       "freq                 NaN               NaN               NaN   \n",
       "mean            6.893019        327.514615        322.962970   \n",
       "std             7.096261        398.019701        408.114237   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             3.000000        109.000000        100.000000   \n",
       "50%             5.000000        230.000000        220.000000   \n",
       "75%             9.000000        437.500000        428.000000   \n",
       "max           131.000000      35190.000000      40335.000000   \n",
       "\n",
       "        total_rech_amt_8  total_rech_amt_9  max_rech_amt_6  max_rech_amt_7  \\\n",
       "count       99999.000000      99999.000000    99999.000000    99999.000000   \n",
       "unique               NaN               NaN             NaN             NaN   \n",
       "top                  NaN               NaN             NaN             NaN   \n",
       "freq                 NaN               NaN             NaN             NaN   \n",
       "mean          324.157122        303.345673      104.637486      104.752398   \n",
       "std           416.540455        404.588583      120.614894      124.523970   \n",
       "min             0.000000          0.000000        0.000000        0.000000   \n",
       "25%            90.000000         52.000000       30.000000       30.000000   \n",
       "50%           225.000000        200.000000      110.000000      110.000000   \n",
       "75%           434.500000        415.000000      120.000000      128.000000   \n",
       "max         45320.000000      37235.000000     4010.000000     4010.000000   \n",
       "\n",
       "        max_rech_amt_8  max_rech_amt_9 date_of_last_rech_6  \\\n",
       "count     99999.000000    99999.000000               98392   \n",
       "unique             NaN             NaN                  30   \n",
       "top                NaN             NaN           6/30/2014   \n",
       "freq               NaN             NaN               16960   \n",
       "mean        107.728207      101.943889                 NaN   \n",
       "std         126.902505      125.375109                 NaN   \n",
       "min           0.000000        0.000000                 NaN   \n",
       "25%          30.000000       28.000000                 NaN   \n",
       "50%          98.000000       61.000000                 NaN   \n",
       "75%         144.000000      144.000000                 NaN   \n",
       "max        4449.000000     3399.000000                 NaN   \n",
       "\n",
       "       date_of_last_rech_7 date_of_last_rech_8 date_of_last_rech_9  \\\n",
       "count                98232               96377               95239   \n",
       "unique                  31                  31                  30   \n",
       "top              7/31/2014           8/31/2014           9/29/2014   \n",
       "freq                 17288               14706               22623   \n",
       "mean                   NaN                 NaN                 NaN   \n",
       "std                    NaN                 NaN                 NaN   \n",
       "min                    NaN                 NaN                 NaN   \n",
       "25%                    NaN                 NaN                 NaN   \n",
       "50%                    NaN                 NaN                 NaN   \n",
       "75%                    NaN                 NaN                 NaN   \n",
       "max                    NaN                 NaN                 NaN   \n",
       "\n",
       "        last_day_rch_amt_6  last_day_rch_amt_7  last_day_rch_amt_8  \\\n",
       "count         99999.000000        99999.000000        99999.000000   \n",
       "unique                 NaN                 NaN                 NaN   \n",
       "top                    NaN                 NaN                 NaN   \n",
       "freq                   NaN                 NaN                 NaN   \n",
       "mean             63.156252           59.385804           62.641716   \n",
       "std              97.356649           95.915385          104.431816   \n",
       "min               0.000000            0.000000            0.000000   \n",
       "25%               0.000000            0.000000            0.000000   \n",
       "50%              30.000000           30.000000           30.000000   \n",
       "75%             110.000000          110.000000          130.000000   \n",
       "max            4010.000000         4010.000000         4449.000000   \n",
       "\n",
       "        last_day_rch_amt_9 date_of_last_rech_data_6 date_of_last_rech_data_7  \\\n",
       "count         99999.000000                    25153                    25571   \n",
       "unique                 NaN                       30                       31   \n",
       "top                    NaN                6/30/2014                7/31/2014   \n",
       "freq                   NaN                     1888                     1813   \n",
       "mean             43.901249                      NaN                      NaN   \n",
       "std              90.809712                      NaN                      NaN   \n",
       "min               0.000000                      NaN                      NaN   \n",
       "25%               0.000000                      NaN                      NaN   \n",
       "50%               0.000000                      NaN                      NaN   \n",
       "75%              50.000000                      NaN                      NaN   \n",
       "max            3399.000000                      NaN                      NaN   \n",
       "\n",
       "       date_of_last_rech_data_8 date_of_last_rech_data_9  total_rech_data_6  \\\n",
       "count                     26339                    25922       25153.000000   \n",
       "unique                       31                       30                NaN   \n",
       "top                   8/31/2014                9/29/2014                NaN   \n",
       "freq                       1998                     2329                NaN   \n",
       "mean                        NaN                      NaN           2.463802   \n",
       "std                         NaN                      NaN           2.789128   \n",
       "min                         NaN                      NaN           1.000000   \n",
       "25%                         NaN                      NaN           1.000000   \n",
       "50%                         NaN                      NaN           1.000000   \n",
       "75%                         NaN                      NaN           3.000000   \n",
       "max                         NaN                      NaN          61.000000   \n",
       "\n",
       "        total_rech_data_7  total_rech_data_8  total_rech_data_9  \\\n",
       "count        25571.000000       26339.000000       25922.000000   \n",
       "unique                NaN                NaN                NaN   \n",
       "top                   NaN                NaN                NaN   \n",
       "freq                  NaN                NaN                NaN   \n",
       "mean             2.666419           2.651999           2.441170   \n",
       "std              3.031593           3.074987           2.516339   \n",
       "min              1.000000           1.000000           1.000000   \n",
       "25%              1.000000           1.000000           1.000000   \n",
       "50%              1.000000           1.000000           2.000000   \n",
       "75%              3.000000           3.000000           3.000000   \n",
       "max             54.000000          60.000000          84.000000   \n",
       "\n",
       "        max_rech_data_6  max_rech_data_7  max_rech_data_8  max_rech_data_9  \\\n",
       "count      25153.000000     25571.000000     26339.000000      25922.00000   \n",
       "unique              NaN              NaN              NaN              NaN   \n",
       "top                 NaN              NaN              NaN              NaN   \n",
       "freq                NaN              NaN              NaN              NaN   \n",
       "mean         126.393392       126.729459       125.717301        124.94144   \n",
       "std          108.477235       109.765267       109.437851        111.36376   \n",
       "min            1.000000         1.000000         1.000000          1.00000   \n",
       "25%           25.000000        25.000000        25.000000         25.00000   \n",
       "50%          145.000000       145.000000       145.000000        145.00000   \n",
       "75%          177.000000       177.000000       179.000000        179.00000   \n",
       "max         1555.000000      1555.000000      1555.000000       1555.00000   \n",
       "\n",
       "        count_rech_2g_6  count_rech_2g_7  count_rech_2g_8  count_rech_2g_9  \\\n",
       "count      25153.000000     25571.000000     26339.000000     25922.000000   \n",
       "unique              NaN              NaN              NaN              NaN   \n",
       "top                 NaN              NaN              NaN              NaN   \n",
       "freq                NaN              NaN              NaN              NaN   \n",
       "mean           1.864668         2.044699         2.016288         1.781807   \n",
       "std            2.570254         2.768332         2.720132         2.214701   \n",
       "min            0.000000         0.000000         0.000000         0.000000   \n",
       "25%            1.000000         1.000000         1.000000         1.000000   \n",
       "50%            1.000000         1.000000         1.000000         1.000000   \n",
       "75%            2.000000         2.000000         2.000000         2.000000   \n",
       "max           42.000000        48.000000        44.000000        40.000000   \n",
       "\n",
       "        count_rech_3g_6  count_rech_3g_7  count_rech_3g_8  count_rech_3g_9  \\\n",
       "count      25153.000000     25571.000000     26339.000000     25922.000000   \n",
       "unique              NaN              NaN              NaN              NaN   \n",
       "top                 NaN              NaN              NaN              NaN   \n",
       "freq                NaN              NaN              NaN              NaN   \n",
       "mean           0.599133         0.621720         0.635711         0.659363   \n",
       "std            1.274428         1.394524         1.422827         1.411513   \n",
       "min            0.000000         0.000000         0.000000         0.000000   \n",
       "25%            0.000000         0.000000         0.000000         0.000000   \n",
       "50%            0.000000         0.000000         0.000000         0.000000   \n",
       "75%            1.000000         1.000000         1.000000         1.000000   \n",
       "max           29.000000        35.000000        45.000000        49.000000   \n",
       "\n",
       "        av_rech_amt_data_6  av_rech_amt_data_7  av_rech_amt_data_8  \\\n",
       "count         25153.000000        25571.000000        26339.000000   \n",
       "unique                 NaN                 NaN                 NaN   \n",
       "top                    NaN                 NaN                 NaN   \n",
       "freq                   NaN                 NaN                 NaN   \n",
       "mean            192.600982          200.981292          197.526489   \n",
       "std             192.646318          196.791224          191.301305   \n",
       "min               1.000000            0.500000            0.500000   \n",
       "25%              82.000000           92.000000           87.000000   \n",
       "50%             154.000000          154.000000          154.000000   \n",
       "75%             252.000000          252.000000          252.000000   \n",
       "max            7546.000000         4365.000000         4076.000000   \n",
       "\n",
       "        av_rech_amt_data_9   vol_2g_mb_6   vol_2g_mb_7   vol_2g_mb_8  \\\n",
       "count         25922.000000  99999.000000  99999.000000  99999.000000   \n",
       "unique                 NaN           NaN           NaN           NaN   \n",
       "top                    NaN           NaN           NaN           NaN   \n",
       "freq                   NaN           NaN           NaN           NaN   \n",
       "mean            192.734315     51.904956     51.229937     50.170154   \n",
       "std             188.400286    213.356445    212.302217    212.347892   \n",
       "min               1.000000      0.000000      0.000000      0.000000   \n",
       "25%              69.000000      0.000000      0.000000      0.000000   \n",
       "50%             164.000000      0.000000      0.000000      0.000000   \n",
       "75%             252.000000      0.000000      0.000000      0.000000   \n",
       "max            4061.000000  10285.900000   7873.550000  11117.610000   \n",
       "\n",
       "         vol_2g_mb_9   vol_3g_mb_6   vol_3g_mb_7   vol_3g_mb_8   vol_3g_mb_9  \\\n",
       "count   99999.000000  99999.000000  99999.000000  99999.000000  99999.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       44.719701    121.396219    128.995847    135.410689    136.056613   \n",
       "std       198.653570    544.247227    541.494013    558.775335    577.394194   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max      8993.950000  45735.400000  28144.120000  30036.060000  39221.270000   \n",
       "\n",
       "           arpu_3g_6     arpu_3g_7     arpu_3g_8     arpu_3g_9     arpu_2g_6  \\\n",
       "count   25153.000000  25571.000000  26339.000000  25922.000000  25153.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean       89.555057     89.384120     91.173849    100.264116     86.398003   \n",
       "std       193.124653    195.893924    188.180936    216.291992    172.767523   \n",
       "min       -30.820000    -26.040000    -24.490000    -71.090000    -35.830000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.480000      0.420000      0.880000      2.605000     10.830000   \n",
       "75%       122.070000    119.560000    122.070000    140.010000    122.070000   \n",
       "max      6362.280000   4980.900000   3716.900000  13884.310000   6433.760000   \n",
       "\n",
       "           arpu_2g_7     arpu_2g_8     arpu_2g_9  night_pck_user_6  \\\n",
       "count   25571.000000  26339.000000  25922.000000      25153.000000   \n",
       "unique           NaN           NaN           NaN               NaN   \n",
       "top              NaN           NaN           NaN               NaN   \n",
       "freq             NaN           NaN           NaN               NaN   \n",
       "mean       85.914450     86.599478     93.712026          0.025086   \n",
       "std       176.379871    168.247852    171.384224          0.156391   \n",
       "min       -15.480000    -55.830000    -45.740000          0.000000   \n",
       "25%         0.000000      0.000000      0.000000          0.000000   \n",
       "50%         8.810000      9.270000     14.800000          0.000000   \n",
       "75%       122.070000    122.070000    140.010000          0.000000   \n",
       "max      4809.360000   3483.170000   3467.170000          1.000000   \n",
       "\n",
       "        night_pck_user_7  night_pck_user_8  night_pck_user_9  monthly_2g_6  \\\n",
       "count       25571.000000      26339.000000      25922.000000  99999.000000   \n",
       "unique               NaN               NaN               NaN           NaN   \n",
       "top                  NaN               NaN               NaN           NaN   \n",
       "freq                 NaN               NaN               NaN           NaN   \n",
       "mean            0.023034          0.020844          0.015971      0.079641   \n",
       "std             0.150014          0.142863          0.125366      0.295058   \n",
       "min             0.000000          0.000000          0.000000      0.000000   \n",
       "25%             0.000000          0.000000          0.000000      0.000000   \n",
       "50%             0.000000          0.000000          0.000000      0.000000   \n",
       "75%             0.000000          0.000000          0.000000      0.000000   \n",
       "max             1.000000          1.000000          1.000000      4.000000   \n",
       "\n",
       "        monthly_2g_7  monthly_2g_8  monthly_2g_9   sachet_2g_6   sachet_2g_7  \\\n",
       "count   99999.000000  99999.000000  99999.000000  99999.000000  99999.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.083221      0.081001      0.068781      0.389384      0.439634   \n",
       "std         0.304395      0.299568      0.278120      1.497320      1.636230   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max         5.000000      5.000000      4.000000     42.000000     48.000000   \n",
       "\n",
       "         sachet_2g_8   sachet_2g_9  monthly_3g_6  monthly_3g_7  monthly_3g_8  \\\n",
       "count   99999.000000  99999.000000  99999.000000  99999.000000  99999.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.450075      0.393104      0.075921      0.078581      0.082941   \n",
       "std         1.630263      1.347140      0.363371      0.387231      0.384947   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        44.000000     40.000000     14.000000     16.000000     16.000000   \n",
       "\n",
       "        monthly_3g_9   sachet_3g_6   sachet_3g_7   sachet_3g_8   sachet_3g_9  \\\n",
       "count   99999.000000  99999.000000  99999.000000  99999.000000  99999.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.086341      0.074781      0.080401      0.084501      0.084581   \n",
       "std         0.384978      0.568344      0.628334      0.660234      0.650457   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        11.000000     29.000000     35.000000     41.000000     49.000000   \n",
       "\n",
       "           fb_user_6     fb_user_7     fb_user_8     fb_user_9           aon  \\\n",
       "count   25153.000000  25571.000000  26339.000000  25922.000000  99999.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        0.914404      0.908764      0.890808      0.860968   1219.854749   \n",
       "std         0.279772      0.287950      0.311885      0.345987    954.733842   \n",
       "min         0.000000      0.000000      0.000000      0.000000    180.000000   \n",
       "25%         1.000000      1.000000      1.000000      1.000000    467.000000   \n",
       "50%         1.000000      1.000000      1.000000      1.000000    863.000000   \n",
       "75%         1.000000      1.000000      1.000000      1.000000   1807.500000   \n",
       "max         1.000000      1.000000      1.000000      1.000000   4337.000000   \n",
       "\n",
       "          aug_vbc_3g    jul_vbc_3g    jun_vbc_3g    sep_vbc_3g  \n",
       "count   99999.000000  99999.000000  99999.000000  99999.000000  \n",
       "unique           NaN           NaN           NaN           NaN  \n",
       "top              NaN           NaN           NaN           NaN  \n",
       "freq             NaN           NaN           NaN           NaN  \n",
       "mean       68.170248     66.839062     60.021204      3.299373  \n",
       "std       267.580450    271.201856    253.938223     32.408353  \n",
       "min         0.000000      0.000000      0.000000      0.000000  \n",
       "25%         0.000000      0.000000      0.000000      0.000000  \n",
       "50%         0.000000      0.000000      0.000000      0.000000  \n",
       "75%         0.000000      0.000000      0.000000      0.000000  \n",
       "max     12916.220000   9165.600000  11166.210000   2618.570000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis of data statistics\n",
    "churn.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oj9hGK-5d81d"
   },
   "outputs": [],
   "source": [
    "# create backup of data\n",
    "original = churn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PuQOZzFSd81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID cols: 1\n",
      "#Date cols:12\n",
      "#Numeric cols:204\n",
      "#Category cols:8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# create column name list by types of columns\n",
    "id_cols = ['circle_id']\n",
    "\n",
    "date_cols = ['last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9'\n",
    "            ]\n",
    "\n",
    "cat_cols =  ['night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9',\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9'\n",
    "            ]\n",
    "\n",
    "num_cols = [column for column in churn.columns if column not in id_cols + date_cols + cat_cols]\n",
    "\n",
    "# print the number of columns in each list\n",
    "print(\"#ID cols: %d\\n#Date cols:%d\\n#Numeric cols:%d\\n#Category cols:%d\" % (len(id_cols), len(date_cols), len(num_cols), len(cat_cols)))\n",
    "\n",
    "# check if we have missed any column or not\n",
    "print(len(id_cols) + len(date_cols) + len(num_cols) + len(cat_cols) == churn.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ynfkq9d81e"
   },
   "source": [
    "# Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UfdahxQ2d81f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arpu_3g_6                   74.85\n",
       "av_rech_amt_data_6          74.85\n",
       "fb_user_6                   74.85\n",
       "night_pck_user_6            74.85\n",
       "total_rech_data_6           74.85\n",
       "max_rech_data_6             74.85\n",
       "count_rech_2g_6             74.85\n",
       "count_rech_3g_6             74.85\n",
       "date_of_last_rech_data_6    74.85\n",
       "arpu_2g_6                   74.85\n",
       "av_rech_amt_data_7          74.43\n",
       "date_of_last_rech_data_7    74.43\n",
       "max_rech_data_7             74.43\n",
       "total_rech_data_7           74.43\n",
       "arpu_3g_7                   74.43\n",
       "fb_user_7                   74.43\n",
       "count_rech_3g_7             74.43\n",
       "arpu_2g_7                   74.43\n",
       "night_pck_user_7            74.43\n",
       "count_rech_2g_7             74.43\n",
       "arpu_3g_9                   74.08\n",
       "max_rech_data_9             74.08\n",
       "date_of_last_rech_data_9    74.08\n",
       "total_rech_data_9           74.08\n",
       "arpu_2g_9                   74.08\n",
       "count_rech_2g_9             74.08\n",
       "night_pck_user_9            74.08\n",
       "count_rech_3g_9             74.08\n",
       "av_rech_amt_data_9          74.08\n",
       "fb_user_9                   74.08\n",
       "date_of_last_rech_data_8    73.66\n",
       "night_pck_user_8            73.66\n",
       "fb_user_8                   73.66\n",
       "total_rech_data_8           73.66\n",
       "av_rech_amt_data_8          73.66\n",
       "arpu_3g_8                   73.66\n",
       "count_rech_3g_8             73.66\n",
       "arpu_2g_8                   73.66\n",
       "max_rech_data_8             73.66\n",
       "count_rech_2g_8             73.66\n",
       "loc_og_t2m_mou_9             7.75\n",
       "ic_others_9                  7.75\n",
       "std_og_mou_9                 7.75\n",
       "isd_og_mou_9                 7.75\n",
       "isd_ic_mou_9                 7.75\n",
       "spl_og_mou_9                 7.75\n",
       "spl_ic_mou_9                 7.75\n",
       "og_others_9                  7.75\n",
       "onnet_mou_9                  7.75\n",
       "offnet_mou_9                 7.75\n",
       "std_ic_mou_9                 7.75\n",
       "loc_ic_t2t_mou_9             7.75\n",
       "loc_ic_t2m_mou_9             7.75\n",
       "std_ic_t2o_mou_9             7.75\n",
       "loc_ic_t2f_mou_9             7.75\n",
       "std_ic_t2f_mou_9             7.75\n",
       "loc_ic_mou_9                 7.75\n",
       "std_ic_t2m_mou_9             7.75\n",
       "std_og_t2c_mou_9             7.75\n",
       "std_ic_t2t_mou_9             7.75\n",
       "std_og_t2f_mou_9             7.75\n",
       "std_og_t2m_mou_9             7.75\n",
       "roam_og_mou_9                7.75\n",
       "std_og_t2t_mou_9             7.75\n",
       "loc_og_t2c_mou_9             7.75\n",
       "roam_ic_mou_9                7.75\n",
       "loc_og_t2t_mou_9             7.75\n",
       "loc_og_mou_9                 7.75\n",
       "loc_og_t2f_mou_9             7.75\n",
       "std_ic_t2f_mou_8             5.38\n",
       "loc_og_t2c_mou_8             5.38\n",
       "loc_ic_t2f_mou_8             5.38\n",
       "spl_ic_mou_8                 5.38\n",
       "loc_ic_mou_8                 5.38\n",
       "loc_og_t2t_mou_8             5.38\n",
       "loc_ic_t2m_mou_8             5.38\n",
       "std_ic_mou_8                 5.38\n",
       "loc_og_t2f_mou_8             5.38\n",
       "std_ic_t2t_mou_8             5.38\n",
       "loc_ic_t2t_mou_8             5.38\n",
       "loc_og_mou_8                 5.38\n",
       "std_ic_t2o_mou_8             5.38\n",
       "onnet_mou_8                  5.38\n",
       "std_og_t2m_mou_8             5.38\n",
       "isd_ic_mou_8                 5.38\n",
       "std_og_t2c_mou_8             5.38\n",
       "std_og_t2f_mou_8             5.38\n",
       "offnet_mou_8                 5.38\n",
       "roam_ic_mou_8                5.38\n",
       "std_og_mou_8                 5.38\n",
       "ic_others_8                  5.38\n",
       "loc_og_t2m_mou_8             5.38\n",
       "isd_og_mou_8                 5.38\n",
       "spl_og_mou_8                 5.38\n",
       "std_ic_t2m_mou_8             5.38\n",
       "og_others_8                  5.38\n",
       "std_og_t2t_mou_8             5.38\n",
       "roam_og_mou_8                5.38\n",
       "date_of_last_rech_9          4.76\n",
       "spl_ic_mou_6                 3.94\n",
       "ic_others_6                  3.94\n",
       "std_ic_mou_6                 3.94\n",
       "isd_ic_mou_6                 3.94\n",
       "std_ic_t2f_mou_6             3.94\n",
       "std_ic_t2o_mou_6             3.94\n",
       "loc_og_t2f_mou_6             3.94\n",
       "loc_og_t2t_mou_6             3.94\n",
       "std_og_t2c_mou_6             3.94\n",
       "std_og_t2f_mou_6             3.94\n",
       "std_og_t2m_mou_6             3.94\n",
       "isd_og_mou_6                 3.94\n",
       "std_og_t2t_mou_6             3.94\n",
       "spl_og_mou_6                 3.94\n",
       "loc_og_mou_6                 3.94\n",
       "og_others_6                  3.94\n",
       "loc_og_t2c_mou_6             3.94\n",
       "loc_ic_t2t_mou_6             3.94\n",
       "std_ic_t2m_mou_6             3.94\n",
       "loc_og_t2m_mou_6             3.94\n",
       "std_og_mou_6                 3.94\n",
       "offnet_mou_6                 3.94\n",
       "onnet_mou_6                  3.94\n",
       "roam_og_mou_6                3.94\n",
       "loc_ic_t2f_mou_6             3.94\n",
       "std_ic_t2t_mou_6             3.94\n",
       "roam_ic_mou_6                3.94\n",
       "loc_ic_t2m_mou_6             3.94\n",
       "loc_ic_mou_6                 3.94\n",
       "loc_og_t2t_mou_7             3.86\n",
       "std_og_t2c_mou_7             3.86\n",
       "std_og_t2f_mou_7             3.86\n",
       "std_og_t2m_mou_7             3.86\n",
       "onnet_mou_7                  3.86\n",
       "offnet_mou_7                 3.86\n",
       "std_og_t2t_mou_7             3.86\n",
       "loc_og_mou_7                 3.86\n",
       "roam_ic_mou_7                3.86\n",
       "loc_og_t2c_mou_7             3.86\n",
       "loc_og_t2f_mou_7             3.86\n",
       "roam_og_mou_7                3.86\n",
       "loc_og_t2m_mou_7             3.86\n",
       "std_ic_t2m_mou_7             3.86\n",
       "loc_ic_t2f_mou_7             3.86\n",
       "ic_others_7                  3.86\n",
       "std_og_mou_7                 3.86\n",
       "std_ic_mou_7                 3.86\n",
       "loc_ic_t2t_mou_7             3.86\n",
       "loc_ic_mou_7                 3.86\n",
       "std_ic_t2f_mou_7             3.86\n",
       "std_ic_t2t_mou_7             3.86\n",
       "spl_ic_mou_7                 3.86\n",
       "og_others_7                  3.86\n",
       "isd_ic_mou_7                 3.86\n",
       "spl_og_mou_7                 3.86\n",
       "isd_og_mou_7                 3.86\n",
       "std_ic_t2o_mou_7             3.86\n",
       "loc_ic_t2m_mou_7             3.86\n",
       "date_of_last_rech_8          3.62\n",
       "date_of_last_rech_7          1.77\n",
       "last_date_of_month_9         1.66\n",
       "date_of_last_rech_6          1.61\n",
       "last_date_of_month_8         1.10\n",
       "loc_og_t2o_mou               1.02\n",
       "loc_ic_t2o_mou               1.02\n",
       "std_og_t2o_mou               1.02\n",
       "last_date_of_month_7         0.60\n",
       "aug_vbc_3g                   0.00\n",
       "jul_vbc_3g                   0.00\n",
       "aon                          0.00\n",
       "jun_vbc_3g                   0.00\n",
       "monthly_2g_9                 0.00\n",
       "sachet_3g_6                  0.00\n",
       "vol_3g_mb_9                  0.00\n",
       "sachet_3g_8                  0.00\n",
       "sachet_3g_7                  0.00\n",
       "monthly_2g_8                 0.00\n",
       "monthly_3g_9                 0.00\n",
       "sachet_3g_9                  0.00\n",
       "monthly_3g_8                 0.00\n",
       "monthly_3g_7                 0.00\n",
       "monthly_3g_6                 0.00\n",
       "sachet_2g_9                  0.00\n",
       "sachet_2g_8                  0.00\n",
       "sachet_2g_7                  0.00\n",
       "sachet_2g_6                  0.00\n",
       "monthly_2g_7                 0.00\n",
       "monthly_2g_6                 0.00\n",
       "circle_id                    0.00\n",
       "vol_3g_mb_8                  0.00\n",
       "total_rech_num_9             0.00\n",
       "total_rech_num_7             0.00\n",
       "total_rech_num_6             0.00\n",
       "total_ic_mou_9               0.00\n",
       "total_ic_mou_8               0.00\n",
       "total_ic_mou_7               0.00\n",
       "total_ic_mou_6               0.00\n",
       "total_og_mou_9               0.00\n",
       "total_og_mou_8               0.00\n",
       "total_og_mou_7               0.00\n",
       "total_og_mou_6               0.00\n",
       "arpu_9                       0.00\n",
       "arpu_8                       0.00\n",
       "arpu_7                       0.00\n",
       "arpu_6                       0.00\n",
       "last_date_of_month_6         0.00\n",
       "total_rech_num_8             0.00\n",
       "total_rech_amt_6             0.00\n",
       "vol_3g_mb_7                  0.00\n",
       "total_rech_amt_7             0.00\n",
       "vol_3g_mb_6                  0.00\n",
       "vol_2g_mb_9                  0.00\n",
       "vol_2g_mb_8                  0.00\n",
       "vol_2g_mb_7                  0.00\n",
       "vol_2g_mb_6                  0.00\n",
       "last_day_rch_amt_9           0.00\n",
       "last_day_rch_amt_8           0.00\n",
       "last_day_rch_amt_7           0.00\n",
       "last_day_rch_amt_6           0.00\n",
       "max_rech_amt_9               0.00\n",
       "max_rech_amt_8               0.00\n",
       "max_rech_amt_7               0.00\n",
       "max_rech_amt_6               0.00\n",
       "total_rech_amt_9             0.00\n",
       "total_rech_amt_8             0.00\n",
       "sep_vbc_3g                   0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at missing value ratio in each column\n",
    "\n",
    "round(((churn.isnull().sum()/len(churn.index))*100),2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faEE1-CDd81f"
   },
   "source": [
    "### i) Impute missing values with zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFZzljnHG76W"
   },
   "source": [
    "Now that we have the information about the amount of missing values in each column, we can go ahead and perform some imputing and deleting. \n",
    "\n",
    "First, we will start with the columns corresponding to the \"recharging of the service\" information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kM6FEVJSd81f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rech_data_6</th>\n",
       "      <th>total_rech_data_7</th>\n",
       "      <th>total_rech_data_8</th>\n",
       "      <th>total_rech_data_9</th>\n",
       "      <th>count_rech_2g_6</th>\n",
       "      <th>count_rech_2g_7</th>\n",
       "      <th>count_rech_2g_8</th>\n",
       "      <th>count_rech_2g_9</th>\n",
       "      <th>count_rech_3g_6</th>\n",
       "      <th>count_rech_3g_7</th>\n",
       "      <th>count_rech_3g_8</th>\n",
       "      <th>count_rech_3g_9</th>\n",
       "      <th>max_rech_data_6</th>\n",
       "      <th>max_rech_data_7</th>\n",
       "      <th>max_rech_data_8</th>\n",
       "      <th>max_rech_data_9</th>\n",
       "      <th>av_rech_amt_data_6</th>\n",
       "      <th>av_rech_amt_data_7</th>\n",
       "      <th>av_rech_amt_data_8</th>\n",
       "      <th>av_rech_amt_data_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.00000</td>\n",
       "      <td>25153.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>25922.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.463802</td>\n",
       "      <td>2.666419</td>\n",
       "      <td>2.651999</td>\n",
       "      <td>2.441170</td>\n",
       "      <td>1.864668</td>\n",
       "      <td>2.044699</td>\n",
       "      <td>2.016288</td>\n",
       "      <td>1.781807</td>\n",
       "      <td>0.599133</td>\n",
       "      <td>0.621720</td>\n",
       "      <td>0.635711</td>\n",
       "      <td>0.659363</td>\n",
       "      <td>126.393392</td>\n",
       "      <td>126.729459</td>\n",
       "      <td>125.717301</td>\n",
       "      <td>124.94144</td>\n",
       "      <td>192.600982</td>\n",
       "      <td>200.981292</td>\n",
       "      <td>197.526489</td>\n",
       "      <td>192.734315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.789128</td>\n",
       "      <td>3.031593</td>\n",
       "      <td>3.074987</td>\n",
       "      <td>2.516339</td>\n",
       "      <td>2.570254</td>\n",
       "      <td>2.768332</td>\n",
       "      <td>2.720132</td>\n",
       "      <td>2.214701</td>\n",
       "      <td>1.274428</td>\n",
       "      <td>1.394524</td>\n",
       "      <td>1.422827</td>\n",
       "      <td>1.411513</td>\n",
       "      <td>108.477235</td>\n",
       "      <td>109.765267</td>\n",
       "      <td>109.437851</td>\n",
       "      <td>111.36376</td>\n",
       "      <td>192.646318</td>\n",
       "      <td>196.791224</td>\n",
       "      <td>191.301305</td>\n",
       "      <td>188.400286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.00000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>179.00000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>1555.00000</td>\n",
       "      <td>7546.000000</td>\n",
       "      <td>4365.000000</td>\n",
       "      <td>4076.000000</td>\n",
       "      <td>4061.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_rech_data_6  total_rech_data_7  total_rech_data_8  \\\n",
       "count       25153.000000       25571.000000       26339.000000   \n",
       "mean            2.463802           2.666419           2.651999   \n",
       "std             2.789128           3.031593           3.074987   \n",
       "min             1.000000           1.000000           1.000000   \n",
       "25%             1.000000           1.000000           1.000000   \n",
       "50%             1.000000           1.000000           1.000000   \n",
       "75%             3.000000           3.000000           3.000000   \n",
       "max            61.000000          54.000000          60.000000   \n",
       "\n",
       "       total_rech_data_9  count_rech_2g_6  count_rech_2g_7  count_rech_2g_8  \\\n",
       "count       25922.000000     25153.000000     25571.000000     26339.000000   \n",
       "mean            2.441170         1.864668         2.044699         2.016288   \n",
       "std             2.516339         2.570254         2.768332         2.720132   \n",
       "min             1.000000         0.000000         0.000000         0.000000   \n",
       "25%             1.000000         1.000000         1.000000         1.000000   \n",
       "50%             2.000000         1.000000         1.000000         1.000000   \n",
       "75%             3.000000         2.000000         2.000000         2.000000   \n",
       "max            84.000000        42.000000        48.000000        44.000000   \n",
       "\n",
       "       count_rech_2g_9  count_rech_3g_6  count_rech_3g_7  count_rech_3g_8  \\\n",
       "count     25922.000000     25153.000000     25571.000000     26339.000000   \n",
       "mean          1.781807         0.599133         0.621720         0.635711   \n",
       "std           2.214701         1.274428         1.394524         1.422827   \n",
       "min           0.000000         0.000000         0.000000         0.000000   \n",
       "25%           1.000000         0.000000         0.000000         0.000000   \n",
       "50%           1.000000         0.000000         0.000000         0.000000   \n",
       "75%           2.000000         1.000000         1.000000         1.000000   \n",
       "max          40.000000        29.000000        35.000000        45.000000   \n",
       "\n",
       "       count_rech_3g_9  max_rech_data_6  max_rech_data_7  max_rech_data_8  \\\n",
       "count     25922.000000     25153.000000     25571.000000     26339.000000   \n",
       "mean          0.659363       126.393392       126.729459       125.717301   \n",
       "std           1.411513       108.477235       109.765267       109.437851   \n",
       "min           0.000000         1.000000         1.000000         1.000000   \n",
       "25%           0.000000        25.000000        25.000000        25.000000   \n",
       "50%           0.000000       145.000000       145.000000       145.000000   \n",
       "75%           1.000000       177.000000       177.000000       179.000000   \n",
       "max          49.000000      1555.000000      1555.000000      1555.000000   \n",
       "\n",
       "       max_rech_data_9  av_rech_amt_data_6  av_rech_amt_data_7  \\\n",
       "count      25922.00000        25153.000000        25571.000000   \n",
       "mean         124.94144          192.600982          200.981292   \n",
       "std          111.36376          192.646318          196.791224   \n",
       "min            1.00000            1.000000            0.500000   \n",
       "25%           25.00000           82.000000           92.000000   \n",
       "50%          145.00000          154.000000          154.000000   \n",
       "75%          179.00000          252.000000          252.000000   \n",
       "max         1555.00000         7546.000000         4365.000000   \n",
       "\n",
       "       av_rech_amt_data_8  av_rech_amt_data_9  \n",
       "count        26339.000000        25922.000000  \n",
       "mean           197.526489          192.734315  \n",
       "std            191.301305          188.400286  \n",
       "min              0.500000            1.000000  \n",
       "25%             87.000000           69.000000  \n",
       "50%            154.000000          164.000000  \n",
       "75%            252.000000          252.000000  \n",
       "max           4076.000000         4061.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some recharge columns have minimum value of 1 while some don't\n",
    "recharge_cols = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "                 'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_2g_9',\n",
    "                 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8', 'count_rech_3g_9',\n",
    "                 'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9',\n",
    "                 'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "                 ]\n",
    "\n",
    "churn[recharge_cols].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XycmOJEMd81g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rech_data_6</th>\n",
       "      <th>date_of_last_rech_data_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    total_rech_data_6 date_of_last_rech_data_6\n",
       "1                 NaN                      NaN\n",
       "2                 NaN                      NaN\n",
       "3                 NaN                      NaN\n",
       "5                 NaN                      NaN\n",
       "6                 NaN                      NaN\n",
       "7                 NaN                      NaN\n",
       "8                 NaN                      NaN\n",
       "9                 NaN                      NaN\n",
       "10                NaN                      NaN\n",
       "11                NaN                      NaN\n",
       "12                NaN                      NaN\n",
       "13                NaN                      NaN\n",
       "14                NaN                      NaN\n",
       "15                NaN                      NaN\n",
       "16                NaN                      NaN\n",
       "17                NaN                      NaN\n",
       "18                NaN                      NaN\n",
       "20                NaN                      NaN\n",
       "21                NaN                      NaN\n",
       "22                NaN                      NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe whether the date of the last recharge and the total recharge data value are missing together\n",
    "# You can do this by displaying the rows that have null values in these two variables\n",
    "\n",
    "churn.loc[churn.total_rech_data_6.isnull() & churn.date_of_last_rech_data_6.isnull(), [\"total_rech_data_6\", \"date_of_last_rech_data_6\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As0NTpnFd81g"
   },
   "source": [
    "In the recharge variables where minumum value is 1, we can impute missing values with zeroes since it means customer didn't recharge their number that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2e7lzAL1d81g"
   },
   "outputs": [],
   "source": [
    "# create a list of recharge columns where we will impute missing values with zeroes\n",
    "zero_impute = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "        'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "        'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vZCXT8dPd81g"
   },
   "outputs": [],
   "source": [
    "# impute missing values with 0 for the above mentioned list of recharge columns\n",
    "\n",
    "churn[zero_impute] = churn[zero_impute].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UVmvI2gZd81g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value ratio:\n",
      "\n",
      "total_rech_data_6     0.0\n",
      "total_rech_data_7     0.0\n",
      "total_rech_data_8     0.0\n",
      "total_rech_data_9     0.0\n",
      "av_rech_amt_data_6    0.0\n",
      "av_rech_amt_data_7    0.0\n",
      "av_rech_amt_data_8    0.0\n",
      "av_rech_amt_data_9    0.0\n",
      "max_rech_data_6       0.0\n",
      "max_rech_data_7       0.0\n",
      "max_rech_data_8       0.0\n",
      "max_rech_data_9       0.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Summary statistics\n",
      "\n",
      "       total_rech_data_6  total_rech_data_7  total_rech_data_8  \\\n",
      "count       99999.000000       99999.000000       99999.000000   \n",
      "mean            0.619726           0.681837           0.698517   \n",
      "std             1.760541           1.924382           1.963417   \n",
      "min             0.000000           0.000000           0.000000   \n",
      "25%             0.000000           0.000000           0.000000   \n",
      "50%             0.000000           0.000000           0.000000   \n",
      "75%             1.000000           1.000000           1.000000   \n",
      "max            61.000000          54.000000          60.000000   \n",
      "\n",
      "       total_rech_data_9  av_rech_amt_data_6  av_rech_amt_data_7  \\\n",
      "count       99999.000000        99999.000000        99999.000000   \n",
      "mean            0.632806           48.445409           51.393440   \n",
      "std             1.669040          127.743863          132.629365   \n",
      "min             0.000000            0.000000            0.000000   \n",
      "25%             0.000000            0.000000            0.000000   \n",
      "50%             0.000000            0.000000            0.000000   \n",
      "75%             1.000000            8.250000           17.000000   \n",
      "max            84.000000         7546.000000         4365.000000   \n",
      "\n",
      "       av_rech_amt_data_8  av_rech_amt_data_9  max_rech_data_6  \\\n",
      "count        99999.000000        99999.000000     99999.000000   \n",
      "mean            52.027022           49.961089        31.792048   \n",
      "std            131.182609          127.804280        77.248778   \n",
      "min              0.000000            0.000000         0.000000   \n",
      "25%              0.000000            0.000000         0.000000   \n",
      "50%              0.000000            0.000000         0.000000   \n",
      "75%             23.000000           17.000000         8.000000   \n",
      "max           4076.000000         4061.000000      1555.000000   \n",
      "\n",
      "       max_rech_data_7  max_rech_data_8  max_rech_data_9  \n",
      "count     99999.000000     99999.000000     99999.000000  \n",
      "mean         32.406314        33.113011        32.387644  \n",
      "std          78.342435        78.872739        78.818696  \n",
      "min           0.000000         0.000000         0.000000  \n",
      "25%           0.000000         0.000000         0.000000  \n",
      "50%           0.000000         0.000000         0.000000  \n",
      "75%          14.000000        17.000000        17.000000  \n",
      "max        1555.000000      1555.000000      1555.000000  \n"
     ]
    }
   ],
   "source": [
    "# now, let's make sure values are imputed correctly\n",
    "print(\"Missing value ratio:\\n\")\n",
    "print(churn[zero_impute].isnull().sum()*100/churn.shape[1])\n",
    "\n",
    "# summary\n",
    "print(\"\\n\\nSummary statistics\\n\")\n",
    "print(churn[zero_impute].describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v93kxYHPd81h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping:  (99999, 225)\n",
      "Shape after dropping:  (99999, 212)\n"
     ]
    }
   ],
   "source": [
    "# drop id and all the date columns\n",
    "print(\"Shape before dropping: \", churn.shape)\n",
    "\n",
    "churn = churn.drop(id_cols + date_cols, axis=1)\n",
    "\n",
    "print(\"Shape after dropping: \", churn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZjdXHUKd81h"
   },
   "source": [
    "### ii) Replace NaN values in categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h_v0ZV_d81h"
   },
   "source": [
    "We will replace missing values in the categorical values with '-1' where '-1' will be a new category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cy0PvmSJd81h"
   },
   "outputs": [],
   "source": [
    "# replace missing values with '-1' in categorical columns\n",
    "\n",
    "churn[cat_cols] = churn[cat_cols].apply(lambda x: x.fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GzXMjtRBd81h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value ratio:\n",
      "\n",
      "night_pck_user_6    0.0\n",
      "night_pck_user_7    0.0\n",
      "night_pck_user_8    0.0\n",
      "night_pck_user_9    0.0\n",
      "fb_user_6           0.0\n",
      "fb_user_7           0.0\n",
      "fb_user_8           0.0\n",
      "fb_user_9           0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# missing value ratio\n",
    "print(\"Missing value ratio:\\n\")\n",
    "print(churn[cat_cols].isnull().sum()*100/churn.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAvcvw2Dd81h"
   },
   "source": [
    "### iii) Drop variables with more than a given threshold of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7YbBJUjHPdb"
   },
   "source": [
    "Here, we will be removing the column variables that have more than 70% of its elements missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sQLEKv75d81h"
   },
   "outputs": [],
   "source": [
    "initial_cols = churn.shape[1]\n",
    "\n",
    "# Insert the threshold value of missing entries\n",
    "MISSING_THRESHOLD = 0.7\n",
    "\n",
    "# Extract a list of columns that have less than the threshold of missing values \n",
    "include_cols = list(churn.apply(lambda column: True if column.isnull().sum()/churn.shape[0] < MISSING_THRESHOLD else False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rnZKs1mRd81i"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>include</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc_og_t2o_mou</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>std_og_t2o_mou</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loc_ic_t2o_mou</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arpu_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arpu_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arpu_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arpu_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onnet_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>onnet_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>onnet_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>onnet_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>offnet_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>offnet_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>offnet_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>offnet_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>roam_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roam_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roam_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>roam_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roam_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>roam_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>roam_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>roam_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loc_og_t2t_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>loc_og_t2t_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>loc_og_t2t_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>loc_og_t2t_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>loc_og_t2m_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>loc_og_t2m_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>loc_og_t2m_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>loc_og_t2m_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>loc_og_t2f_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>loc_og_t2f_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>loc_og_t2f_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>loc_og_t2f_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>loc_og_t2c_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>loc_og_t2c_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>loc_og_t2c_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>loc_og_t2c_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>loc_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>loc_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>loc_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>loc_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>std_og_t2t_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>std_og_t2t_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>std_og_t2t_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>std_og_t2t_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>std_og_t2m_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>std_og_t2m_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>std_og_t2m_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>std_og_t2m_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>std_og_t2f_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>std_og_t2f_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>std_og_t2f_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>std_og_t2f_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>std_og_t2c_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>std_og_t2c_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>std_og_t2c_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>std_og_t2c_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>std_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>std_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>std_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>std_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>isd_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>isd_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>isd_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>isd_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>spl_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>spl_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>spl_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>spl_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>og_others_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>og_others_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>og_others_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>og_others_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>total_og_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>total_og_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>total_og_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>total_og_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>loc_ic_t2t_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>loc_ic_t2t_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>loc_ic_t2t_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>loc_ic_t2t_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>loc_ic_t2m_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>loc_ic_t2m_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>loc_ic_t2m_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>loc_ic_t2m_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>loc_ic_t2f_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>loc_ic_t2f_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>loc_ic_t2f_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>loc_ic_t2f_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>loc_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>loc_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>loc_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>loc_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>std_ic_t2t_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>std_ic_t2t_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>std_ic_t2t_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>std_ic_t2t_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>std_ic_t2m_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>std_ic_t2m_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>std_ic_t2m_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>std_ic_t2m_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>std_ic_t2f_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>std_ic_t2f_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>std_ic_t2f_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>std_ic_t2f_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>std_ic_t2o_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>std_ic_t2o_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>std_ic_t2o_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>std_ic_t2o_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>std_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>std_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>std_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>std_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>total_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>total_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>total_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>total_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>spl_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>spl_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>spl_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>spl_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>isd_ic_mou_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>isd_ic_mou_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>isd_ic_mou_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>isd_ic_mou_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>ic_others_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ic_others_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>ic_others_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ic_others_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>total_rech_num_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>total_rech_num_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>total_rech_num_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>total_rech_num_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>total_rech_amt_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>total_rech_amt_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>total_rech_amt_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>total_rech_amt_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>max_rech_amt_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>max_rech_amt_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>max_rech_amt_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>max_rech_amt_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>last_day_rch_amt_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>last_day_rch_amt_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>last_day_rch_amt_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>last_day_rch_amt_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>total_rech_data_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>total_rech_data_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>total_rech_data_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>total_rech_data_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>max_rech_data_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>max_rech_data_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>max_rech_data_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>max_rech_data_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>av_rech_amt_data_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>av_rech_amt_data_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>av_rech_amt_data_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>av_rech_amt_data_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>vol_2g_mb_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>vol_2g_mb_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>vol_2g_mb_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>vol_2g_mb_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>vol_3g_mb_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>vol_3g_mb_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>vol_3g_mb_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>vol_3g_mb_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>night_pck_user_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>night_pck_user_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>night_pck_user_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>night_pck_user_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>monthly_2g_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>monthly_2g_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>monthly_2g_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>monthly_2g_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>sachet_2g_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>sachet_2g_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>sachet_2g_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>sachet_2g_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>monthly_3g_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>monthly_3g_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>monthly_3g_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>monthly_3g_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>sachet_3g_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>sachet_3g_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>sachet_3g_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>sachet_3g_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>fb_user_6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>fb_user_7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>fb_user_8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>fb_user_9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>aon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>aug_vbc_3g</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>jul_vbc_3g</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>jun_vbc_3g</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>sep_vbc_3g</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  include\n",
       "0        loc_og_t2o_mou     True\n",
       "1        std_og_t2o_mou     True\n",
       "2        loc_ic_t2o_mou     True\n",
       "3                arpu_6     True\n",
       "4                arpu_7     True\n",
       "5                arpu_8     True\n",
       "6                arpu_9     True\n",
       "7           onnet_mou_6     True\n",
       "8           onnet_mou_7     True\n",
       "9           onnet_mou_8     True\n",
       "10          onnet_mou_9     True\n",
       "11         offnet_mou_6     True\n",
       "12         offnet_mou_7     True\n",
       "13         offnet_mou_8     True\n",
       "14         offnet_mou_9     True\n",
       "15        roam_ic_mou_6     True\n",
       "16        roam_ic_mou_7     True\n",
       "17        roam_ic_mou_8     True\n",
       "18        roam_ic_mou_9     True\n",
       "19        roam_og_mou_6     True\n",
       "20        roam_og_mou_7     True\n",
       "21        roam_og_mou_8     True\n",
       "22        roam_og_mou_9     True\n",
       "23     loc_og_t2t_mou_6     True\n",
       "24     loc_og_t2t_mou_7     True\n",
       "25     loc_og_t2t_mou_8     True\n",
       "26     loc_og_t2t_mou_9     True\n",
       "27     loc_og_t2m_mou_6     True\n",
       "28     loc_og_t2m_mou_7     True\n",
       "29     loc_og_t2m_mou_8     True\n",
       "30     loc_og_t2m_mou_9     True\n",
       "31     loc_og_t2f_mou_6     True\n",
       "32     loc_og_t2f_mou_7     True\n",
       "33     loc_og_t2f_mou_8     True\n",
       "34     loc_og_t2f_mou_9     True\n",
       "35     loc_og_t2c_mou_6     True\n",
       "36     loc_og_t2c_mou_7     True\n",
       "37     loc_og_t2c_mou_8     True\n",
       "38     loc_og_t2c_mou_9     True\n",
       "39         loc_og_mou_6     True\n",
       "40         loc_og_mou_7     True\n",
       "41         loc_og_mou_8     True\n",
       "42         loc_og_mou_9     True\n",
       "43     std_og_t2t_mou_6     True\n",
       "44     std_og_t2t_mou_7     True\n",
       "45     std_og_t2t_mou_8     True\n",
       "46     std_og_t2t_mou_9     True\n",
       "47     std_og_t2m_mou_6     True\n",
       "48     std_og_t2m_mou_7     True\n",
       "49     std_og_t2m_mou_8     True\n",
       "50     std_og_t2m_mou_9     True\n",
       "51     std_og_t2f_mou_6     True\n",
       "52     std_og_t2f_mou_7     True\n",
       "53     std_og_t2f_mou_8     True\n",
       "54     std_og_t2f_mou_9     True\n",
       "55     std_og_t2c_mou_6     True\n",
       "56     std_og_t2c_mou_7     True\n",
       "57     std_og_t2c_mou_8     True\n",
       "58     std_og_t2c_mou_9     True\n",
       "59         std_og_mou_6     True\n",
       "60         std_og_mou_7     True\n",
       "61         std_og_mou_8     True\n",
       "62         std_og_mou_9     True\n",
       "63         isd_og_mou_6     True\n",
       "64         isd_og_mou_7     True\n",
       "65         isd_og_mou_8     True\n",
       "66         isd_og_mou_9     True\n",
       "67         spl_og_mou_6     True\n",
       "68         spl_og_mou_7     True\n",
       "69         spl_og_mou_8     True\n",
       "70         spl_og_mou_9     True\n",
       "71          og_others_6     True\n",
       "72          og_others_7     True\n",
       "73          og_others_8     True\n",
       "74          og_others_9     True\n",
       "75       total_og_mou_6     True\n",
       "76       total_og_mou_7     True\n",
       "77       total_og_mou_8     True\n",
       "78       total_og_mou_9     True\n",
       "79     loc_ic_t2t_mou_6     True\n",
       "80     loc_ic_t2t_mou_7     True\n",
       "81     loc_ic_t2t_mou_8     True\n",
       "82     loc_ic_t2t_mou_9     True\n",
       "83     loc_ic_t2m_mou_6     True\n",
       "84     loc_ic_t2m_mou_7     True\n",
       "85     loc_ic_t2m_mou_8     True\n",
       "86     loc_ic_t2m_mou_9     True\n",
       "87     loc_ic_t2f_mou_6     True\n",
       "88     loc_ic_t2f_mou_7     True\n",
       "89     loc_ic_t2f_mou_8     True\n",
       "90     loc_ic_t2f_mou_9     True\n",
       "91         loc_ic_mou_6     True\n",
       "92         loc_ic_mou_7     True\n",
       "93         loc_ic_mou_8     True\n",
       "94         loc_ic_mou_9     True\n",
       "95     std_ic_t2t_mou_6     True\n",
       "96     std_ic_t2t_mou_7     True\n",
       "97     std_ic_t2t_mou_8     True\n",
       "98     std_ic_t2t_mou_9     True\n",
       "99     std_ic_t2m_mou_6     True\n",
       "100    std_ic_t2m_mou_7     True\n",
       "101    std_ic_t2m_mou_8     True\n",
       "102    std_ic_t2m_mou_9     True\n",
       "103    std_ic_t2f_mou_6     True\n",
       "104    std_ic_t2f_mou_7     True\n",
       "105    std_ic_t2f_mou_8     True\n",
       "106    std_ic_t2f_mou_9     True\n",
       "107    std_ic_t2o_mou_6     True\n",
       "108    std_ic_t2o_mou_7     True\n",
       "109    std_ic_t2o_mou_8     True\n",
       "110    std_ic_t2o_mou_9     True\n",
       "111        std_ic_mou_6     True\n",
       "112        std_ic_mou_7     True\n",
       "113        std_ic_mou_8     True\n",
       "114        std_ic_mou_9     True\n",
       "115      total_ic_mou_6     True\n",
       "116      total_ic_mou_7     True\n",
       "117      total_ic_mou_8     True\n",
       "118      total_ic_mou_9     True\n",
       "119        spl_ic_mou_6     True\n",
       "120        spl_ic_mou_7     True\n",
       "121        spl_ic_mou_8     True\n",
       "122        spl_ic_mou_9     True\n",
       "123        isd_ic_mou_6     True\n",
       "124        isd_ic_mou_7     True\n",
       "125        isd_ic_mou_8     True\n",
       "126        isd_ic_mou_9     True\n",
       "127         ic_others_6     True\n",
       "128         ic_others_7     True\n",
       "129         ic_others_8     True\n",
       "130         ic_others_9     True\n",
       "131    total_rech_num_6     True\n",
       "132    total_rech_num_7     True\n",
       "133    total_rech_num_8     True\n",
       "134    total_rech_num_9     True\n",
       "135    total_rech_amt_6     True\n",
       "136    total_rech_amt_7     True\n",
       "137    total_rech_amt_8     True\n",
       "138    total_rech_amt_9     True\n",
       "139      max_rech_amt_6     True\n",
       "140      max_rech_amt_7     True\n",
       "141      max_rech_amt_8     True\n",
       "142      max_rech_amt_9     True\n",
       "143  last_day_rch_amt_6     True\n",
       "144  last_day_rch_amt_7     True\n",
       "145  last_day_rch_amt_8     True\n",
       "146  last_day_rch_amt_9     True\n",
       "147   total_rech_data_6     True\n",
       "148   total_rech_data_7     True\n",
       "149   total_rech_data_8     True\n",
       "150   total_rech_data_9     True\n",
       "151     max_rech_data_6     True\n",
       "152     max_rech_data_7     True\n",
       "153     max_rech_data_8     True\n",
       "154     max_rech_data_9     True\n",
       "163  av_rech_amt_data_6     True\n",
       "164  av_rech_amt_data_7     True\n",
       "165  av_rech_amt_data_8     True\n",
       "166  av_rech_amt_data_9     True\n",
       "167         vol_2g_mb_6     True\n",
       "168         vol_2g_mb_7     True\n",
       "169         vol_2g_mb_8     True\n",
       "170         vol_2g_mb_9     True\n",
       "171         vol_3g_mb_6     True\n",
       "172         vol_3g_mb_7     True\n",
       "173         vol_3g_mb_8     True\n",
       "174         vol_3g_mb_9     True\n",
       "183    night_pck_user_6     True\n",
       "184    night_pck_user_7     True\n",
       "185    night_pck_user_8     True\n",
       "186    night_pck_user_9     True\n",
       "187        monthly_2g_6     True\n",
       "188        monthly_2g_7     True\n",
       "189        monthly_2g_8     True\n",
       "190        monthly_2g_9     True\n",
       "191         sachet_2g_6     True\n",
       "192         sachet_2g_7     True\n",
       "193         sachet_2g_8     True\n",
       "194         sachet_2g_9     True\n",
       "195        monthly_3g_6     True\n",
       "196        monthly_3g_7     True\n",
       "197        monthly_3g_8     True\n",
       "198        monthly_3g_9     True\n",
       "199         sachet_3g_6     True\n",
       "200         sachet_3g_7     True\n",
       "201         sachet_3g_8     True\n",
       "202         sachet_3g_9     True\n",
       "203           fb_user_6     True\n",
       "204           fb_user_7     True\n",
       "205           fb_user_8     True\n",
       "206           fb_user_9     True\n",
       "207                 aon     True\n",
       "208          aug_vbc_3g     True\n",
       "209          jul_vbc_3g     True\n",
       "210          jun_vbc_3g     True\n",
       "211          sep_vbc_3g     True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16 columns dropped.\n"
     ]
    }
   ],
   "source": [
    "# Include the columns extracted in the above list in the main data set \n",
    "# These columns will have the percentage of missing values less than the threshold\n",
    "drop_missing = pd.DataFrame({'features':churn.columns , 'include': include_cols})\n",
    "drop_missing.loc[drop_missing.include == True,:]\n",
    "\n",
    "# Display the number of columns dropped\n",
    "churn = churn.loc[:, include_cols]\n",
    "\n",
    "dropped_cols = churn.shape[1] - initial_cols\n",
    "print(\"{0} columns dropped.\".format(dropped_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IPnD_JTIeisC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc_og_t2o_mou        1.018010\n",
       "std_og_t2o_mou        1.018010\n",
       "loc_ic_t2o_mou        1.018010\n",
       "arpu_6                0.000000\n",
       "arpu_7                0.000000\n",
       "arpu_8                0.000000\n",
       "arpu_9                0.000000\n",
       "onnet_mou_6           3.937039\n",
       "onnet_mou_7           3.859039\n",
       "onnet_mou_8           5.378054\n",
       "onnet_mou_9           7.745077\n",
       "offnet_mou_6          3.937039\n",
       "offnet_mou_7          3.859039\n",
       "offnet_mou_8          5.378054\n",
       "offnet_mou_9          7.745077\n",
       "roam_ic_mou_6         3.937039\n",
       "roam_ic_mou_7         3.859039\n",
       "roam_ic_mou_8         5.378054\n",
       "roam_ic_mou_9         7.745077\n",
       "roam_og_mou_6         3.937039\n",
       "roam_og_mou_7         3.859039\n",
       "roam_og_mou_8         5.378054\n",
       "roam_og_mou_9         7.745077\n",
       "loc_og_t2t_mou_6      3.937039\n",
       "loc_og_t2t_mou_7      3.859039\n",
       "loc_og_t2t_mou_8      5.378054\n",
       "loc_og_t2t_mou_9      7.745077\n",
       "loc_og_t2m_mou_6      3.937039\n",
       "loc_og_t2m_mou_7      3.859039\n",
       "loc_og_t2m_mou_8      5.378054\n",
       "loc_og_t2m_mou_9      7.745077\n",
       "loc_og_t2f_mou_6      3.937039\n",
       "loc_og_t2f_mou_7      3.859039\n",
       "loc_og_t2f_mou_8      5.378054\n",
       "loc_og_t2f_mou_9      7.745077\n",
       "loc_og_t2c_mou_6      3.937039\n",
       "loc_og_t2c_mou_7      3.859039\n",
       "loc_og_t2c_mou_8      5.378054\n",
       "loc_og_t2c_mou_9      7.745077\n",
       "loc_og_mou_6          3.937039\n",
       "loc_og_mou_7          3.859039\n",
       "loc_og_mou_8          5.378054\n",
       "loc_og_mou_9          7.745077\n",
       "std_og_t2t_mou_6      3.937039\n",
       "std_og_t2t_mou_7      3.859039\n",
       "std_og_t2t_mou_8      5.378054\n",
       "std_og_t2t_mou_9      7.745077\n",
       "std_og_t2m_mou_6      3.937039\n",
       "std_og_t2m_mou_7      3.859039\n",
       "std_og_t2m_mou_8      5.378054\n",
       "std_og_t2m_mou_9      7.745077\n",
       "std_og_t2f_mou_6      3.937039\n",
       "std_og_t2f_mou_7      3.859039\n",
       "std_og_t2f_mou_8      5.378054\n",
       "std_og_t2f_mou_9      7.745077\n",
       "std_og_t2c_mou_6      3.937039\n",
       "std_og_t2c_mou_7      3.859039\n",
       "std_og_t2c_mou_8      5.378054\n",
       "std_og_t2c_mou_9      7.745077\n",
       "std_og_mou_6          3.937039\n",
       "std_og_mou_7          3.859039\n",
       "std_og_mou_8          5.378054\n",
       "std_og_mou_9          7.745077\n",
       "isd_og_mou_6          3.937039\n",
       "isd_og_mou_7          3.859039\n",
       "isd_og_mou_8          5.378054\n",
       "isd_og_mou_9          7.745077\n",
       "spl_og_mou_6          3.937039\n",
       "spl_og_mou_7          3.859039\n",
       "spl_og_mou_8          5.378054\n",
       "spl_og_mou_9          7.745077\n",
       "og_others_6           3.937039\n",
       "og_others_7           3.859039\n",
       "og_others_8           5.378054\n",
       "og_others_9           7.745077\n",
       "total_og_mou_6        0.000000\n",
       "total_og_mou_7        0.000000\n",
       "total_og_mou_8        0.000000\n",
       "total_og_mou_9        0.000000\n",
       "loc_ic_t2t_mou_6      3.937039\n",
       "loc_ic_t2t_mou_7      3.859039\n",
       "loc_ic_t2t_mou_8      5.378054\n",
       "loc_ic_t2t_mou_9      7.745077\n",
       "loc_ic_t2m_mou_6      3.937039\n",
       "loc_ic_t2m_mou_7      3.859039\n",
       "loc_ic_t2m_mou_8      5.378054\n",
       "loc_ic_t2m_mou_9      7.745077\n",
       "loc_ic_t2f_mou_6      3.937039\n",
       "loc_ic_t2f_mou_7      3.859039\n",
       "loc_ic_t2f_mou_8      5.378054\n",
       "loc_ic_t2f_mou_9      7.745077\n",
       "loc_ic_mou_6          3.937039\n",
       "loc_ic_mou_7          3.859039\n",
       "loc_ic_mou_8          5.378054\n",
       "loc_ic_mou_9          7.745077\n",
       "std_ic_t2t_mou_6      3.937039\n",
       "std_ic_t2t_mou_7      3.859039\n",
       "std_ic_t2t_mou_8      5.378054\n",
       "std_ic_t2t_mou_9      7.745077\n",
       "std_ic_t2m_mou_6      3.937039\n",
       "std_ic_t2m_mou_7      3.859039\n",
       "std_ic_t2m_mou_8      5.378054\n",
       "std_ic_t2m_mou_9      7.745077\n",
       "std_ic_t2f_mou_6      3.937039\n",
       "std_ic_t2f_mou_7      3.859039\n",
       "std_ic_t2f_mou_8      5.378054\n",
       "std_ic_t2f_mou_9      7.745077\n",
       "std_ic_t2o_mou_6      3.937039\n",
       "std_ic_t2o_mou_7      3.859039\n",
       "std_ic_t2o_mou_8      5.378054\n",
       "std_ic_t2o_mou_9      7.745077\n",
       "std_ic_mou_6          3.937039\n",
       "std_ic_mou_7          3.859039\n",
       "std_ic_mou_8          5.378054\n",
       "std_ic_mou_9          7.745077\n",
       "total_ic_mou_6        0.000000\n",
       "total_ic_mou_7        0.000000\n",
       "total_ic_mou_8        0.000000\n",
       "total_ic_mou_9        0.000000\n",
       "spl_ic_mou_6          3.937039\n",
       "spl_ic_mou_7          3.859039\n",
       "spl_ic_mou_8          5.378054\n",
       "spl_ic_mou_9          7.745077\n",
       "isd_ic_mou_6          3.937039\n",
       "isd_ic_mou_7          3.859039\n",
       "isd_ic_mou_8          5.378054\n",
       "isd_ic_mou_9          7.745077\n",
       "ic_others_6           3.937039\n",
       "ic_others_7           3.859039\n",
       "ic_others_8           5.378054\n",
       "ic_others_9           7.745077\n",
       "total_rech_num_6      0.000000\n",
       "total_rech_num_7      0.000000\n",
       "total_rech_num_8      0.000000\n",
       "total_rech_num_9      0.000000\n",
       "total_rech_amt_6      0.000000\n",
       "total_rech_amt_7      0.000000\n",
       "total_rech_amt_8      0.000000\n",
       "total_rech_amt_9      0.000000\n",
       "max_rech_amt_6        0.000000\n",
       "max_rech_amt_7        0.000000\n",
       "max_rech_amt_8        0.000000\n",
       "max_rech_amt_9        0.000000\n",
       "last_day_rch_amt_6    0.000000\n",
       "last_day_rch_amt_7    0.000000\n",
       "last_day_rch_amt_8    0.000000\n",
       "last_day_rch_amt_9    0.000000\n",
       "total_rech_data_6     0.000000\n",
       "total_rech_data_7     0.000000\n",
       "total_rech_data_8     0.000000\n",
       "total_rech_data_9     0.000000\n",
       "max_rech_data_6       0.000000\n",
       "max_rech_data_7       0.000000\n",
       "max_rech_data_8       0.000000\n",
       "max_rech_data_9       0.000000\n",
       "av_rech_amt_data_6    0.000000\n",
       "av_rech_amt_data_7    0.000000\n",
       "av_rech_amt_data_8    0.000000\n",
       "av_rech_amt_data_9    0.000000\n",
       "vol_2g_mb_6           0.000000\n",
       "vol_2g_mb_7           0.000000\n",
       "vol_2g_mb_8           0.000000\n",
       "vol_2g_mb_9           0.000000\n",
       "vol_3g_mb_6           0.000000\n",
       "vol_3g_mb_7           0.000000\n",
       "vol_3g_mb_8           0.000000\n",
       "vol_3g_mb_9           0.000000\n",
       "night_pck_user_6      0.000000\n",
       "night_pck_user_7      0.000000\n",
       "night_pck_user_8      0.000000\n",
       "night_pck_user_9      0.000000\n",
       "monthly_2g_6          0.000000\n",
       "monthly_2g_7          0.000000\n",
       "monthly_2g_8          0.000000\n",
       "monthly_2g_9          0.000000\n",
       "sachet_2g_6           0.000000\n",
       "sachet_2g_7           0.000000\n",
       "sachet_2g_8           0.000000\n",
       "sachet_2g_9           0.000000\n",
       "monthly_3g_6          0.000000\n",
       "monthly_3g_7          0.000000\n",
       "monthly_3g_8          0.000000\n",
       "monthly_3g_9          0.000000\n",
       "sachet_3g_6           0.000000\n",
       "sachet_3g_7           0.000000\n",
       "sachet_3g_8           0.000000\n",
       "sachet_3g_9           0.000000\n",
       "fb_user_6             0.000000\n",
       "fb_user_7             0.000000\n",
       "fb_user_8             0.000000\n",
       "fb_user_9             0.000000\n",
       "aon                   0.000000\n",
       "aug_vbc_3g            0.000000\n",
       "jul_vbc_3g            0.000000\n",
       "jun_vbc_3g            0.000000\n",
       "sep_vbc_3g            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at missing value ratio in each column\n",
    "churn.isnull().sum()*100/churn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVbD6u5kd81i"
   },
   "source": [
    "### iv) Impute missing values using MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMdp2spW14xJ"
   },
   "source": [
    "[MICE](https://scikit-learn.org/stable/modules/impute.html) is called \"Multiple Imputation by Chained Equation\". It uses machine learning techniques in order to see what are the trends in the values of that column. Using this information, it will smartly fill in the missing values in that column.\n",
    "\n",
    "MICE is now called Iterative Imputer. \n",
    "\n",
    "You can specify the machine learning algorithm to be used in order to fill in the missing values of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fL3-VmJL14xK"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN3kmZ0GIong"
   },
   "source": [
    "So, we will be using linear regression for filling the missing values in the rest of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7iDa6Llpd81i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (99999, 196)\n",
      "[IterativeImputer] Ending imputation round 1/1, elapsed time 398.65\n",
      "[IterativeImputer] Change: 242732.7527920915, scaled tolerance: 45.735400000000006 \n"
     ]
    }
   ],
   "source": [
    "churn_cols = churn.columns\n",
    "\n",
    "# using MICE technique to impute missing values in the rest of the columns\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Implement the Iterative Imputer technique to impute appropriate values in the missing entries of the rest of the numeric columns\n",
    "# Note: Set the 'estimator' parameter to 'lr'  - This specifies that we will be using linear regression to estimate the missing values\n",
    "# Note: Set the 'missing_values' parameter to 'np.nan' - This specifies that we have impute the entries which are NaNs\n",
    "# Note: Set the 'max_iter' parameter to '1' - This specifies the number of iterations the algorithm scans through the data set \n",
    "#       to converge to appropriate values it is going to impute in the missing entries. It takes around 6 min to run.\n",
    "# Note: Set the 'verbose' parameter to '2' - This specifies the amount of details it will show while imputing \n",
    "# Note: Set the 'imputation_order' parameter to 'roman' - This specifies the order in which features will be imputed. 'roman' means left to right\n",
    "# Note: Set the 'random_state' parameter to '0' - This is for reproducibility\n",
    "\n",
    "imp = IterativeImputer(estimator = lr, missing_values = np.nan, max_iter = 1, verbose = 2, imputation_order = 'roman', random_state = 0 )\n",
    "\n",
    "churn = imp.fit_transform(churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iOG-vjZ8uWAE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.  ,    0.  ,    0.  , ...,    0.  ,  101.2 ,    3.58],\n",
       "       [   0.  ,    0.  ,    0.  , ...,    0.  ,    0.  ,    0.  ],\n",
       "       [   0.  ,    0.  ,    0.  , ...,    0.  ,    4.17,    0.  ],\n",
       "       ...,\n",
       "       [   0.  ,    0.  ,    0.  , ...,    0.  ,    0.  ,    0.  ],\n",
       "       [   0.  ,    0.  ,    0.  , ..., 1151.03, 1173.18,    0.  ],\n",
       "       [   0.  ,    0.  ,    0.  , ...,    0.  ,    0.  ,    0.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5RX4Qew1d81i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc_og_t2o_mou        0.0\n",
      "std_og_t2o_mou        0.0\n",
      "loc_ic_t2o_mou        0.0\n",
      "arpu_6                0.0\n",
      "arpu_7                0.0\n",
      "arpu_8                0.0\n",
      "arpu_9                0.0\n",
      "onnet_mou_6           0.0\n",
      "onnet_mou_7           0.0\n",
      "onnet_mou_8           0.0\n",
      "onnet_mou_9           0.0\n",
      "offnet_mou_6          0.0\n",
      "offnet_mou_7          0.0\n",
      "offnet_mou_8          0.0\n",
      "offnet_mou_9          0.0\n",
      "roam_ic_mou_6         0.0\n",
      "roam_ic_mou_7         0.0\n",
      "roam_ic_mou_8         0.0\n",
      "roam_ic_mou_9         0.0\n",
      "roam_og_mou_6         0.0\n",
      "roam_og_mou_7         0.0\n",
      "roam_og_mou_8         0.0\n",
      "roam_og_mou_9         0.0\n",
      "loc_og_t2t_mou_6      0.0\n",
      "loc_og_t2t_mou_7      0.0\n",
      "loc_og_t2t_mou_8      0.0\n",
      "loc_og_t2t_mou_9      0.0\n",
      "loc_og_t2m_mou_6      0.0\n",
      "loc_og_t2m_mou_7      0.0\n",
      "loc_og_t2m_mou_8      0.0\n",
      "loc_og_t2m_mou_9      0.0\n",
      "loc_og_t2f_mou_6      0.0\n",
      "loc_og_t2f_mou_7      0.0\n",
      "loc_og_t2f_mou_8      0.0\n",
      "loc_og_t2f_mou_9      0.0\n",
      "loc_og_t2c_mou_6      0.0\n",
      "loc_og_t2c_mou_7      0.0\n",
      "loc_og_t2c_mou_8      0.0\n",
      "loc_og_t2c_mou_9      0.0\n",
      "loc_og_mou_6          0.0\n",
      "loc_og_mou_7          0.0\n",
      "loc_og_mou_8          0.0\n",
      "loc_og_mou_9          0.0\n",
      "std_og_t2t_mou_6      0.0\n",
      "std_og_t2t_mou_7      0.0\n",
      "std_og_t2t_mou_8      0.0\n",
      "std_og_t2t_mou_9      0.0\n",
      "std_og_t2m_mou_6      0.0\n",
      "std_og_t2m_mou_7      0.0\n",
      "std_og_t2m_mou_8      0.0\n",
      "std_og_t2m_mou_9      0.0\n",
      "std_og_t2f_mou_6      0.0\n",
      "std_og_t2f_mou_7      0.0\n",
      "std_og_t2f_mou_8      0.0\n",
      "std_og_t2f_mou_9      0.0\n",
      "std_og_t2c_mou_6      0.0\n",
      "std_og_t2c_mou_7      0.0\n",
      "std_og_t2c_mou_8      0.0\n",
      "std_og_t2c_mou_9      0.0\n",
      "std_og_mou_6          0.0\n",
      "std_og_mou_7          0.0\n",
      "std_og_mou_8          0.0\n",
      "std_og_mou_9          0.0\n",
      "isd_og_mou_6          0.0\n",
      "isd_og_mou_7          0.0\n",
      "isd_og_mou_8          0.0\n",
      "isd_og_mou_9          0.0\n",
      "spl_og_mou_6          0.0\n",
      "spl_og_mou_7          0.0\n",
      "spl_og_mou_8          0.0\n",
      "spl_og_mou_9          0.0\n",
      "og_others_6           0.0\n",
      "og_others_7           0.0\n",
      "og_others_8           0.0\n",
      "og_others_9           0.0\n",
      "total_og_mou_6        0.0\n",
      "total_og_mou_7        0.0\n",
      "total_og_mou_8        0.0\n",
      "total_og_mou_9        0.0\n",
      "loc_ic_t2t_mou_6      0.0\n",
      "loc_ic_t2t_mou_7      0.0\n",
      "loc_ic_t2t_mou_8      0.0\n",
      "loc_ic_t2t_mou_9      0.0\n",
      "loc_ic_t2m_mou_6      0.0\n",
      "loc_ic_t2m_mou_7      0.0\n",
      "loc_ic_t2m_mou_8      0.0\n",
      "loc_ic_t2m_mou_9      0.0\n",
      "loc_ic_t2f_mou_6      0.0\n",
      "loc_ic_t2f_mou_7      0.0\n",
      "loc_ic_t2f_mou_8      0.0\n",
      "loc_ic_t2f_mou_9      0.0\n",
      "loc_ic_mou_6          0.0\n",
      "loc_ic_mou_7          0.0\n",
      "loc_ic_mou_8          0.0\n",
      "loc_ic_mou_9          0.0\n",
      "std_ic_t2t_mou_6      0.0\n",
      "std_ic_t2t_mou_7      0.0\n",
      "std_ic_t2t_mou_8      0.0\n",
      "std_ic_t2t_mou_9      0.0\n",
      "std_ic_t2m_mou_6      0.0\n",
      "std_ic_t2m_mou_7      0.0\n",
      "std_ic_t2m_mou_8      0.0\n",
      "std_ic_t2m_mou_9      0.0\n",
      "std_ic_t2f_mou_6      0.0\n",
      "std_ic_t2f_mou_7      0.0\n",
      "std_ic_t2f_mou_8      0.0\n",
      "std_ic_t2f_mou_9      0.0\n",
      "std_ic_t2o_mou_6      0.0\n",
      "std_ic_t2o_mou_7      0.0\n",
      "std_ic_t2o_mou_8      0.0\n",
      "std_ic_t2o_mou_9      0.0\n",
      "std_ic_mou_6          0.0\n",
      "std_ic_mou_7          0.0\n",
      "std_ic_mou_8          0.0\n",
      "std_ic_mou_9          0.0\n",
      "total_ic_mou_6        0.0\n",
      "total_ic_mou_7        0.0\n",
      "total_ic_mou_8        0.0\n",
      "total_ic_mou_9        0.0\n",
      "spl_ic_mou_6          0.0\n",
      "spl_ic_mou_7          0.0\n",
      "spl_ic_mou_8          0.0\n",
      "spl_ic_mou_9          0.0\n",
      "isd_ic_mou_6          0.0\n",
      "isd_ic_mou_7          0.0\n",
      "isd_ic_mou_8          0.0\n",
      "isd_ic_mou_9          0.0\n",
      "ic_others_6           0.0\n",
      "ic_others_7           0.0\n",
      "ic_others_8           0.0\n",
      "ic_others_9           0.0\n",
      "total_rech_num_6      0.0\n",
      "total_rech_num_7      0.0\n",
      "total_rech_num_8      0.0\n",
      "total_rech_num_9      0.0\n",
      "total_rech_amt_6      0.0\n",
      "total_rech_amt_7      0.0\n",
      "total_rech_amt_8      0.0\n",
      "total_rech_amt_9      0.0\n",
      "max_rech_amt_6        0.0\n",
      "max_rech_amt_7        0.0\n",
      "max_rech_amt_8        0.0\n",
      "max_rech_amt_9        0.0\n",
      "last_day_rch_amt_6    0.0\n",
      "last_day_rch_amt_7    0.0\n",
      "last_day_rch_amt_8    0.0\n",
      "last_day_rch_amt_9    0.0\n",
      "total_rech_data_6     0.0\n",
      "total_rech_data_7     0.0\n",
      "total_rech_data_8     0.0\n",
      "total_rech_data_9     0.0\n",
      "max_rech_data_6       0.0\n",
      "max_rech_data_7       0.0\n",
      "max_rech_data_8       0.0\n",
      "max_rech_data_9       0.0\n",
      "av_rech_amt_data_6    0.0\n",
      "av_rech_amt_data_7    0.0\n",
      "av_rech_amt_data_8    0.0\n",
      "av_rech_amt_data_9    0.0\n",
      "vol_2g_mb_6           0.0\n",
      "vol_2g_mb_7           0.0\n",
      "vol_2g_mb_8           0.0\n",
      "vol_2g_mb_9           0.0\n",
      "vol_3g_mb_6           0.0\n",
      "vol_3g_mb_7           0.0\n",
      "vol_3g_mb_8           0.0\n",
      "vol_3g_mb_9           0.0\n",
      "night_pck_user_6      0.0\n",
      "night_pck_user_7      0.0\n",
      "night_pck_user_8      0.0\n",
      "night_pck_user_9      0.0\n",
      "monthly_2g_6          0.0\n",
      "monthly_2g_7          0.0\n",
      "monthly_2g_8          0.0\n",
      "monthly_2g_9          0.0\n",
      "sachet_2g_6           0.0\n",
      "sachet_2g_7           0.0\n",
      "sachet_2g_8           0.0\n",
      "sachet_2g_9           0.0\n",
      "monthly_3g_6          0.0\n",
      "monthly_3g_7          0.0\n",
      "monthly_3g_8          0.0\n",
      "monthly_3g_9          0.0\n",
      "sachet_3g_6           0.0\n",
      "sachet_3g_7           0.0\n",
      "sachet_3g_8           0.0\n",
      "sachet_3g_9           0.0\n",
      "fb_user_6             0.0\n",
      "fb_user_7             0.0\n",
      "fb_user_8             0.0\n",
      "fb_user_9             0.0\n",
      "aon                   0.0\n",
      "aug_vbc_3g            0.0\n",
      "jul_vbc_3g            0.0\n",
      "jun_vbc_3g            0.0\n",
      "sep_vbc_3g            0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# convert imputed numpy array to pandas dataframe\n",
    "churn = pd.DataFrame(churn, columns=churn_cols)\n",
    "print(churn.isnull().sum()*100/churn.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffpvc8NGIwhQ"
   },
   "source": [
    "You can now see that we have removed or filled all the missing values from the data set. We will now proceed to feature engineering to further prepare the data for testing machine learning and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0aju3lDd81i"
   },
   "source": [
    "# Task 3: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX4iEYxM14xL"
   },
   "source": [
    "### Filter high-value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMm21PvOd81i"
   },
   "source": [
    "### Calculate total data recharge amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dd-oajkqd81i"
   },
   "outputs": [],
   "source": [
    "# calculate and store the total data recharge amount for June --> number of data recharges * average data recharge amount\n",
    "# You have to use the total recharge for data and the average recharge amount for data\n",
    "# June, July, August and September - The months are encoded as 6, 7, 8 and 9, respectively.\n",
    "\n",
    "churn['total_data_rech_6'] = churn.total_rech_data_6 * churn.av_rech_amt_data_6\n",
    "\n",
    "# calculate and store the total data recharge amount for July --> number of data recharges * average data recharge amount\n",
    "\n",
    "churn['total_data_rech_7'] = churn.total_rech_data_7 * churn.av_rech_amt_data_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikMe4dp2d81j"
   },
   "source": [
    "### Add total data recharge and total recharge to get total combined recharge amount for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7mMlFJZ6d81j"
   },
   "outputs": [],
   "source": [
    "# calculate and store total recharge amount for call and internet data for June --> total call recharge amount + total data recharge amount\n",
    "\n",
    "churn['amt_data_6'] = churn.total_rech_amt_6 + churn.total_data_rech_6\n",
    "\n",
    "# calculate and store total recharge amount for call and internet data for July --> total call recharge amount + total data recharge amount\n",
    "\n",
    "churn['amt_data_7'] = churn.total_rech_amt_7 + churn.total_data_rech_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zK6nozryd81j"
   },
   "outputs": [],
   "source": [
    "# calculate average data recharge amount done by customer in June and July\n",
    "\n",
    "churn['av_amt_data_6_7'] = (churn.amt_data_6 + churn.amt_data_7)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "JFZVPQZad81j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recharge amount at 70th percentile: 478.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate and display the 70th percentile average data recharge amount of June and July\n",
    "\n",
    "print(\"Recharge amount at 70th percentile: {0}\".format(churn.av_amt_data_6_7.quantile(0.7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nFQ5xy7Ad81j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30001, 201)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retain only those customers who have recharged their mobiles with more than or equal to 70th percentile amount\n",
    "# You have see whether each customer row has the average data recharge amount more than the 70th percentile of the average data recharge amount\n",
    "\n",
    "churn_filtered = churn.loc[churn.av_amt_data_6_7 >= churn.av_amt_data_6_7.quantile(0.7), :]\n",
    "churn_filtered = churn_filtered.reset_index(drop=True)\n",
    "churn_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uDYloA8Zd81j"
   },
   "outputs": [],
   "source": [
    "# delete variables created to filter high-value customers\n",
    "\n",
    "churn_filtered = churn_filtered.drop(['total_data_rech_6', 'total_data_rech_7',\n",
    "                                     'amt_data_6', 'amt_data_7', 'av_amt_data_6_7'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RWJWnBjZvGcD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30001, 196)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of customers retained in the data set\n",
    "\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLxHeovyd81j"
   },
   "source": [
    "### Derive churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UBQRZGlzd81j"
   },
   "outputs": [],
   "source": [
    "# calculate total incoming and outgoing minutes of usage for the month of September\n",
    "\n",
    "churn_filtered['total_calls_mou_9'] = churn_filtered.total_ic_mou_9 + churn_filtered.total_og_mou_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6rPBN3x_d81j"
   },
   "outputs": [],
   "source": [
    "# calculate the total volumn of 2g and 3g data consumption for the month of September\n",
    "\n",
    "churn_filtered['total_internet_mb_9'] =  churn_filtered.vol_2g_mb_9 + churn_filtered.vol_3g_mb_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gbNXqSJqd81j"
   },
   "outputs": [],
   "source": [
    "# create churn variable: those who have not used either calls or internet in the month of September are customers who have churned\n",
    "# 0 - not churn, 1 - churn\n",
    "\n",
    "churn_filtered['churn'] = churn_filtered.apply(lambda row: 1 if (row.total_calls_mou_9 == 0 and row.total_internet_mb_9 == 0) else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "CJysD9WGd81j"
   },
   "outputs": [],
   "source": [
    "# delete derived variables\n",
    "\n",
    "churn_filtered = churn_filtered.drop(['total_calls_mou_9', 'total_internet_mb_9'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-z-9hywed81k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn Ratio:\n",
      "0    91.863605\n",
      "1     8.136395\n",
      "Name: churn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# change the 'churn' variable data type to 'category'\n",
    "\n",
    "churn_filtered.churn = churn_filtered.churn.astype(\"category\")\n",
    "\n",
    "# display the churn ratio\n",
    "\n",
    "print(\"Churn Ratio:\")\n",
    "print(churn_filtered.churn.value_counts()*100/churn_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB1Lknlid81k"
   },
   "source": [
    "### Calculate difference between 8th and previous months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQVfi5tJd81k"
   },
   "source": [
    "Let's derive some variables. The most important feature, in this situation, can be the difference between the 8th month and the previous months. The difference can be in patterns such as usage difference or recharge value difference. Let's calculate difference variable as the difference between 8th month and the average of 6th and 7th month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6qAeeSRvd81k"
   },
   "outputs": [],
   "source": [
    "cols =  ['arpu',\n",
    "         'onnet_mou',\n",
    "         'offnet_mou',\n",
    "         'roam_ic_mou',\n",
    "         'roam_og_mou',\n",
    "         'loc_og_mou',\n",
    "         'std_og_mou',\n",
    "         'isd_og_mou',\n",
    "         'spl_og_mou',\n",
    "         'total_og_mou',\n",
    "         'loc_ic_mou',\n",
    "         'std_ic_mou',\n",
    "         'isd_ic_mou',\n",
    "         'spl_ic_mou',\n",
    "         'total_ic_mou',\n",
    "         'total_rech_num',\n",
    "         'total_rech_amt',\n",
    "         'max_rech_amt',\n",
    "         'total_rech_data',\n",
    "         'max_rech_data',\n",
    "         'av_rech_amt_data',\n",
    "         'vol_2g_mb',\n",
    "         'vol_3g_mb'\n",
    "         ]\n",
    "\n",
    "# Create new columns that hold the value of the difference between the variable value \n",
    "# in the month of August and average of the variable values in the month of June and July\n",
    "\n",
    "churn_filtered['arpu_diff'] = churn_filtered.arpu_8 - ((churn_filtered.arpu_6 + churn_filtered.arpu_7)/2)\n",
    "\n",
    "churn_filtered['onnet_mou_diff'] = churn_filtered.onnet_mou_8 - ((churn_filtered.onnet_mou_6 + churn_filtered.onnet_mou_7)/2)\n",
    "\n",
    "churn_filtered['offnet_mou_diff'] = churn_filtered.offnet_mou_8 - ((churn_filtered.offnet_mou_6 + churn_filtered.offnet_mou_7)/2)\n",
    "\n",
    "churn_filtered['roam_ic_mou_diff'] = churn_filtered.roam_ic_mou_8 - ((churn_filtered.roam_ic_mou_6 + churn_filtered.roam_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['roam_og_mou_diff'] = churn_filtered.roam_og_mou_8 - ((churn_filtered.roam_og_mou_6 + churn_filtered.roam_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['loc_og_mou_diff'] = churn_filtered.loc_og_mou_8 - ((churn_filtered.loc_og_mou_6 + churn_filtered.loc_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['std_og_mou_diff'] = churn_filtered.std_og_mou_8 - ((churn_filtered.std_og_mou_6 + churn_filtered.std_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['isd_og_mou_diff'] = churn_filtered.isd_og_mou_8 - ((churn_filtered.isd_og_mou_6 + churn_filtered.isd_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['spl_og_mou_diff'] = churn_filtered.spl_og_mou_8 - ((churn_filtered.spl_og_mou_6 + churn_filtered.spl_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['total_og_mou_diff'] = churn_filtered.total_og_mou_8 - ((churn_filtered.total_og_mou_6 + churn_filtered.total_og_mou_7)/2)\n",
    "\n",
    "churn_filtered['loc_ic_mou_diff'] = churn_filtered.loc_ic_mou_8 - ((churn_filtered.loc_ic_mou_6 + churn_filtered.loc_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['std_ic_mou_diff'] = churn_filtered.std_ic_mou_8 - ((churn_filtered.std_ic_mou_6 + churn_filtered.std_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['isd_ic_mou_diff'] = churn_filtered.isd_ic_mou_8 - ((churn_filtered.isd_ic_mou_6 + churn_filtered.isd_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['spl_ic_mou_diff'] = churn_filtered.spl_ic_mou_8 - ((churn_filtered.spl_ic_mou_6 + churn_filtered.spl_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['total_ic_mou_diff'] = churn_filtered.total_ic_mou_8 - ((churn_filtered.total_ic_mou_6 + churn_filtered.total_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['total_rech_num_diff'] = churn_filtered.total_rech_num_8 - ((churn_filtered.total_rech_num_6 + churn_filtered.total_rech_num_7)/2)\n",
    "\n",
    "churn_filtered['total_rech_amt_diff'] = churn_filtered.total_rech_amt_8 - ((churn_filtered.total_rech_amt_6 + churn_filtered.total_rech_amt_7)/2)\n",
    "\n",
    "churn_filtered['max_rech_amt_diff'] = churn_filtered.max_rech_amt_8 - ((churn_filtered.max_rech_amt_6 + churn_filtered.max_rech_amt_7)/2)\n",
    "\n",
    "churn_filtered['total_rech_data_diff'] = churn_filtered.total_rech_data_8 - ((churn_filtered.total_rech_data_6 + churn_filtered.total_rech_data_7)/2)\n",
    "\n",
    "churn_filtered['max_rech_data_diff'] = churn_filtered.max_rech_data_8 - ((churn_filtered.max_rech_data_6 + churn_filtered.max_rech_data_7)/2)\n",
    "\n",
    "churn_filtered['av_rech_amt_data_diff'] = churn_filtered.av_rech_amt_data_8 - ((churn_filtered.av_rech_amt_data_6 + churn_filtered.av_rech_amt_data_7)/2)\n",
    "\n",
    "churn_filtered['vol_2g_mb_diff'] = churn_filtered.vol_2g_mb_8 - ((churn_filtered.vol_2g_mb_6 + churn_filtered.vol_2g_mb_7)/2)\n",
    "\n",
    "churn_filtered['vol_3g_mb_diff'] = churn_filtered.vol_3g_mb_8 - ((churn_filtered.vol_3g_mb_6 + churn_filtered.vol_3g_mb_7)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5RXeZq3rd81k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30001.000000\n",
       "mean       -67.437337\n",
       "std        502.630069\n",
       "min      -7213.410000\n",
       "25%       -168.025000\n",
       "50%        -14.625000\n",
       "75%         67.915000\n",
       "max      12768.705000\n",
       "Name: total_og_mou_diff, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at summary of one of the difference variables\n",
    "# The variable mentioned below is the total outgoing calls minutes of usage difference between the total OG MOU in August and average of the total OG MOU of June and July\n",
    "\n",
    "churn_filtered['total_og_mou_diff'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWDxLdtLd81k"
   },
   "source": [
    "Delete columns that belong to the churn month (9th month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "x3sej5Gmd81k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30001, 173)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all variables relating to 9th month\n",
    "\n",
    "churn_filtered = churn_filtered.filter(regex='[^9]$', axis=1)\n",
    "churn_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "BTqWtPaNd81k"
   },
   "outputs": [],
   "source": [
    "# update num_cols and cat_cols column name list\n",
    "\n",
    "# extract all names that end with 9\n",
    "\n",
    "col_9_names = churn.filter(regex='9$', axis=1).columns\n",
    "\n",
    "# update cal_cols so that all the variables related to the month of September are removed\n",
    "\n",
    "cat_cols = [col for col in cat_cols if col not in col_9_names]\n",
    "cat_cols.append('churn')\n",
    "\n",
    "# update num_cols so that all the variables related to the month of September are removed\n",
    "\n",
    "num_cols = [col for col in churn_filtered.columns if col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b512tiEd81k"
   },
   "source": [
    "# Task 4: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "E49BBqd5d81k"
   },
   "outputs": [],
   "source": [
    "# ensure that all the numerical and categorical columns are of the correct data types\n",
    "\n",
    "churn_filtered[num_cols] = churn_filtered[num_cols].apply(pd.to_numeric)\n",
    "churn_filtered[cat_cols] = churn_filtered[cat_cols].apply(lambda column: column.astype(\"category\"), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZIZU7VEYd81k"
   },
   "outputs": [],
   "source": [
    "# create plotting functions\n",
    "def data_type(variable):\n",
    "    if variable.dtype == np.int64 or variable.dtype == np.float64:\n",
    "        return 'numerical'\n",
    "    elif variable.dtype == 'category':\n",
    "        return 'categorical'\n",
    "    \n",
    "def univariate(variable, stats=True):\n",
    "    \n",
    "    if data_type(variable) == 'numerical':\n",
    "        sns.distplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.describe())\n",
    "    \n",
    "    elif data_type(variable) == 'categorical':\n",
    "        sns.countplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.value_counts())\n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid variable passed: either pass a numeric variable or a categorical vairable.\")\n",
    "        \n",
    "def bivariate(var1, var2):\n",
    "    if data_type(var1) == 'numerical' and data_type(var2) == 'numerical':\n",
    "        sns.regplot(var1, var2)\n",
    "    elif (data_type(var1) == 'categorical' and data_type(var2) == 'numerical') or (data_type(var1) == 'numerical' and data_type(var2) == 'categorical'):        \n",
    "        sns.boxplot(var1, var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR9shtqMd81l"
   },
   "source": [
    "## Univariate EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "uQBgBb0Md81l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99999.000000\n",
      "mean       282.987358\n",
      "std        328.439770\n",
      "min      -2258.709000\n",
      "25%         93.411500\n",
      "50%        197.704000\n",
      "75%        371.060000\n",
      "max      27731.088000\n",
      "Name: arpu_6, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEHCAYAAABm9dtzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAelklEQVR4nO3df5BdZZ3n8fenb/8g/IySdkQCJmhYN4yOYktQR8dd1xFYNePsWBtcC0R32ZRQs7OWW8Zxf7iza5WOte4OK0vEWWpFRWREMFOiGNmVKXcNkAyIBAx0gkIPURrUxBDSnc797h/n3O7bN7fvPd3p0/2c9OdVdavvPec85z6Pt8yX5/k+z3MUEZiZmc2HnsWugJmZHT8cVMzMbN44qJiZ2bxxUDEzs3njoGJmZvOmd7ErsJhWrFgRq1atWuxqmJlVyo4dO56JiMF250oNKpIuAv4CqAF/GRGfajmv/PwlwEHg/RHxt53KSvoM8E5gHNgNXBERv5a0CngE2JXffltEbOxUv1WrVrF9+/b5aKqZ2ZIh6WcznStt+EtSDbgWuBhYC1wqaW3LZRcDa/LXlcB1BcpuBX47Il4FPAp8rOl+uyPi1fmrY0AxM7P5V2ZO5QJgOCL2RMQ4cDOwvuWa9cCNkdkGLJd0RqeyEfHdiJjIy28DVpbYBjMzm4Uyg8qZwJNNn0fyY0WuKVIW4APAt5s+r5Z0v6S7Jb1prhU3M7O5KTOnojbHWveEmemarmUlfRyYAL6SH9oLnB0Rz0p6LXC7pPMiYn9LuSvJhto4++yzuzbCzMyKK7OnMgKc1fR5JfBUwWs6lpV0OfAO4J9FvnlZRIxFxLP5+x1kSfxzWysVEddHxFBEDA0Otp28YGZmc1RmULkPWCNptaR+YAOwpeWaLcBlylwI7IuIvZ3K5rPCPgq8KyIONm4kaTBP8CPpHLLk/54S22dmZi1KG/6KiAlJVwN3kk0LviEidkramJ/fDNxBNp14mGxK8RWdyua3/hwwAGzNZiRPTh1+M/BnkiaAI8DGiPhlWe0zM7OjaSlvfT80NBRep2JmNjuSdkTEULtz3qYlUf9166N8+js/WexqmJnNypLepiVl9z7+Sw4ePrLY1TAzmxX3VBJVj2DMQcXMKsZBJVERMD5RX+xqmJnNioNKouoRjDmomFnFOKgkKgsqHv4ys2pxUElUAGOH3VMxs2pxUElUPfDwl5lVjoNKoiKC8SN16vWluzjVzKrHQSVR9Xyng/Ej7q2YWXU4qCSqnscS51XMrEocVBLV6Kl4BpiZVYmDSqIa+3w6WW9mVeKgkij3VMysihxUEtUIKoecUzGzCnFQSZSHv8ysihxUEuXhLzOrIgeVRNXdUzGzCnJQSdRkT8U5FTOrEAeVRE3lVDz8ZWbV4aCSqJjMqbinYmbV4aCSKOdUzKyKHFQSNZVT8fCXmVWHg0qi3FMxsypyUEmUcypmVkUOKony4kczqyIHlURNDn95nYqZVYiDSqLqHv4yswpyUEmUFz+aWRU5qCTKPRUzqyIHlUR57y8zqyIHlUTVPfxlZhXkoJIqL340swpyUEmUcypmVkWlBhVJF0naJWlY0qY25yXpmvz8g5LO71ZW0mck/SS//jZJy5vOfSy/fpekt5fZtrJ57y8zq6LSgoqkGnAtcDGwFrhU0tqWyy4G1uSvK4HrCpTdCvx2RLwKeBT4WF5mLbABOA+4CPgf+X0qqZFTGXdPxcwqpMyeygXAcETsiYhx4GZgfcs164EbI7MNWC7pjE5lI+K7ETGRl98GrGy6180RMRYRjwPD+X0qp7HvF3j4y8yqpcygcibwZNPnkfxYkWuKlAX4APDtWXwfkq6UtF3S9tHR0QLNWHj1qZji2V9mVillBhW1ORYFr+laVtLHgQngK7P4PiLi+ogYioihwcHBNkUWX725p+J1KmZWIb0l3nsEOKvp80rgqYLX9HcqK+ly4B3AW2NqrKjI91XCtKByxEHFzKqjzJ7KfcAaSasl9ZMl0be0XLMFuCyfBXYhsC8i9nYqK+ki4KPAuyLiYMu9NkgakLSaLPl/b4ntK00jpgz09jA+UZ+WYzEzS1lpPZWImJB0NXAnUANuiIidkjbm5zcDdwCXkCXVDwJXdCqb3/pzwACwVRLAtojYmN/7FuBhsmGxqyKikgmJRk9lWX+NsYk6YxN1Tuir7EQ2M1tCyhz+IiLuIAsczcc2N70P4KqiZfPjL+/wfZ8EPjnX+qaikahf1lfj1xx2UDGzyvCK+gRN9lTyQOIZYGZWFQ4qCYo8N9/onXgGmJlVhYNKgoKpnAp4AaSZVYeDSoKacyrg4S8zqw4HlQQ1ciqTw1/uqZhZRTioJKh5SjE4p2Jm1eGgkqCYHP7Kfp5DHv4ys4pwUElQo6cy0Jv1VA57+MvMKsJBJUGNRH1/b/bzTNS9TYuZVYODSoLqeRDpq2U/z2FvKmlmFeGgkqBo7akccU/FzKrBQSVBUzmVxvCXeypmVg0OKglqDSrj7qmYWUU4qCToqES9cypmVhEOKknKokp/zTkVM6sWB5UEtfZUDjunYmYV4aCSoEZOxbO/zKxqHFQS1OiY9PZ4nYqZVYuDSoIaPZVaj+iv9XDYPRUzqwgHlQQ1Fj/2CHpr8uwvM6sMB5UENXoqPRK9PfLeX2ZWGQ4qCWoEFZTt/+WciplVhYNKguqTw1/Kh7/cUzGzanBQSVBMDn+5p2Jm1eKgkqDmnkpfrYfDzqmYWUU4qCSokVORyBL17qmYWUU4qCRo2uwvr1MxswpxUEnRtOEv+XkqZlYZDioJqjctfnSi3syqxEElQVM5lWzxo4e/zKwqHFQSVG+ZUuxEvZlVhYNKgqJ18aOnFJtZRTioJGj63l+e/WVm1eGgkqBGx0Qim/3l4S8zq4hSg4qkiyTtkjQsaVOb85J0TX7+QUnndysr6T2SdkqqSxpqOr5K0vOSHshfm8tsW5maFz969peZVUlvWTeWVAOuBd4GjAD3SdoSEQ83XXYxsCZ/rQOuA9Z1KfsQ8IfA59t87e6IeHVJTVowMW3xo2d/mVl1lNlTuQAYjog9ETEO3Aysb7lmPXBjZLYByyWd0alsRDwSEbtKrPeim7b3V0+PFz+aWWWUGVTOBJ5s+jySHytyTZGy7ayWdL+kuyW9qd0Fkq6UtF3S9tHR0QK3XHjNU4q99b2ZVUmZQUVtjrX+6zjTNUXKttoLnB0RrwE+DNwk6dSjbhJxfUQMRcTQ4OBgl1sujqlEvZxTMbNKKTOojABnNX1eCTxV8JoiZaeJiLGIeDZ/vwPYDZw7p5ovsunPU3FOxcyqo1BQkXSrpH8saTZB6D5gjaTVkvqBDcCWlmu2AJfls8AuBPZFxN6CZVvrOJgn+JF0Dlnyf88s6puM6YsfnVMxs+ooGiSuA94LPCbpU5Je0a1AREwAVwN3Ao8At0TETkkbJW3ML7uD7B/+YeALwIc6lQWQ9G5JI8DrgW9JujO/15uBByX9CPg6sDEiflmwfUlpXvzYl+/91ei9mJmlrNCU4oj4HvA9SacBlwJbJT1JFgi+HBGHZyh3B1ngaD62uel9AFcVLZsfvw24rc3xW4Fbi7Qndc2LH3trWdw/Ug96a+1STWZm6Sg8nCXpdOD9wD8H7gf+Ajgf2FpKzZawyZ5KjyYDiff/MrMqKNRTkfQN4BXAl4B35nkPgK9J2l5W5ZaqaYn6nizujx+pc0JfbTGrZWbWVdEV9X+ZD0dNkjSQz7gamqmQzc20xY+NnopngJlZBRQd/vrPbY79cD4rYlMm9/5iKqfiTSXNrAo69lQkvZhsJfsySa9halHiqcCJJddtyZq++DH7n/ywcypmVgHdhr/eTpacXwl8tun4b4A/LalOS15zTqW3xz0VM6uOjkElIr4IfFHSP8mn7NoCqNen71IMeFW9mVVCt+Gv90XEl4FVkj7cej4iPtummB2j5kR9f55T8f5fZlYF3Ya/Tsr/nlx2RWzKZKK+pzlR756KmaWv2/DX5/O//3FhqmPQuvdXI1HvnoqZpa/ohpJ/LulUSX2S7pL0jKT3lV25pSo4evGjeypmVgVF16n8fkTsB95Bti39ucC/Ka1WS1y9TU/Fs7/MrAqKBpW+/O8lwFeruvtvVUzmVAR9taltWszMUld0m5a/lvQT4HngQ5IGgUPlVWtpC2/TYmYVVainEhGbyJ5fMpRvc/8csL7Mii1l09apNHIqTtSbWQUU7akA/H2y9SrNZW6c5/oYzTkVprZpcU/FzCqg6Nb3XwJeBjwAHMkPBw4qpZjKqWhqnYp7KmZWAUV7KkPA2vAzbRdERKB8687eHvdUzKw6is7+egh4cZkVsSn1yPIpAP293qbFzKqjaE9lBfCwpHuBscbBiHhXKbVa4uoR9LT0VDz7y8yqoGhQ+USZlbDp6pHlU2Bq7y/3VMysCgoFlYi4W9JLgTUR8T1JJwJ+YHpJoqmnMrlOxQ/pMrMKKLr3178Avg58Pj90JnB7SXVa8oKpnIof0mVmVVI0UX8V8EZgP0BEPAa8qKxKLXX1ekwGlUZPZdw5FTOrgKJBZSwixhsf8gWQ/leuJFlOJXsvid4euadiZpVQNKjcLelPgWWS3gb8FfDX5VVractmf2nyc29NzqmYWSUUDSqbgFHgx8C/BO4A/m1ZlVrqmhP1kD1TxbO/zKwKis7+qku6Hbg9IkbLrZI1L36EvKfinIqZVUDHnooyn5D0DPATYJekUUn/fmGqtzTVIybXqUC2VsU9FTOrgm7DX39CNuvrdRFxekS8EFgHvFHSvy67cktVc6IeoL/W472/zKwSugWVy4BLI+LxxoGI2AO8Lz9nJWjNqWSJevdUzCx93YJKX0Q803owz6v0tbne5sFRs796nFMxs2roFlTG53jOjkFror7PORUzq4huQeV3JO1v8/oN8MpuN5d0kaRdkoYlbWpzXpKuyc8/KOn8bmUlvUfSTkl1SUMt9/tYfv0uSW/v3vw01ZuepwJep2Jm1dFxSnFEzHnTSEk14FrgbcAIcJ+kLRHxcNNlFwNr8tc64DpgXZeyDwF/yNQ+ZI3vWwtsAM4DXgJ8T9K5EXGEign3VMysoooufpyLC4DhiNiTb/FyM7C+5Zr1wI2R2QYsl3RGp7IR8UhE7GrzfeuBmyNiLJ9YMJzfp3K8+NHMqqrMoHIm8GTT55H8WJFripSdy/dVghc/mllVlRlU1OZY67+MM11TpOxcvg9JV0raLmn76GiamwMcnVPp4bBzKmZWAWUGlRHgrKbPK4GnCl5TpOxcvo+IuD4ihiJiaHBwsMstF8dRORXvUmxmFVFmULkPWCNptaR+siT6lpZrtgCX5bPALgT2RcTegmVbbQE2SBqQtJos+X/vfDZoobSuU+mr9TA+4aBiZukr+oz6WYuICUlXA3eSPXr4hojYKWljfn4z2W7Hl5Al1Q8CV3QqCyDp3cB/BwaBb0l6ICLent/7FuBhYAK4qoozv+Do4a9l/TUOTVSyKWa2xJQWVAAi4g6ywNF8bHPT+yB7qmShsvnx24DbZijzSeCTx1DlJLQm6pf113h+3EHFzNJXalCxuYmmnspN9zzBz555jv2HJrjpnicmr3nvurMXqXZmZjMrM6dic3TUNi29PRyeqJN17MzM0uWgkqB6y+LH/loPAd6qxcyS56CSoOx5KtNnfwFeVW9myXNQSVDrNi39k0HFPRUzS5uDSoKOWqfSmwcVr1Uxs8Q5qCSodUV9fy17P+7hLzNLnINKgloXPzqnYmZV4aCSoNYpxf358Jd7KmaWOgeVBEUEPU2/zGRPxTkVM0ucg0qC2j2jHmDcs7/MLHEOKgnKcipHD385p2JmqXNQSVDWU5n63NeY/eXhLzNLnINKgiJi2mMsPfvLzKrCQSVBrYsfe3uEcFAxs/Q5qCSoXp++95ck+nr99EczS5+DSoJadymGbAjMe3+ZWeocVBLUuk0LZFu1ePjLzFLnoJKgesviR8h6Kl5Rb2apc1BJUDA9pwLZWhX3VMwsdQ4qCWqd/QV5T2XCORUzS5uDSoKiZfEjZA/qck/FzFLnoJKg9j0VOadiZslzUElQ6/NUwDkVM6sGB5UE1etHTynuq/V463szS56DSoJihsWPHv4ys9Q5qCSoHiDaTSkOIjwDzMzS1bvYFbDMTfc8Mfn+ufEJ9jxzYNqxxk7FE/WY3ArfzCw17qkkKNr0VPxMFTOrAgeVBEW72V9+poqZVYCDSoKybVqmH+vrbTyn3kHFzNLloJKgdsNfkz0Vb9ViZglzUElQcPTwVyNR756KmaXMQSVBWU9luv48Ue+cipmlrNSgIukiSbskDUva1Oa8JF2Tn39Q0vndykp6oaStkh7L/74gP75K0vOSHshfm8tsW5nabX3fyKk4qJhZykoLKpJqwLXAxcBa4FJJa1suuxhYk7+uBK4rUHYTcFdErAHuyj837I6IV+evjeW0rHydZn+NHXZQMbN0ldlTuQAYjog9ETEO3Aysb7lmPXBjZLYByyWd0aXseuCL+fsvAn9QYhsWRbtE/Yn92TrVg4ePLEaVzMwKKTOonAk82fR5JD9W5JpOZX8rIvYC5H9f1HTdakn3S7pb0pvaVUrSlZK2S9o+Ojo62zYtiHZTigf6ehDw/PjEYlTJzKyQMoNKu71EWufDznRNkbKt9gJnR8RrgA8DN0k69aibRFwfEUMRMTQ4ONjllouj3fBXj8Sy/hoHx91TMbN0lRlURoCzmj6vBJ4qeE2nsr/Ih8jI/z4NEBFjEfFs/n4HsBs4d15assDazf4CONFBxcwSV2ZQuQ9YI2m1pH5gA7Cl5ZotwGX5LLALgX35kFansluAy/P3lwPfBJA0mCf4kXQOWfJ/T3nNK0+72V+Q5VUOevjLzBJW2i7FETEh6WrgTqAG3BAROyVtzM9vBu4ALgGGgYPAFZ3K5rf+FHCLpA8CTwDvyY+/GfgzSRPAEWBjRPyyrPaVpbG1/Uw9lX3PH17YCpmZzUKpW99HxB1kgaP52Oam9wFcVbRsfvxZ4K1tjt8K3HqMVV50jcRRm44KJ/bX+Pm+QwtaHzOz2fCK+sQ0nsHVbvhrWZ9zKmaWNgeVxAQdhr8Gehk/UmfCq+rNLFEOKonp1FM5sb8G4N6KmSXLQSUxk0GlzTmvqjez1DmoJGZy+KtNVFnW1+ipeFqxmaXJQSUxhYa/xtxTMbM0OagkpvPwVxZUnndOxcwS5aCSmE7DX5M5FQ9/mVmiHFQS06mn0t/bQ2+PnKg3s2Q5qCRmakV9u7DiTSXNLG0OKomZ3PurfUzJN5V0UDGzNDmoJGZq+Kt9VMmeqeKcipmlyUElMZ02lAQPf5lZ2hxUEtNp63uAU07oZf/zhyevMzNLiYNKYrol6lecPMDYRJ3RA2MLVykzs4IcVBIztaK+/fnBkwcA2DP63ALVyMysOAeVxHQb/ho8JQsqu0cPLFCNzMyKc1BJTLfhr1OX9dFXk3sqZpYkB5XEdBv+6pFYcfKAeypmliQHlcR0G/4CHFTMLFkOKonpNvwFWV5l5FfPc8h7gJlZYhxUEtNpQ8mGwZMHiICfPuu8ipmlxUElMZ22vm+YnAH2tIOKmaXFQSUx3fb+giyo9Nd6+NHIrxemUmZmBTmoJKbb7C+AvloPr33pC/jBY88sTKXMzApyUElMkeEvgN9ds4KH9+7nWW/XYmYJcVBJTJHhL4A3vnwFAP9v97NlV8nMrDAHlcR02/q+4ZVnnsYpJ/Tyf4c9BGZm6XBQSUy3Jz821HrEG152On/z6ChH6t4G38zS0LvYFbDpig5/3XTPE7zwpAGe2neIj9/2Y161cjkA7113dsk1NDObmXsqiSk6/AVw3ktOZfCUAb6/a5S6H9plZglwUElM0eEvyDaXfMu5g/x8/yF2PrW/5JqZmXXnoJKYyZ5Kl+GvhletXM4Zp53A7ff/Hb86OF5exczMCnBQScyv88DQXyv209R6xHsvOJt6BF/Z9jOe/s2hMqtnZtZRqUFF0kWSdkkalrSpzXlJuiY//6Ck87uVlfRCSVslPZb/fUHTuY/l1++S9PYy21aGiOCHu5/lRacM8JLlJxQud/rJA2x43VmMHhjjHdf8gC9t+xmPP/Pc5FCamdlCKW32l6QacC3wNmAEuE/Sloh4uOmyi4E1+WsdcB2wrkvZTcBdEfGpPNhsAj4qaS2wATgPeAnwPUnnRkRl9off88xzPLXvEO9+zZkdt75v5++9+FQ2/t7L+M5DP+ff3f4QAC857QTe8PIVDL30BSw/sZ+TBmqc2N/LyQO9nNhf46T870Bvz6y/z8ysnTKnFF8ADEfEHgBJNwPrgeagsh64MbL/pN4mabmkM4BVHcquB96Sl/8i8H3go/nxmyNiDHhc0nBehx/Od8Me2bufjV/ecdTxdh2DoH1vofXa58YmeP7wEU7qr/Hqs5bPqV5nnLaM979hFc8eGGd49AC7Rw/wrQf38vUdIx3L9faIZf01eiSkbNt9aSqrk8Ub0SPy89Ova9b42Hy4OT80/biZLZZ/8IoX8R/eed6837fMoHIm8GTT5xGy3ki3a87sUva3ImIvQETslfSipntta3OvaSRdCVyZfzwgaVfRBi2U9/8nAFYAx8ty+eOlLcdLO+D4aYvbMUd3A5+Ye/GXznSizKDS7j9EW/+zfaZripSdy/cREdcD13e516KTtD0ihha7HvPheGnL8dIOOH7a4nakp8xE/QhwVtPnlcBTBa/pVPYX+RAZ+d+nZ/F9ZmZWojKDyn3AGkmrJfWTJdG3tFyzBbgsnwV2IbAvH9rqVHYLcHn+/nLgm03HN0gakLSaLPl/b1mNMzOzo5U2/BURE5KuBu4EasANEbFT0sb8/GbgDuASYBg4CFzRqWx+608Bt0j6IPAE8J68zE5Jt5Al8yeAq6o086uN5IfoZuF4acvx0g44ftridiRGXstgZmbzxSvqzcxs3jiomJnZvHFQSVC37W1SIOmnkn4s6QFJ2/Njs95CR9Jr8/sM51v2lLomUtINkp6W9FDTsXmrdz5R5Gv58XskrVrgtnxC0t/lv8sDki5JvS2SzpL0fyQ9ImmnpH+VH6/U79KhHZX7TY5JRPiV0ItsYsJu4BygH/gRsHax69Wmnj8FVrQc+3NgU/5+E/Dp/P3avB0DwOq8fbX83L3A68nWGX0buLjker8ZOB94qIx6Ax8CNufvNwBfW+C2fAL4SJtrk20LcAZwfv7+FODRvL6V+l06tKNyv8mxvNxTSc/k9jYRMQ40tqipgvVkW+eQ//2DpuM3R8RYRDxONtvvAmXrjE6NiB9G9v+SG5vKlCIi/gb4ZYn1br7X14G3ltX7mqEtM0m2LRGxNyL+Nn//G+ARst0wKvW7dGjHTJJsx7FyUEnPTFvXpCaA70raoWzrG2jZQgdo3kJnpu14RtocX2jzWe/JMhExAewDTi+t5u1drWzX7xuahowq0ZZ8OOc1wD1U+HdpaQdU+DeZLQeV9Mxli5rF8MaIOJ9sp+mrJL25w7XzuR3PQppLvRe7TdcBLwNeDewF/kt+PPm2SDoZuBX4k4jo9CjTpNvSph2V/U3mwkElPZXYbiYinsr/Pg3cRjZsN9stdEby963HF9p81nuyjKRe4DSKD1Eds4j4RUQciYg68AWy32VavXJJtUVSH9k/xF+JiG/khyv3u7RrR1V/k7lyUElPke1tFpWkkySd0ngP/D7wELPcQicf0viNpAvzceHLmsospPmsd/O9/gj43/m4+IJo/COcezfZ79KoV5Jtyb/3fwKPRMRnm05V6neZqR1V/E2OyWLPFPDr6BfZ1jWPks0G+fhi16dN/c4hm7XyI2Bno45kY7t3AY/lf1/YVObjeXt20TTDCxgi+z/ZbuBz5Ls8lFj3r5INQRwm+6++D85nvYETgL8iS7reC5yzwG35EvBj4EGyf4DOSL0twO+SDeE8CDyQvy6p2u/SoR2V+02O5eVtWszMbN54+MvMzOaNg4qZmc0bBxUzM5s3DipmZjZvHFTMzGzeOKiYmdm8cVAxOw5IOlvSd/Nt1x9Ockt0WxK8TsVskUiqRcSRebrX94FPRsTWfO+pekQcnI97m82GeypmJZF0e76L887GTs6SDkj6M0n3AK9X9rCzT0u6N3+9PL/uf0n6o6Z7HejwPWuB3ojYChARBxxQbLE4qJiV5wMR8VqyLTf+WNLpwElkD9VaFxE/yK/bHxEXkG3H8d/m8D3nAr+W9A1J90v6jKTafDTAbLYcVMzK88eSfgRsI9tZdg1whGwX22Zfbfr7+jl8Ty/wJuAjwOvI9mZ7/xzuY3bMHFTMSiDpLcA/Al4fEb8D3E+2GeChNnmUaPN+gvz/n/lOtf0dvm4EuD+yp4VOALeTPWbYbME5qJiV4zTgVxFxUNIrgAs7XPtPm/7+MH//U+C1+fv1QF+H8vcBL5A0mH/+h8DDc6m02bHqXewKmB2nvgNslPQg2bbm2zpcO5An7nuAS/NjXwC+Kelesm3fn5upcEQckfQR4K68V7MjL2+24Dyl2GwRSfopMBQRzyx2Xczmg4e/zMxs3rinYlYRkl5J9hTBZmMRsW4x6mPWjoOKmZnNGw9/mZnZvHFQMTOzeeOgYmZm88ZBxczM5s3/ByNFBNoTiMJvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the average revenue per user in June\n",
    "\n",
    "univariate(churn.arpu_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CKn712mFd81l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99999.0\n",
      "mean         0.0\n",
      "std          0.0\n",
      "min          0.0\n",
      "25%          0.0\n",
      "50%          0.0\n",
      "75%          0.0\n",
      "max          0.0\n",
      "Name: loc_og_t2o_mou, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minutes of usage of local (within same telecom circle) outgoing calls of Operator T to other operator fixed line\n",
    "\n",
    "univariate(churn.loc_og_t2o_mou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LRF-ci9Rd81l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99999.0\n",
      "mean         0.0\n",
      "std          0.0\n",
      "min          0.0\n",
      "25%          0.0\n",
      "50%          0.0\n",
      "75%          0.0\n",
      "max          0.0\n",
      "Name: std_og_t2o_mou, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minutes of usage of STD (outside the calling circle) outgoing calls of Operator T to other operator fixed line\n",
    "\n",
    "univariate(churn.std_og_t2o_mou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "_oYZd8eod81l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99999.000000\n",
      "mean       -46.527395\n",
      "std        810.863274\n",
      "min      -3660.874418\n",
      "25%          4.090000\n",
      "50%         28.040000\n",
      "75%        106.740000\n",
      "max      10752.560000\n",
      "Name: onnet_mou_8, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEHCAYAAABm9dtzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmd0lEQVR4nO3df7xcdX3n8dd7Zu5NSEIIPwIEQpq4BndD6w+MEdfq2lrLj7Wk1rqP+KMg2tJsYVvrY3eFutvWtuxq7aPdsiIptSi0UkQRjTY2olV0VX4ERSBA8CZBSImQBEzIr/tj5rN/nDO5k7lz505u5px75+T9fDzmkZnv+X7PfObm3vu53x/nexQRmJmZdUNpqgMwM7PicFIxM7OucVIxM7OucVIxM7OucVIxM7OuqUx1AFPplFNOicWLF091GGZmPeX+++/fGRHzWx07ppPK4sWL2bBhw1SHYWbWUyT9eLxjHv4yM7OucVIxM7OucVIxM7OucVIxM7OucVIxM7OucVIxM7OucVIxM7OucVIxM7OucVKxKbH7wDD/8dpvs/Hp3VMdipl1kZOKTYmHtu1m49N7eGz7C1Mdipl1kZOKTYmBZ5NkMlKrTXEkZtZNTio2JTbv2AfASM23szYrEicVmxKbd+wFYKTqpGJWJE4qNiUGnk2TinsqZoXipGK523NwmGdfGARgpOo5FbMicVKx3G1OeyngnopZ0TipWO7qk/TgORWzonFSsdxt3rGXvrIALyk2KxonFcvdk7v2c9aJs+gri2H3VMwKJdOkIukCSZskDUi6qsVxSbo2Pf6gpHMnaivpT9O6D0j6qqQzGo5dndbfJOn8LD+bTd7gSI2ZfWUqpRJV91TMCiWzpCKpDFwHXAgsA94uaVlTtQuBpenjcuD6Dtp+NCJeGhEvB74M/GHaZhmwCjgHuAD4eHoem2aqtRrlkqi4p2JWOFn2VFYAAxGxJSKGgFuBlU11VgI3R+JuYJ6kBe3aRsSehvazgWg4160RMRgRW4GB9Dw2zYzUIkkqJVH16i+zQskyqZwJPNXwelta1kmdtm0lXSPpKeCdpD2VDt8PSZdL2iBpw44dO47oA1l31CKolESlXPJEvVnBZJlU1KKs+c/S8eq0bRsRH4yIs4BPA1cewfsRETdExPKIWD5//vyWgVu2RqqjPRUPf5kVS5ZJZRtwVsPrhcDTHdbppC3ALcBbj+D9bBqo1oe/yh7+MiuaLJPKfcBSSUsk9ZNMoq9tqrMWuCRdBXYesDsitrdrK2lpQ/uLgccazrVK0gxJS0gm/+/N6sPZ5FUjSSp9pRLD3qbFrFAqWZ04IkYkXQmsB8rAjRGxUdLq9PgaYB1wEcmk+n7gsnZt01N/WNJLgBrwY6B+vo2SbgMeAUaAKyKimtXns8mr1pI5lbIn6s0KJ7OkAhAR60gSR2PZmobnAVzRadu0/K0tqtePXQNcM9l4LR+H5lTKJc+pmBWMr6i33FUblhR79ZdZsTipWO6qEVRKJU/UmxWQk4rlrloLSp6oNyskJxXL3Uit5ol6s4JyUrHc1Wp47y+zgnJSsdyN1GqUJfq8TYtZ4TipWO6qtaBcToa/fOdHs2JxUrHc1S9+7CvL96g3KxgnFcvdSC0oSZRLJU/UmxWMk4rl7lBPpSQvKTYrGCcVy119TqVS9pyKWdE4qVjuqrWgnA5/eU7FrFicVCx3I4dN1Hv4y6xInFQsV7W0Z1IulZIr6j38ZVYoTiqWq5FDSQX6yiWG3VMxKxQnFctVtaGnUvHFj2aF46RiuapGkkQqh+6nEkQ4sZgVhZOK5ao+h1JK7/wI+AJIswJxUrFc1Vd71be+T8qcVMyKwknFclUf/iqnS4rBScWsSJxULFejE/WiUkq+/Ua8VYtZYTipWK7qq73qN+kC91TMisRJxXJVO2z1V72n4qRiVhSZJhVJF0jaJGlA0lUtjkvStenxByWdO1FbSR+V9Fha/w5J89LyxZIOSHogfazJ8rPZ5IzUxvZUvFOxWXFkllQklYHrgAuBZcDbJS1rqnYhsDR9XA5c30HbO4GfjYiXAo8DVzecb3NEvDx9rM7mk9nROHxORYeVmVnvy7KnsgIYiIgtETEE3AqsbKqzErg5EncD8yQtaNc2Ir4aESNp+7uBhRl+BuuyegKpNFyn4k0lzYojy6RyJvBUw+ttaVkndTppC/Ae4CsNr5dI+oGkuyS9brKBW3aat2kBT9SbFUklw3OrRVnzb4/x6kzYVtIHgRHg02nRdmBRROyS9ErgC5LOiYg9Te0uJxlqY9GiRRN+COuuxg0lI9Kk4ol6s8LIsqeyDTir4fVC4OkO67RtK+lS4M3AOyPdOCoiBiNiV/r8fmAzcHZzUBFxQ0Qsj4jl8+fPn+RHs8lq7Kn0pcNfnqg3K44sk8p9wFJJSyT1A6uAtU111gKXpKvAzgN2R8T2dm0lXQB8ALg4IvbXTyRpfjrBj6QXkUz+b8nw89kkNM6plD1Rb1Y4mQ1/RcSIpCuB9UAZuDEiNkpanR5fA6wDLgIGgP3AZe3apqf+GDADuFMSwN3pSq/XA38iaQSoAqsj4rmsPp9NTn1SviSR/AkAwx7+MiuMLOdUiIh1JImjsWxNw/MArui0bVr+4nHq3w7cfjTxWvbqC70qZR2aU3FPxaw4Mk0qZs3qPZX60Bfguz+aFYiTiuXq0ES9REle/WVWNE4qlqvGbVrqSaXqnopZYTipWK5q9dVfZVFWfe8v91TMisK7FFuuRhqGv3w7YbPicVKxXLXaUNIXP5oVh5OK5Wr04seSb9JlVkBOKparelIplUaXFTupmBWHJ+otF7fc8yQA925NNjn40g+301dPKh7+MisM91QsV/XbCZcEJe/9ZVY4TiqWq9GkMnqdipcUmxWHk4rlqt4pKWl0l2IPf5kVh5OK5SpidKK+vv2XJ+rNisNJxXJVv6K+JKG0t+J71JsVh5OK5araMPwFyc26vKGkWXE4qViu6sNfaU5JkoqHv8wKw0nFclWLQDT0VMolT9SbFYiTiuWqFqMJBdxTMSsaJxXLVS2CUsN3XaXsORWzInFSsVzVatHUUyn5dsJmBeKkYrmqxegkPSQ9FW/TYlYcTiqWq1o091Q8/GVWJE4qlqtacOg2wgB95ZIvfjQrECcVy1Ut4rDhr7J7KmaFkmlSkXSBpE2SBiRd1eK4JF2bHn9Q0rkTtZX0UUmPpfXvkDSv4djVaf1Nks7P8rPZ5ETEoS3vIblOZdhzKmaFkVlSkVQGrgMuBJYBb5e0rKnahcDS9HE5cH0Hbe8EfjYiXgo8DlydtlkGrALOAS4APp6ex6aRVtepVD38ZVYYWfZUVgADEbElIoaAW4GVTXVWAjdH4m5gnqQF7dpGxFcjYiRtfzewsOFct0bEYERsBQbS89g0Uq0FpcbVXyX5fipmBZJlUjkTeKrh9ba0rJM6nbQFeA/wlSN4PyRdLmmDpA07duzo4GNYN0XT6q++cslLis0KJMukohZlzb89xqszYVtJHwRGgE8fwfsRETdExPKIWD5//vwWTSxLzcNfyUS9h7/MiqKS4bm3AWc1vF4IPN1hnf52bSVdCrwZeGPUt73t7P1siiXXqYy+7it7+MusSLLsqdwHLJW0RFI/yST62qY6a4FL0lVg5wG7I2J7u7aSLgA+AFwcEfubzrVK0gxJS0gm/+/N8PPZJNSaVn+VS76i3qxIOuqpSLoduBH4SkR0NFYRESOSrgTWA2XgxojYKGl1enwNsA64iGRSfT9wWbu26ak/BswA7lQyjHJ3RKxOz30b8AjJsNgVEVHtJFbLz5jVX2Xv/WVWJJ0Of11P8gv/WkmfBT4VEY9N1Cgi1pEkjsayNQ3PA7ii07Zp+YvbvN81wDUTxWVTp1Y7/OLHPvdUzAqlo+GviPhaRLwTOBd4gqSX8F1Jl0nqyzJAK5axE/UlX1FvViAdz6lIOhl4N/CbwA+AvyZJMndmEpkVUi3isL2/+iti2Ku/zAqj0zmVzwP/Fvh74FfSyXSAz0jakFVwVjzNe3/1lUtOKmYF0umcyifSOY5DJM1Ir15fnkFcVlDNW98nScXDX2ZF0enw15+1KPteNwOxY0MEhy0p7iuXGHJPxaww2vZUJJ1OstXJcZJewehV63OBWRnHZgXUvPdXfzmZU4kIpFabIphZL5lo+Ot8ksn5hcBfNpS/APxBRjFZgTWv/uorl4hIkk2l7KRi1uvaJpWIuAm4SdJbI+L2nGKyAovmbVoqyQjscDWo+EYFZj1vouGvd0XEPwCLJb2/+XhE/GWLZmbjajVRDzBUrXEczipmvW6i4a/Z6b9zsg7Ejg3Nw1/96ZCXlxWbFcNEw19/k/77oXzCsaJLNpQcfV3vqTipmBVDR0uKJf25pLmS+iR9XdJOSe/KOjgrnmTvr7HDX8MjvlbFrAg6vU7llyNiD8k9TLYBZwP/LbOorLDGrP6qjM6pmFnv6zSp1DeNvAj4x4h4LqN4rOCSvb9GX/eVPKdiViSdbtPyJUmPAQeA35E0HziYXVhWVOOt/nJSMSuGTre+vwp4DbA8IoaBfcDKLAOzYqoFh8+pVJxUzIrkSO5R/+9IrldpbHNzl+Oxgosxq7+SBDPkiXqzQuh06/u/B/4N8ABQv0Vv4KRiRyAiWlyn4p6KWZF02lNZDixLb/9rNin1uwaXmu6nAjDi+9SbFUKnq78eBk7PMhArvvrfJC23afHwl1khdNpTOQV4RNK9wGC9MCIuziQqK6TRnsrhtxMGD3+ZFUWnSeWPswzCjg21Qz2V0TIvKTYrlo6SSkTcJelngKUR8TVJs8BbytqROZRUSr5OxayoOt3767eAzwF/kxadCXyhg3YXSNokaUDSVS2OS9K16fEHJZ07UVtJb5O0UVJN0vKG8sWSDkh6IH2s6eSzWX5aDX+Nbn3vORWzIuh0+OsKYAVwD0BE/EjSqe0aSCoD1wFvItkv7D5JayPikYZqFwJL08ergeuBV0/Q9mHg1xhNcI02R8TLO/xMlrNamlV02O2E6xtKuqdiVgSdrv4ajIih+ov0AsiJ/rRcAQxExJa07a2MvQp/JXBzJO4G5kla0K5tRDwaEZs6jNumkfrwV/mwK+o9UW9WJJ0mlbsk/QFwnKQ3AZ8FvjRBmzOBpxpeb0vLOqnTSdtWlkj6gaS7JL2uVQVJl0vaIGnDjh07OjildUu74S8nFbNi6DSpXAXsAB4CfhtYB/yPCdqoRVlz72a8Op20bbYdWBQRrwDeD9wiae6Yk0TcEBHLI2L5/PnzJzildVO9p9I4/FVJJ+09p2JWDJ2u/qpJ+gLwhYjo9M/7bcBZDa8XAk93WKe/g7bNMQ6SXkMTEfdL2kxy35cNHcZrGWu1+ksS/eWSeypmBdG2p5KuzvpjSTuBx4BNknZI+sMOzn0fsFTSEkn9wCpgbVOdtcAl6fucB+yOiO0dtm2OdX46wY+kF5FM/m/pIE7LSavhL4BKWZ6oNyuIiYa/3ge8FnhVRJwcESeRrNJ6raTfb9cwIkaAK4H1wKPAbRGxUdJqSavTautIfvEPAH8L/E67tgCS3iJpG8lW/P8kaX16rtcDD0r6Icny59W+mdj0Ul/9VWoa3OxzT8WsMCYa/roEeFNE7KwXRMSW9P70XwX+ql3jiFhHkjgay9Y0PA+S5codtU3L7wDuaFF+O3B7u3hsasU4PZW+cslzKmYFMVFPpa8xodSl8yp9LeqbjataX1Lc1FXpL8s9FbOCmCipDE3ymNkY1drYXYohufujk4pZMUw0/PUySXtalAuYmUE8VmD1pNLcU+krlxjx8JdZIbRNKhHhTSOta9ollSH3VMwKodOLH82OWs1zKmaF56RiuRmpjd37C7yk2KxInFQsN4euU2n6rusrlxj27YTNCsFJxXJTn1OpNGWVvornVMyKwknFclNtcTth8JyKWZE4qVhu2q3+clIxKwYnFctN+6TiORWzInBSsdyMt6S4r1xiyLsUmxWCk4rlpjrOkuL+iudUzIrCScVyc2jvr6aeSqXkORWzonBSsdxUa0FJrbe+95yKWTE4qVhuqhFjEgpAX0W+TsWsIJxULDfVWoyZpAcO3aM+wr0Vs17npGK5GS+p9JVLRIzOuZhZ73JSsdy0SyqA51XMCsBJxXJTixiznBigr5yUDdc8r2LW65xULDfjzqlU0p6KL4A063lOKpabai3GXKMCHv4yKxInFctNNaDSNqm4p2LW6zJNKpIukLRJ0oCkq1ocl6Rr0+MPSjp3oraS3iZpo6SapOVN57s6rb9J0vlZfjY7ctVarfV1Kumciq9VMet9mSUVSWXgOuBCYBnwdknLmqpdCCxNH5cD13fQ9mHg14BvNb3fMmAVcA5wAfDx9Dw2TdRqYzeThOQ6FXBPxawIsuyprAAGImJLRAwBtwIrm+qsBG6OxN3APEkL2rWNiEcjYlOL91sJ3BoRgxGxFRhIz2PTxMhES4p9S2GznpdlUjkTeKrh9ba0rJM6nbSdzPsh6XJJGyRt2LFjxwSntG4ad0lxuvrLw19mvS/LpDL2twc0/yk6Xp1O2k7m/YiIGyJieUQsnz9//gSntG4a/+LH9DoVJxWznlfJ8NzbgLMaXi8Enu6wTn8HbSfzfjaFxltS7DkVs+LIsqdyH7BU0hJJ/SST6Gub6qwFLklXgZ0H7I6I7R22bbYWWCVphqQlJJP/93bzA9nRGa+nUnFSMSuMzHoqETEi6UpgPVAGboyIjZJWp8fXAOuAi0gm1fcDl7VrCyDpLcD/BeYD/yTpgYg4Pz33bcAjwAhwRURUs/p8duSqEZRbDFIeWlLsiXqznpfl8BcRsY4kcTSWrWl4HsAVnbZNy+8A7hinzTXANUcRsmWoVgvKpbGdYw9/mRWHr6i33CTDX2PLfUW9WXE4qVhuqjHO6q+Kk4pZUTipWG6qtfZb3w95l2KznuekYrkZb0nx7P5kam/fkNdVmPU6JxXLzXhLimf1lymXxN6DI1MQlZl1k5OK5aIWQUDL4S9JzJlR4YWDw/kHZmZd5aRiuajWkmtQWvVUgDSpuKdi1uucVCwXtQmSyvEzK7ww6KRi1uucVCwXE/VUjp/p4S+zInBSsVxUI0kqre78CHD8zD72uqdi1vOcVCwX9Z5Kq3vUg+dUzIrCScVyUU8qra5TgWT4y0uKzXqfk4rloj781WpJMcCcme6pmBWBk4rlYqKJ+rkz+xiq1hgc8VX1Zr0s063vzepq6bZezUnllnueBOCx7XsAuOm7P2bOjArvePWiXOMzs+5wT8VyUU2zyng9lZl9ZQAGh91TMetlTiqWi2p6U8fxlhTPqCRJ5eCwdyo262VOKpaLieZUZvYl34oHPadi1tOcVCwXEyWVGR7+MisEJxXLRS0m6Kmkd3/08JdZb3NSsVyM1Npfp1KfqPfwl1lvc1KxXNQOXVHf+viMPvdUzIrAScVyMbr3V+tvuUqpRKUkX/xo1uMyTSqSLpC0SdKApKtaHJeka9PjD0o6d6K2kk6SdKekH6X/npiWL5Z0QNID6WNNlp/NjszoLsXj15nRV3ZPxazHZZZUJJWB64ALgWXA2yUta6p2IbA0fVwOXN9B26uAr0fEUuDr6eu6zRHx8vSxOptPZpMx0eovSCbr3VMx621Z9lRWAAMRsSUihoBbgZVNdVYCN0fibmCepAUTtF0J3JQ+vwn41Qw/g3VJR0mlr8xBLyk262lZJpUzgacaXm9Lyzqp067taRGxHSD999SGeksk/UDSXZJed/Qfwbqlk6Qyo6/EoIe/zHpalhtKtvrtER3W6aRts+3AoojYJemVwBcknRMRew57Q+lykqE2Fi3ypoV5qU2w9T3AzEqZXYODeYVkZhnIsqeyDTir4fVC4OkO67Rr+0w6REb677MAETEYEbvS5/cDm4Gzm4OKiBsiYnlELJ8/f/4kP5odqZEJbtIFyVYt7qmY9bYsk8p9wFJJSyT1A6uAtU111gKXpKvAzgN2p0Na7dquBS5Nn18KfBFA0vx0gh9JLyKZ/N+S3cezI1GrBSWNv6EkJKu/9g9XiZioU2pm01Vmw18RMSLpSmA9UAZujIiNklanx9cA64CLgAFgP3BZu7bpqT8M3CbpvcCTwNvS8tcDfyJpBKgCqyPiuaw+nx2ZakTbhAJw4nF9DI3UODDkyXqzXpXpTboiYh1J4mgsW9PwPIArOm2blu8C3tii/Hbg9qMM2TJSrUXbSXqAk+fMAGDnvqE8QjKzDPiKestFZ0mlH4Bdez1Zb9arnFQsF50klZNm9SNgl3sqZj3LScVyUYtou5wYoFIuMW9Wn3sqZj3MScVyMdJBTwXg5Nkz3FMx62FOKpaLWi3aXqNSd/KcfnbtdVIx61VOKpaLai2odNRT6efAcJXn3Vsx60lOKpaLTq5TgdFlxU/s2pd1SGaWAScVy0Unq78g6amAk4pZr3JSsVwcGKpyXHof+nZOmt1PuSQ2/uueCeua2fTjpJKjiDhmtyDZOzjCnBkTb+BQKZdYcvJs7np8Rw5RmVm3Oank6LP3b+OVf3Ynm37ywlSHkquISJLKzM52BTr79OP50bN72fb8/owjM7Nuc1LJ0b1bn2P/UJXfu/UHx9Rtc3cfGKYWMLuDngrA2afNAeCbm9xbMes1Tio5enT7Hk49fgaP/eQF/vZbx86u/DvT6046Gf4CmD9nBmeddJyTilkPclLJyXC1xo+e2ctbXnEm55wxl3u2Hju78u9Mt13pNKlI4hdecirfGdjJ7gPDWYZmZl3mpJKTzTv2MlStseyMubzk9ON5/JljZ15l1xH2VABWvWoRB4arfOo7T2QUlZllwUklB7fc8yR/9+2tAGzZsY/9g1We2TPIT/cfG1eNH+qpdDhRD7DsjLm8adlp/N3/28ILB91bMesVTio5+cnug1RK4pQ5Mzht7kwAHn9m7xRHlY+dewcRMKt/4utUGv3uLy5lz8ER1ty1OZvAzKzrMr3zo43avvsgp82dSbkkTpubbEWy6ZkXWLHkpCmOLHs79w4xa0alo21a6m6550kAXnHWPD7+jc2MVIOfOXk273j1oqzCNLMucE8lBxHB9t0HOP2EpIdywnF9zKiUePwYuV5l595B5sw4sl5K3a+87Azmzerjtg1PeRjMrAc4qeTguX1D7BuqsvDE44BkddNpc2ey6RiZrN+1d/CIJukbzewrs+pVi9g7OMInv/PEMTMPZdarnFRysHVnsjniklNmHyo7be5MHn/mBSJiqsLKzc69Qx1f+NjKWSfN4jfOW8yOvYP82se/y4+OkWRs1oucVHLwxK59zO4vMz/d1h3gtLkz+On+YbY9f2AKI8vHrr2DHH8USQXgxafO4T2vXcKegyNc/LHvcN03Bo6pXQnMeoWTSg627tzH4lNmo4aJ6rNPOx6Af374J1MVVi4ODFXZN1Sd9PBXoyWnzOaffvfned3SU/jo+k38wke/yU3ffYK9gyNdiNTMusFJJWNP//QAz+8fPmzoC+CUOTP4uTNPYO0Pn56iyPJRv0blaIa/Gn390Wd5w0tO5bLXLqZSLvFHazdy7p/cyZW3fJ/P3PekN6E0m2KZLimWdAHw10AZ+EREfLjpuNLjFwH7gXdHxPfbtZV0EvAZYDHwBPCfIuL59NjVwHuBKvC7EbE+y8/Xibu37AJg8cmzxxy7+GVncM26R9m6c9+YpFMUz+w5CBzZhY+dWHrq8bx4/hyeem4/G378PPdsfY4vP7gdgDPnHcfLzjqBly6cx0tOP57T587k9LkzmTer77Deopl1X2ZJRVIZuA54E7ANuE/S2oh4pKHahcDS9PFq4Hrg1RO0vQr4ekR8WNJV6esPSFoGrALOAc4Avibp7IiYsoH3R57ew59++RFOnNV3aDlxoze/bAH/6yuP8rF/GeBDK8/pyhDRdLLn4DD/84sbmdVfZsEJx3X9/JJYdPJsFp08m4jg2RcGGXh2Lz9+bj/f27yLdQ8dPrTYVxYnzurnpNn9o//O7uOkWf2cOLufWf1lKqUSfZUS/WXRVy4devRXml6XS/Q1lPWXS1TKoiQhoJ67nMTsWJPlb7EVwEBEbAGQdCuwEmhMKiuBmyNZAnW3pHmSFpD0QsZruxJ4Q9r+JuCbwAfS8lsjYhDYKmkgjeF73f5gj27fw+p/uP/Q61YLuA4OV9m5d5DT5s7kHSsWtbzwb8EJx/GOFYv49D1Psu6h7cyZWaEsUS6JUolDbRrPH8TYshbvX19VFrSu1/I8Lc/Xrl6MKWusOzRSY7ha48Z3vyrzBQn1ZdqnzZ3Ja9OyfYMj7Nw7yJ6DI+w5MMzewRH2D42wb7DK07sPMPDsXvYNjXBgqErWa/Ak0mST/J/qUFlyQIfVU8v6NJTpsDaNiUyjCa3F+Wg85xTnu6l+f0i//lP5/lP49r/wklP544vP6fp5s0wqZwJPNbzeRtIbmajOmRO0PS0itgNExHZJpzac6+4W5zqMpMuBy9OXeyVt6vQDNTgF2NlJxa1NQTV65yTeuEMdx5eH//C/WxZPqxjH4Ri7wzF2R1dj/Bbwock3/5nxDmSZVFrl4OY/CMer00nbybwfEXEDcMME52r/RtKGiFh+NOfI0nSPDxxjtzjG7nCM3ZPl6q9twFkNrxcCzUudxqvTru0z6RAZ6b/PHsH7mZlZhrJMKvcBSyUtkdRPMom+tqnOWuASJc4DdqdDW+3argUuTZ9fCnyxoXyVpBmSlpBM/t+b1YczM7OxMhv+iogRSVcC60mWBd8YERslrU6PrwHWkSwnHiBZUnxZu7bpqT8M3CbpvcCTwNvSNhsl3UYymT8CXJHhyq+jGj7LwXSPDxxjtzjG7nCMXaJjYe8pMzPLh6+oNzOzrnFSMTOzrnFSGYek/yopJJ3SUHa1pAFJmySd31D+SkkPpceuTbefIV008Jm0/B5Ji7sU20clPSbpQUl3SJo33WKcIP4L0vgG0l0RciHpLEnfkPSopI2Sfi8tP0nSnZJ+lP57YkObI/p6djHWsqQfSPrydIwxvVD5c+n34aOSXjMNY/z99P/5YUn/KGnmVMco6UZJz0p6uKGsazFNxc/zGBHhR9ODZGnyeuDHwClp2TLgh8AMYAmwGSinx+4FXkNyrcxXgAvT8t8B1qTPVwGf6VJ8vwxU0ucfAT4y3WJsE3s5jetFQH8a77Kc/l8XAOemz48HHk+/Zn8OXJWWX3U0X88uxvp+4Bbgy+nraRUjyW4Wv5k+7wfmTacYSS583gocl76+DXj3VMcIvB44F3i4oaxrMZHzz3PLz5j3G/bCA/gc8DKSDSvrSeVq4OqGOuvT/9QFwGMN5W8H/qaxTvq8QnI1rLoc61uAT0/nGJvifQ2wvuH1YTHn/P/8RZL95TYBC9KyBcCmyX49uxTXQuDrwC8ymlSmTYzAXJJf2Goqn04x1nflOCn9vv4yyR9jUx4jyTZUjUmlazHl/fPc6uHhryaSLgb+NSJ+2HSo3ZYy21qUH9YmIkaA3cDJXQ75PSR/qUznGBuNF2Ou0mGBVwD30LT1D9C49c+Rfj274f8A/x2oNZRNpxhfBOwAPpkO0X1C0uzpFGNE/CvwFySXHWwnuQbuq9MpxgbdjCnvn+cxirUtbockfQ04vcWhDwJ/QPIXzZhmLcom2lJmMtvNTBhjRHwxrfNBkmtyPj0VMU5S3u83NgBpDnA78L6I2NNmiLyb2wh1GtubgWcj4n5Jb+ikyTixZPl1rpAM4fyXiLhH0l+TDNuMZyq+jieSbDK7BPgp8FlJ72rXZJxYpvL7tRd+nsc4JpNKRPxSq3JJP0fyTfjD9BfNQuD7klbQfkuZhS3KaWizTVIFOAF47mhibIj1UuDNwBsj7evmHeMkTel2OpL6SBLKpyPi82nxM5IWRLJBaSdb/7T7eh6t1wIXS7oImAnMlfQP0yzGbcC2iLgnff05kqQynWL8JWBrROwAkPR54N9PsxjruhlT3j/PY3j4q0FEPBQRp0bE4ohYTPIfdG5E/IRxtoFJu6svSDovXYFxCYdvHVPfUubXgX9pSACTpuQGZh8ALo6IxlsdTpsY2+hk+55MpJ/974BHI+IvGw4d0dY/E3w9j0pEXB0RC9Pvv1Uk/x/vmmYx/gR4StJL0qI3kuxkMW1iJBn2Ok/SrPTcbwQenWYx1nUzprx/nsfKcwKn1x40TNSnrz9IsgJjEw0rQIDlwMPpsY8xulPBTOCzJNvQ3Au8qEtxDZCMmz6QPtZMtxgniP8ikpVXm0mG8/L6//x5kqGABxu+dheRjDl/HfhR+u9Jk/16djneNzA6UT+tYgReDmxIv5ZfAE6chjF+CHgsPf/fk6yimtIYgX8kmeMZJvmj9b3djGkqfp6bH96mxczMusbDX2Zm1jVOKmZm1jVOKmZm1jVOKmZm1jVOKmZm1jVOKmZm1jVOKmZTQNJiSe+Y6jjqJPVJuindTv1RSVdPdUzWm5xUzKbGYmDaJBXgbcCMiPg54JXAb0/JvTis5zmpmI1D0vuV3ODpYUnvS3sXj0r6WyU3f/qqpOPSut+U9BFJ90p6XNLr0vKykpuq3afkpmq/nZ7+w8DrJD0g6ffHef93S/qCpC9J2irpyjSmH0i6W9JJab2Xp6/rN207sSGm5enzUyQ90ebjBjA73S/qOGAI2HP0X0U71jipmLUg6ZXAZcCrgfOA3yLZimQpcF1EnEOy++1bG5pVImIF8D7gj9Ky95Jsu/4q4FXAb6X7OF0FfDsiXh4Rf9UmlJ8l6dGsAK4B9kfEK4Dvkez5BHAz8IGIeCnwUMN7H4nPAftIthB5EviLiMh1I0IrhmNyl2KzDvw8cEdE7INDu9y+jmTn2wfSOveTDGPVfb5F+S8DL5X06+nrE0gS01CHcXwjIl4g2UBwN/CltPyh9LwnAPMi4q60/CaSvZ+O1AqgCpxBkjy/LelrEbFlEueyY5iTillr491kZbDheZVkqKj5WJXRny2R3Hdk/WEn7+xeKc3vV2t4XWPin98RRkcjZk5Q9x3AP0fEMPCspO+QbFropGJHxMNfZq19C/jVdOv02SS3bf72JM6zHvjP6X1ckHR2er4XgOOPNsiI2A08X5/DAX4DqPdaniCZdIdkG/R2ngR+UYnZJEN+jx1tfHbscU/FrIWI+L6kT5FsHw7wCeD5SZzqEyRDYd9P732xA/hVki3jRyT9EPjUBPMqE7kUWCNpFknP4rK0/C+A2yT9BvAvE5zjOuCTJNupC/hkRDx4FDHZMcpb35uZWdd4+MvMzLrGw19mU0zS+cBHmoq3RsRbevm97Njk4S8zM+saD3+ZmVnXOKmYmVnXOKmYmVnXOKmYmVnX/H9n4Gqi1aAQQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minutes of usage of all kind of calls within the same operator network for the month of August\n",
    "\n",
    "univariate(churn.onnet_mou_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "iuTcpbgwd81l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99999.000000\n",
      "mean       254.162786\n",
      "std        377.673187\n",
      "min          0.000000\n",
      "25%         31.230000\n",
      "50%        101.290000\n",
      "75%        289.895000\n",
      "max      10310.760000\n",
      "Name: offnet_mou_9, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEHCAYAAABm9dtzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAoW0lEQVR4nO3dfbRcdX3v8fdn5jzkiRAeAkIeTNCgDV4vxAjYqldrFUK9pEpdBvViaW2aCr1tXb011C6X2ksXrV29t1gksnq5FyoYqRSb2li01mprDRAkAgGi4TGBUPMgIeThnMzM9/6x9+RM5syZM+fM3uSc7M9rrVln9m//fnvvXwLnm9/jVkRgZmaWhdKxfgAzMzt+OKiYmVlmHFTMzCwzDipmZpYZBxUzM8tMz7F+gGPp1FNPjQULFhzrxzAzm1Tuv//+XRExu9W5QgeVBQsWsHHjxmP9GGZmk4qkp0c65+4vMzPLjIOKmZllxkHFzMwy46BiZmaZcVAxM7PMOKiYmVlmcg0qki6WtEXSVkmrW5yXpOvT8w9KWjKGsr8nKSSd2pB2TZp/i6SL8quZmZm1kltQkVQGbgCWAYuByyUtbsq2DFiUflYCN3ZSVtI84J3AMw1pi4EVwDnAxcDn0+uYmdnLJM+WyvnA1oh4IiIGgbXA8qY8y4FbI7EBmCXpjA7K/i/g94FoutbaiBiIiCeBrel1Xha33fM0H7tj08t1OzOzCSnPoDIH2NZwvD1N6yTPiGUlXQo8GxE/HMf9kLRS0kZJG3fu3Nl5bUZx35N7+Lcf78rsemZmk1GeQUUt0ppfMzlSnpbpkqYBnwA+Oc77ERE3RcTSiFg6e3bLrWvGZbBa43C1ltn1zMwmozz3/toOzGs4ngs812GevhHSXwUsBH4oqZ7+A0nnd3i/3AxWahyu+tXMZlZsebZU7gMWSVooqY9kEH1dU551wBXpLLALgb0RsWOkshHxUEScFhELImIBSSBZEhHPp9daIalf0kKSwf97c6zfUQYqNQbdUjGzgsutpRIRFUlXA3cDZeDmiNgsaVV6fg2wHriEZFD9AHBlu7Kj3G+zpDuAR4AKcFVEVPOp3XBJS6VGRJC2oszMCifXre8jYj1J4GhMW9PwPYCrOi3bIs+CpuNrgWvH+bhdGazWiIBKLegtO6iYWTF5RX1GBitJ15cH682syBxUMnIkqFQ8WG9mxeWgkpH6IL0H682syBxUMuLuLzMzB5XMOKiYmTmoZMZBxczMQSUzA2kwGag4qJhZcTmoZCAiGloqnv1lZsXloJKBxkDi7i8zKzIHlQw0TiM+7O4vMyswB5UMDDYEEq9TMbMic1DJQGNQ8ZiKmRWZg0oGjg4qbqmYWXE5qGRgsDq0w76DipkVmYNKBhrXpnidipkVmYNKBtz9ZWaWcFDJQGPrxFOKzazIcg0qki6WtEXSVkmrW5yXpOvT8w9KWjJaWUl/lObdJOkbks5M0xdIOpimb5K0pvl+efHsLzOzRG5BRVIZuAFYBiwGLpe0uCnbMmBR+lkJ3NhB2c9GxOsj4lzga8AnG673eEScm35W5VOz4bxOxcwskWdL5Xxga0Q8ERGDwFpgeVOe5cCtkdgAzJJ0RruyEfFiQ/npwDFvGhy1ot5BxcwKLM+gMgfY1nC8PU3rJE/bspKulbQN+CBHt1QWSnpA0nckvaXVQ0laKWmjpI07d+4ca51a8kC9mVkiz6CiFmnNrYqR8rQtGxGfiIh5wG3A1WnyDmB+RJwHfAy4XdLMYReJuCkilkbE0tmzZ3dQjdF5TMXMLJFnUNkOzGs4ngs812GeTsoC3A5cBhARAxGxO/1+P/A4cHYXz9+xgYbWyaBnf5lZgeUZVO4DFklaKKkPWAGsa8qzDrginQV2IbA3Ina0KytpUUP5S4HH0vTZ6QA/ks4iGfx/Ir/qDakHkt6yPFBvZoXWk9eFI6Ii6WrgbqAM3BwRmyWtSs+vAdYDlwBbgQPAle3Kppe+TtJrgBrwNFCf5fVW4DOSKkAVWBURe/KqX6N6UJnR3+N1KmZWaLkFFYCIWE8SOBrT1jR8D+CqTsum6ZeNkP9O4M5unne86kFlen+PB+rNrNC8oj4Dg9Uq5ZKY0lv2QL2ZFZqDSgYGKzX6yiV6yyWPqZhZoTmoZGCwUqOvp0RfWe7+MrNCc1DJwGA1CSq95ZKnFJtZoTmoZGCgofvLLRUzK7JcZ38VxWClxkClyq6XBjh4uMrt9zxz5NwHLph/DJ/MzOzl5ZZKBgYrNXpKJXpKolrz7C8zKy4HlQwMVmuUS6LsoGJmBeegkoGkpeKgYmbmoJKBwUqNclmUSyUHFTMrNAeVDAxW3VIxMwMHlUzUB+rLJVFxUDGzAnNQycBgJRmo9+wvMys6B5UMDHig3swMcFDJxGC1Rk85DSoRJDv6m5kVj4NKBpLur2TxI0DVQcXMCspBJQON61QAd4GZWWHlGlQkXSxpi6Stkla3OC9J16fnH5S0ZLSykv4ozbtJ0jckndlw7po0/xZJF+VZt0aNU4rBQcXMiiu3oCKpDNwALAMWA5dLWtyUbRmwKP2sBG7soOxnI+L1EXEu8DXgk2mZxcAK4BzgYuDz6XVyVa0F1Vqkix8dVMys2PJsqZwPbI2IJyJiEFgLLG/Ksxy4NRIbgFmSzmhXNiJebCg/HYiGa62NiIGIeBLYml4nV/X3p/SUSpSVBBWvVTGzosozqMwBtjUcb0/TOsnTtqykayVtAz5I2lLp8H5IWilpo6SNO3fuHFOFWhkKKm6pmJnlGVTUIq35t+1IedqWjYhPRMQ84Dbg6jHcj4i4KSKWRsTS2bNnt3zwsRioVgGSxY/l5I/TQcXMiirPoLIdmNdwPBd4rsM8nZQFuB24bAz3y9xRLRW5pWJmxZZnULkPWCRpoaQ+kkH0dU151gFXpLPALgT2RsSOdmUlLWoofynwWMO1Vkjql7SQZPD/3rwqV3ckqHig3swsv9cJR0RF0tXA3UAZuDkiNktalZ5fA6wHLiEZVD8AXNmubHrp6yS9BqgBTwP1622WdAfwCFABroqIal71q6sHkJKGgooH6s2sqHJ9R31ErCcJHI1paxq+B3BVp2XT9MtaZK+fuxa4drzPOx711fOSWypmZl5R36WhlgpD27TUasfykczMjhkHlS7V40fJLRUzMweVbg11f+ExFTMrPAeVLtW7ukpSQ/eXg4qZFZODSpeq7v4yMzvCQaVL9QDi7i8zMweVrtViaJ1KT8nbtJhZsTmodKlxSrFbKmZWdA4qXWpc/NhTToNK1etUzKyYHFS6VGtoqZQkSnJLxcyKy0GlS5WGvb8Aesolt1TMrLAcVLpUa5j9BclWLW6pmFlROah0qRpNLRUHFTMrMAeVLlWbWyrlkqcUm1lhOah0qdaipXLYYypmVlAOKl1q3KYFkqDiloqZFVWuQUXSxZK2SNoqaXWL85J0fXr+QUlLRisr6bOSHkvz3yVpVpq+QNJBSZvSz5rm++VhaEPJ5DiZ/eWgYmbFlFtQkVQGbgCWAYuByyUtbsq2jORd8ouAlcCNHZT9JvC6iHg98CPgmobrPR4R56afVfnU7Gj1lorSlkq5JCp+SZeZFVSeLZXzga0R8UREDAJrgeVNeZYDt0ZiAzBL0hntykbENyKikpbfAMzNsQ6jGpr9lRz3lj37y8yKK8+gMgfY1nC8PU3rJE8nZQF+Ffh6w/FCSQ9I+o6kt4z3wcdiaJ1KvaXi7i8zK66eHK+tFmnNv21HyjNqWUmfACrAbWnSDmB+ROyW9Abgq5LOiYgXm8qtJOlqY/78+aNWYjSNG0pCfZ2Ku7/MrJjybKlsB+Y1HM8FnuswT9uykj4MvBv4YETS/xQRAxGxO/1+P/A4cHbzQ0XETRGxNCKWzp49e5xVG9JqSrG7v8ysqPIMKvcBiyQtlNQHrADWNeVZB1yRzgK7ENgbETvalZV0MfBx4NKIOFC/kKTZ6QA/ks4iGfx/Isf6Aa0XP7r7y8yKKrfur4ioSLoauBsoAzdHxGZJq9Lza4D1wCXAVuAAcGW7suml/xLoB76ZjmNsSGd6vRX4jKQKUAVWRcSevOpXN2xDSXd/mVmBdRRUJN0J3Ax8PSI6/o0ZEetJAkdj2pqG7wFc1WnZNP3VI+S/E7iz02fLSm3YLsVyS8XMCqvT7q8bgQ8AP5Z0naTX5vhMk8rQS7qS4/qK+nSox8ysUDoKKhHxTxHxQWAJ8BRJ19O/S7pSUm+eDzjRDW+plAjAY/VmVkQdD9RLOgX4FeAjwAPAX5AEmW/m8mSTRDXiyLvpIWmpgF8pbGbF1OmYyt8CrwX+Gviv6QwtgC9L2pjXw00G1RqU1SKo1IL+Y/VQZmbHSKezv/4qHTg/QlJ/ujZkaQ7PNWnUhrVUksaf16qYWRF12v31P1ukfT/LB5msKtWmoFJ295eZFVfbloqkV5DsuTVV0nkMbZ8yE5iW87NNCrUIGmLKkQDjloqZFdFo3V8XkQzOzwX+vCF9H/AHOT3TpFKtufvLzKyubVCJiFuAWyRdli4utCbDZn+5+8vMCmy07q8PRcQXgQWSPtZ8PiL+vEWxQqnV4sgaFTh69peZWdGM1v01Pf05I+8HmayGdX+V0+4vb9ViZgU0WvfXF9Kfn355HmfyGT6mojTd3V9mVjwdTSmW9KeSZkrqlfQtSbskfSjvh5sMRlpRf9jdX2ZWQJ2uU3lX+gbFd5O8QOts4H/k9lSTSLUWR6+oT7u/qu7+MrMC6jSo1DeNvAT40svxnpLJohZBqWVLxd1fZlY8nW7T8veSHgMOAh+VNBs4lN9jTR7DWipHxlTcUjGz4ul06/vVwJuApRFxGNgPLM/zwSaLao2jWyqe/WVmBTaWd9T/DPB+SVcAvwy8a7QCki6WtEXSVkmrW5yXpOvT8w9KWjJaWUmflfRYmv8uSbMazl2T5t8i6aIx1G3ckg0lh46Htmlx95eZFU+ns7/+Gvgz4M3AG9NP292JJZWBG4BlwGLgckmLm7ItAxaln5Ukb5gcrew3gddFxOuBHwHXpGUWAyuAc4CLgc+n18lVpRaUS0N/jOWSKMktFTMrpk7HVJYCi2Ns78g9H9gaEU8ASFpL0mX2SEOe5cCt6XU3SJol6QxgwUhlI+IbDeU3kLSa6tdaGxEDwJOStqbPkOtuyrVaUNbRaeWSvKLezAqp0+6vh4FXjPHac4BtDcfb07RO8nRSFuBXga+P4X6Za178CMmmku7+MrMi6rSlcirwiKR7gYF6YkRc2qaMWqQ1//N9pDyjlpX0CaAC3DaG+yFpJUlXG/Pnz29RZGyqcfTeX5BsKunuLzMrok6DyqfGce3twLyG47nAcx3m6WtXVtKHSRZivqOhS66T+xERNwE3ASxdurTr3/y1WtDXc3SDr8fdX2ZWUJ1OKf4O8BTQm36/D/jBKMXuAxZJWiipj2QQfV1TnnXAFekssAuBvRGxo11ZSRcDHwcujYgDTddaIalf0kKSwf97O6lfN5q3aYF695eDipkVT0ctFUm/TtJldDLwKpKxijXAO0YqExEVSVcDdwNl4OaI2CxpVXp+DbCeZJX+VuAAcGW7suml/xLoB76ppNtpQ0SsSq99B8lEgApwVURUO/6TGKeWYypl+X0qZlZInXZ/XUUyk+oegIj4saTTRisUEetJAkdj2pqG75Feu6Oyafqr29zvWuDa0Z4rS80r6iHp/vKKejMrok5nfw1ExGD9QFIPLQbBi6haO3rvL0hW1R/2QL2ZFVCnQeU7kv4AmCrpncDfAH+f32NNHrUYqaXi7i8zK55Og8pqYCfwEPAbJN1Sf5jXQ00mrdepePaXmRVTR2MqEVGT9FXgqxGxM99HmlxqwbDur3K55HUqZlZIbVsq6VTfT0naBTwGbJG0U9InX57Hm/iqLbZp6S3JK+rNrJBG6/76HeDngDdGxCkRcTJwAfBzkn4374ebDKpNG0qC9/4ys+IaLahcAVweEU/WE9JNHj+Uniu8JKgcndbj7i8zK6jRgkpvROxqTkzHVXpb5C+cVivq+8risBc/mlkBjRZUBsd5rjBqteEbSvb1lKnUwuMqZlY4o83++s+SXmyRLmBKDs8z6bRqqfSnG0wOHnZQMbNiaRtUIiL3NydOdtUWLZV6UBmoOKiYWbGM5R311kKtFvQ0t1R6k1jsoGJmReOg0qVKixX1Qy2V3DdJNjObUBxUulSL4RtKuvvLzIrKQaVLrba+7+9x95eZFZODShciouXeX0daKofd/WVmxeKg0oX6TizDWiq97v4ys2LKNahIuljSFklbJa1ucV6Srk/PPyhpyWhlJb1P0mZJNUlLG9IXSDooaVP6WdN8v6zV3+7YvE2Lu7/MrKg6fZ3wmEkqAzcA7wS2A/dJWhcRjzRkWwYsSj8XADcCF4xS9mHgvcAXWtz28Yg4N6cqDTMUVIZvKNlTkmd/mVnh5NlSOR/YGhFPpK8iXgssb8qzHLg1EhuAWZLOaFc2Ih6NiC05PnfHqtG6pQLQ11NyS8XMCifPoDIH2NZwvD1N6yRPJ2VbWSjpAUnfkfSWVhkkrZS0UdLGnTu7e99YvaXSvKIeYEpvmUEHFTMrmDyDyvDftNC8H/xIeTop22wHMD8izgM+Btwuaeawi0TcFBFLI2Lp7NmzR7lke7Uj3V/DH7e/p+TZX2ZWOHkGle3AvIbjucBzHebppOxRImIgInan3+8HHgfOHteTd2io+2t4UHH3l5kVUZ5B5T5gkaSFkvqAFcC6pjzrgCvSWWAXAnsjYkeHZY8iaXY6wI+ks0gG/5/ItkpHq7Xp/up3UDGzAspt9ldEVCRdDdwNlIGbI2KzpFXp+TXAeuASYCtwALiyXVkASe8BPgfMBv5B0qaIuAh4K/AZSRWgCqyKiD151Q+ObqlEU+dcf0+ZPfv9yhkzK5bcggpARKwnCRyNaWsavgdwVadl0/S7gLtapN8J3NnlI49J/ZXB5ZKGvT7YLRUzKyKvqO9Crd5ScfeXmRngoNKVarvZX+mU4vq4i5lZETiodKHeUmneUBKGNpXcP1h5WZ/JzOxYclDpQjXt3Wrd/ZXs/7V/wGtVzKw4HFS6MNKGkjDUUnlpwC0VMysOB5UujLShJDR0fzmomFmBOKh0oe2Gkr0OKmZWPA4qXWi3oWR9TGWfg4qZFYiDShdqbfb+muLuLzMrIAeVLhwZU2nRUulzUDGzAnJQ6cKRDSVbtVR63f1lZsXjoNKFdlvf95ZL9JbFT72ppJkViINKFypttmkBmNHfw+6XHFTMrDgcVLpQazOmAjC9v4ddbqmYWYE4qHSh3YaSUG+pDLycj2Rmdkw5qHThyIaSbVoq7v4ysyJxUOnCkQ0l27VU9g8Qza+FNDM7TuUaVCRdLGmLpK2SVrc4L0nXp+cflLRktLKS3idps6SapKVN17smzb9F0kV51g3ab9MCSUvlcDV48ZCnFZtZMeQWVCSVgRuAZcBi4HJJi5uyLQMWpZ+VwI0dlH0YeC/w3ab7LQZWAOcAFwOfT6+Tm1qbbVoAZvQnt/e4ipkVRZ4tlfOBrRHxREQMAmuB5U15lgO3RmIDMEvSGe3KRsSjEbGlxf2WA2sjYiAingS2ptfJTX1KcU+LXYohaakA7PYMMDMriDyDyhxgW8Px9jStkzydlB3P/ZC0UtJGSRt37tw5yiXbG1pR3/r8jHpQcUvFzAoiz6DSqk+oecR6pDydlB3P/YiImyJiaUQsnT179iiXbK/dinoYaqns9AwwMyuInhyvvR2Y13A8F3iuwzx9HZQdz/0y1W5DSYDpfW6pmFmx5NlSuQ9YJGmhpD6SQfR1TXnWAVeks8AuBPZGxI4OyzZbB6yQ1C9pIcng/71ZVqjZkXUqI7RUyiUxa1qv16qYWWHk1lKJiIqkq4G7gTJwc0RslrQqPb8GWA9cQjKofgC4sl1ZAEnvAT4HzAb+QdKmiLgovfYdwCNABbgqIqp51Q9Gb6kAnDK9j9373VIxs2LIs/uLiFhPEjga09Y0fA/gqk7Lpul3AXeNUOZa4NouHnlMjgSVcpugMqOfXW6pmFlBeEV9FzppqZw6o89jKmZWGA4qXRht9hfAKdP7vU7FzArDQaULo62oBzh1Rj8vHDjMocO5Du+YmU0IDipdGG1DSYB5J08F4NkXDmZ6741P7XGgMrMJx0GlC9UjW9+PnGf+ydMA2LbnQGb33bN/kPd94ft85f7tmV3TzCwLDipdqNWCkkBtur/m5RBUXjgwSAQ8vXt/Ztc0M8uCg0oXKrUYcTPJutkz+unvKfFMhkFl/0DS7fXcC4cyu6aZWRYcVLpQixhxM8m6UknMPWkq2/ZkN6by0kDyfpbn9mY7TmNm1i0HlS5Ua9F2jUrd/JOnZdxSSYNKxoP/ZmbdclDpQrUWI+771Wj+ydPYtudAZq8V3j+YBJWf7BtgsFLL5JpmZllwUOlCLaLtdOK6eSdPY99Ahb0HD2dy33r3VwT8x4seVzGzicNBpQuddn/VZ4Bl1QVW7/6C7Ne/mJl1I9cNJY931droLZXb73mGHemA+pfu3cbDz74IwAcumD/u+740MLTo0eMqZjaRuKXShU6CCsDJ0/oA2JPRxpL7Byr0lZO/uh173f1lZhOHWypdqEa03ferrr+3zMwpPTyf0fjH/oEKJ07rpVYLd3+Z2YTioNKFWoctFYA5J03LLAC8NFBhRn8PM/p73P1lZhOKu7+6UI32m0k2mnvSVHa9NMjBwe43gdw/UGF6f5kzZ01xUDGzCSXXoCLpYklbJG2VtLrFeUm6Pj3/oKQlo5WVdLKkb0r6cfrzpDR9gaSDkjalnzXN98tafe+vTsydld1uxfsHq0zv6+GME6d6TMXMJpTcgoqkMnADsAxYDFwuaXFTtmXAovSzErixg7KrgW9FxCLgW+lx3eMRcW76WZVPzYZUarVR9/6qm3NSGlR+2v204v1p99esab3sO1Q58gZKM7NjLc+WyvnA1oh4IiIGgbXA8qY8y4FbI7EBmCXpjFHKLgduSb/fAvxSjnVoa/9AlWn95Y7yTuvr4eTpfWzPoqUyUGF6fw8zp/QCsO9QNosqzcy6lWdQmQNsazjenqZ1kqdd2dMjYgdA+vO0hnwLJT0g6TuS3tLqoSStlLRR0sadO3eOtU5H2TdQ4YT0F3sn5p40le0/7T6ovDRQTYLK1OTeLx6sjFLCzOzlkWdQaTXa0NxPM1KeTso22wHMj4jzgI8Bt0uaOewiETdFxNKIWDp79uxRLtnevkOHOWFK5xPo5p00jb0HD/PCge7eWZ90f5U5sR5U3FIxswkiz6CyHZjXcDwXeK7DPO3K/kfaRUb68ycAETEQEbvT7/cDjwNnZ1KTEew7VOGE/s6DytmnnwDAY8/vG/c9q7Xg4OG0pZIGtKz2FDMz61ae61TuAxZJWgg8C6wAPtCUZx1wtaS1wAXA3ojYIWlnm7LrgA8D16U//w5A0mxgT0RUJZ1FMvj/RI71G3NL5dQZfZwyvY8t4wgqt9/zDMCR99L/6Pl91NINil90UDGzCSK3oBIRFUlXA3cDZeDmiNgsaVV6fg2wHrgE2AocAK5sVza99HXAHZJ+DXgGeF+a/lbgM5IqQBVYFRF78qrf4WqNQ4drYxpTkcRrX3EC9zy5hwODFab1jf2PfyDd6r6/p8yU3qSh6ZaKmU0Uua6oj4j1JIGjMW1Nw/cAruq0bJq+G3hHi/Q7gTu7fOSO7TuUDI6PpaUC8JpXzOR7j+/me1t3887Fp4/5vgNpS6Wvt8TU3mTmmcdUzGyi8Ir6capP4x1LSwVgwanT6O8pcffm58d136GWSom+nhLlkjz7y8wmDAeVcRpvS6WnVOJ1Z57I1x/awYHBsQeDxu4vScyc0uPuLzObMBxUxmm8QQVgyStPYv9gdVytlcFK2v3Vk/zVnTi1191fZjZhOKiM05Hur/6xdX8BvPKUacw7eSpfuX/7mMs2dn8BzJza65aKmU0YDirj1E1LpSRx2ZK5/Pvju3lq1/4xlR0WVKb0ekqxmU0YDirjNDRQP74JdB+4YD595RKf++etYyrXOKYC9e4vD9Sb2cTgoDJOQy2VsXd/AZx2whQ+dOEr+eqmZ8fUWhmoVBHQW052spk51QP1ZjZxOKiM076BypFpveNx+z3PcNoJ/ZQEv3nbD7htw9NHVs23M1Cp0ddTQulrjN39ZWYTiYPKOCVbtIyvlVJ3wpRefuFnTufRHS/yvcd3d1Tm0GCVKb1D2+3PnNrLQKV2ZPsWM7Njye+oH6d9hypHNnTsxptffSpP7z7APz68g3npi7za2bH3EKfP7D9yPLNhp+LGYGNmdiy4pTJO+w5VmJFBUFE6E2zWtD6+dO8z7HppYMS8g5UaP9l3iDmzhoLPiX6niplNIA4q4zTWHYrbmdpX5gPnz+fAYJWP3LJxxPetPL/3ILWAObOmHUnz9vdmNpE4qIxT8i6V7sZUGp05ayor3jiPR3a8yC+v+T6P7nhxWJ76q4jnNnSTzfSLusxsAnFQGad9hyqZtVTqFp95IrdceT579g/y7s/9G3+8/lH2Dwx1az3704OcMGXoNcLQ2P3loGJmx54H6scpi9lfrTy5az8f/S+v4u5Hnuem7z7Bl+/bxs+/9jTOmz+L7S8cPGo8BZIpxeCgYmYTg1sq41CtBfsHq5m3VOqm9ffwnvPm8htvPYsZ/T3c9cCz/PH6R9m1b4A5TTPEZk3rZUpviXuezO19ZGZmHXNQGYeXutj3ayxeecp0Pvq2V/ErP7uA1515ImfOmsriM2Yelae3XOIjbz6Lrz24g03bXsj1eczMRpNrUJF0saQtkrZKWt3ivCRdn55/UNKS0cpKOlnSNyX9OP15UsO5a9L8WyRdlFe99g10t+/XWEji7NNP4L1L5nLV21/NGScOX8uy6m2v4tQZ/ay+80G+uOFpvr3lJ2za9gJP797P3oOHSV6waWaWv9x+K0oqAzcA7wS2A/dJWhcRjzRkWwYsSj8XADcCF4xSdjXwrYi4Lg02q4GPS1oMrADOAc4E/knS2RGR+VLzbvf9ylJ9a5d3LT6dr256lj/86sPD8vSUxKxpvcya1sdJDT9PmtbHlN5y+ikxpbdMb7lET0mUSqKvLPp7y0xNP1PSn309JUoClOy4LNKfSoJguoMMJFmSn2m+5DvUj+p5j/xEDd9blGu8uJlNOHn+U/t8YGtEPAEgaS2wHGgMKsuBW9N31W+QNEvSGcCCNmWXA29Ly98C/Avw8TR9bUQMAE9K2po+w/ezrpgEr597Iqed0D965pfJ6+acyDlnzmTvwcO8ePAwBwar6afC/obvu14a5Jk9BzgwWOXgYJVKbfK2YpLgVP/eImgdFZzq3zViuXqReuBqlU8NmdV0/cZA2ViOhnKN+ezl5X+PHO3trzmNT116TubXzTOozAG2NRxvJ2mNjJZnzihlT4+IHQARsUPSaQ3X2tDiWkeRtBJYmR6+JGlLpxVq9ve/ddThqcCu8V5rEilCPYtQR3A9jzdjqud3gU+P/16vHOlEnkGl1b8Lmv9ZPFKeTsqO535ExE3ATaNca8wkbYyIpVlfd6IpQj2LUEdwPY83E6WeeQ7UbwfmNRzPBZ7rME+7sv+RdpGR/vzJGO5nZmY5yjOo3AcskrRQUh/JIPq6pjzrgCvSWWAXAnvTrq12ZdcBH06/fxj4u4b0FZL6JS0kGfy/N6/KmZnZcLl1f0VERdLVwN1AGbg5IjZLWpWeXwOsBy4BtgIHgCvblU0vfR1wh6RfA54B3peW2SzpDpLB/ApwVR4zv9rIvEttgipCPYtQR3A9jzcTop7yGgYzM8uKV9SbmVlmHFTMzCwzDipdGm0rmolO0jxJ35b0qKTNkn47TR/zdjiS3iDpofTc9Zpgy98llSU9IOlr6fHxWMdZkr4i6bH07/RNx2k9fzf97/VhSV+SNOV4qKekmyX9RNLDDWmZ1SudyPTlNP0eSQsyr0RE+DPOD8kkgseBs4A+4IfA4mP9XGOswxnAkvT7CcCPgMXAnwKr0/TVwJ+k3xen9ewHFqb1L6fn7gXeRLJm6OvAsmNdv6a6fgy4Hfhaenw81vEW4CPp9z5g1vFWT5JFzU8CU9PjO4BfOR7qCbwVWAI83JCWWb2AjwJr0u8rgC9nXodj/R/IZP6kf2l3NxxfA1xzrJ+ryzr9Hcmea1uAM9K0M4AtrepIMkPvTWmexxrSLwe+cKzr0/A8c4FvAT/PUFA53uo4M/1lq6b0462e9R03TiaZwfo14F3HSz1JtqlqDCqZ1aueJ/3eQ7ICX1k+v7u/ujPSNjOTUtoUPg+4h6btcIDG7XBG2lpne4v0ieJ/A78P1BrSjrc6ngXsBP5v2s33V5Kmc5zVMyKeBf6MZEnBDpL1bd/gOKtngyzrdaRMRFSAvcApWT6sg0p3xrOdzIQkaQZwJ/A7EfFiu6wt0sa7tc7LQtK7gZ9ExP2dFmmRNqHrmOoh6Tq5MSLOA/aTdJeMZFLWMx1TWE7S5XMmMF3Sh9oVaZE24evZgfHUK/c6O6h057jYGkZSL0lAuS0i/jZNHut2ONvT783pE8HPAZdKegpYC/y8pC9yfNURkufbHhH3pMdfIQkyx1s9fwF4MiJ2RsRh4G+Bn+X4q2ddlvU6UkZSD3AikOlrYx1UutPJVjQTWjor5P8Aj0bEnzecGtN2OGmzfJ+kC9NrXtFQ5piKiGsiYm5ELCD5O/rniPgQx1EdASLieWCbpNekSe8g2WHiuKonSbfXhZKmpc/3DuBRjr961mVZr8Zr/TLJ/wvZts6O9aDUZP+QbDPzI5KZF5841s8zjud/M0nz90FgU/q5hKSf9VvAj9OfJzeU+URa3y00zJYBlgIPp+f+kowHADOq79sYGqg/7uoInAtsTP8+vwqcdJzW89PAY+kz/jXJDKhJX0/gSyTjRIdJWhW/lmW9gCnA35BsjXUvcFbWdfA2LWZmlhl3f5mZWWYcVMzMLDMOKmZmlhkHFTMzy4yDipmZZcZBxczMMuOgYtYhSf893U7+tnTB2T9J2iTp/eO41rmSLsnjOcdD0kmS7pL0oKR7Jb3uWD+TTU65vaPe7Dj0UZIFZk9KuhDojYhzx3mtc0kWqK3P6Nm69QfApoh4j6TXAjeQrFQ3GxO3VMxakPSx9AVQD0v6HUlrSHYBXifp48AXgXPTlsqrJD0l6dOSfpC+HOm16XWmpy9eui/dOXh5uqXPZ4D3t2vpSPqUpFskfSO9/nsl/Wl6/X9M92xD0jvSaz+U3qs/TX9K0qnp96WS/qVNlReTrNYmIh4DFkg6PYs/SysWBxWzJpLeAFwJXABcCPw68AWSTfneHhF/AnwE+NeIODciHk+L7oqIJcCNwO+laZ8g2V/pjcDbgc8CvcAnSV6QdG5EfLnN47wK+EWSXXm/CHw7Iv4TcBD4RUlTgP8HvD9N7wF+cxzV/iHw3rT+5wOv5OhNCc064qBiNtybgbsiYn9EvESyC+5bOihX3+H5fpIXLUHy8qjVkjYB/0Ky99L8MTzL1yPZifchkjeN/mOa/lB6j9eQ7Nj7ozT9FpK3B47VdcBJ6XP+FvAAUBnHdazgPKZiNtx431M+kP6sMvT/loDLImLLUTeQLhjLNSOiJulwDG3WV0vv0e5ZKwz9w3FKu5tE8g6dK9NnE8kbJJ/s8BnNjnBLxWy47wK/lG6tPh14D/Cv47zW3cBvpb+okXRemr4POKHrJ0126l0g6dXp8X8DvpN+fwp4Q/r9snYXkTQrHeuBpGvvu9H+ZW1mLTmomDWJiB+QjFPcS/Jq5b+KiAfGebk/IhlDeVDSw+kxwLeBxeOdktzwrIdIWhh/I+khkhbMmvT0p4G/kPSvJK2ndn4G2CzpMWAZ8NvjfSYrNm99b2ZmmXFLxczMMuOBerNjTNKVDO9u+l5EXDWZ72XF5O4vMzPLjLu/zMwsMw4qZmaWGQcVMzPLjIOKmZll5v8DgPanaaEr2PAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minutes of usage of all kind of calls outside the operator T network for the month of September\n",
    "\n",
    "univariate(churn.offnet_mou_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWIWHFJpd81l"
   },
   "source": [
    "## Bivariate EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "riv0S-gvd81l"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the relationship between whether the customer churned or not and the age on network (number of days the customer is using the operator T network)\n",
    "bivariate(churn_filtered.churn, churn_filtered.aon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "34_7xaXtd81l"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEHCAYAAACk6V2yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUUElEQVR4nO3dfXBcV3nH8d9jrYxsK6G24jKxMdkERW1SQnlRobSUGrBTKS64DZSEf6QA0wwF7MQz8TRFGiQNzqS4YIhFBxpeGplmQlKalHiCBHYbQ1uYFpmJ89I0RFC5cZ0Xs8QQ4WkiyU//2LvL7nolr6292ntPvp8Zje7LuUfP2Wv/dHW0utfcXQCA8CxpdAEAgHgQ8AAQKAIeAAJFwANAoAh4AAhUptEFlDrvvPM8m802ugwASI2DBw/+xN1XV9uXqIDPZrMaHx9vdBkAkBpmdniufUzRAECgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAIVRMAPDw9reHi40WUAQKIEEfBjY2MaGxtrdBkAkChBBDwA4FQEPAAEioAHgEAR8AAQKAIeAAJFwANAoAh4AAgUAQ8AgSLgASBQBDwABIqAB4BAEfAAECgCHgACRcADQKAIeAAIFAEPAIEi4AEgUAQ8AASKgAeAQBHwABCoTKMLqIcTJ040ugQASJwgAt7dG10CACQOUzQAECgCHgACRcADQKAIeAAIFAEPAIEi4AEgUAQ8AASKgAeAQBHwABAoAh4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQqCCe6FSwfv36RpdwVlauXKnjx4/P+2SqpUuXKpvNatOmTfr0pz+tgYEBnXPOOdq+fbvcXWvWrNHx48d1ww03aOfOnXr++eeVyWTU1NSkNWvWKJPJ6IUXXtBTTz0lM9Pw8LAkacuWLVq1apWOHj2qj33sY7r99tt19OhRbd++XTt37tS6det0880369lnn9WWLVuK621tbZKkXC6n/v5+zc7OqqmpSTt27CjuKzUxMaHrrrtOt9xyi9rb25XL5TQ0NKSBgQG1tbUV13t6ejQwMKChoSHdeuuteuKJJ/Txj39ce/bsKbYtVdqPJA0NDWnr1q3avXt31fbVjpurzWKqtZ6k1V2Q1LrSIM7XzuJ83J2ZdUm6RVKTpC+6+1/O176zs9PHx8fP+OukNdjPhpnJ3ZXJZNTS0qKpqamy/ZlMRjMzM6ftJ5vNSpImJyerHlu6vHnzZh06dKjYdvPmzdq2bZskadeuXbr33nuLfZTuK3XNNddocnJS2WxWt912m3bt2qW9e/fqne98p7Zt21ZcX7FihaamptTa2locW2trq37xi18U25Yq7cfdtXfvXl1wwQU6fPhw1fbVjpurzWKqtZ6k1V2Q1LrSYKGvnZkddPfOavtim6IxsyZJfy2pW9Klkt5rZpfW++u8mMJd+uXzZ2dmZk4J98L2WkxOTpaFe+Wxpcv33XdfWdtvfOMbyuVyyuVyGh0dLetjdHRUuVyubNvExETx+MnJSY2Pj2tsbEzurrGxMU1MTBTXC2MqHdvU1FSxbWnfuVyueNzo6GhxeXJysmr7asfN1WYx1VpP0uouSGpdaRD3axfnHPwbJE24+4/d/QVJX5W0Ocavh5hUftOYnp7Wnj17NDIyMue+Ujt27ChbHxwc1MmTJyVJs7Oz2rFjR3F9PrOzs2V9j4yMFI+bnp7W9PT0vO2rHTdXm8VUaz1Jq7sgqXWlQdyvXZwBv1bSEyXrR6JtZczsWjMbN7PxY8eOxVgO6mnfvn3av3//Kb83cHft27evbFvlTwpTU1PFbwwzMzOanJys6SePmZmZsr73799fPM7dT6mlsn214+Zqs5hqrSdpdRckta40iPu1izPgrcq2Uyb83f1Wd+90987Vq1fHWA7qaePGjdqwYYPMyk+zmWnjxo1l2wrz/QWtra3KZPK/389kMspms8X1+WQymbK+N2zYUDzOzE6ppbJ9tePmarOYaq0naXUXJLWuNIj7tYsz4I9IWley/nJJR2P8eohJZfg2Nzerp6dHvb29c+4r1d/fX7Y+ODioJUvy//SamprU399fXJ9PU1NTWd+9vb3F45qbm9Xc3Dxv+2rHzdVmMdVaT9LqLkhqXWkQ92sXZ8B/X9LFZnahmS2VdLWke09zzBk7cOBAvbtMtMJVaiaTUWtr6yn7a7kSlvJX1ZVX1qXHli5v2rSprO0VV1yhtrY2tbW1qbu7u6yP7u7uU97q1d7eXjw+m82qs7NTXV1dMjN1dXWpvb29uF4YU+nYWltbi21L+25rayse193dXVzOZrNV21c7bq42i6nWepJWd0FS60qDuF+72ALe3WckfUTSNyU9Kukud38krq+XZitXrjxleqHS0qVL1dHRoeuvv16S1NfXp8HBweJxa9as0fLly/XRj35ULS0tMjM1NzerpaVFF110kTo6OpTNZtXS0qJly5apv79f/f39WrZsmdauXSszU19fn9rb27V8+XL19fVp2bJl6ujoUE9PT7FtYb2gt7dXl1xyiTo6OnTJJZfMeQXS39+vFStWFK/me3t7ddlllxXbF9YHBwe1YsUKDQ4OqqOjQ8uWLdPg4GBZ21Kl/RSW+/v752xf7bgkqLWepNVdkNS60iDO1y7W98GfqYW+D/7FdjUPAA15HzwAoLEIeAAIFAEPAIEi4AEgUAQ8AASKgAeAQBHwABAoAh4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAJV2/PdEu50T0MCgBejIAJ++fLljS4BABKHKRoACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAJFwANAoAh4AAgUAQ8AgSLgASBQBDwABIqAB4BAEfAAECgCHgACRcADQKAIeAAIFAEPAIEi4AEgUDU9k9XMOiRtl3RB6THu/raY6jojXV1djS4BABKn1odu/72kz0v6gqTZ+Mo5O1u2bGl0CQCQOLUG/Iy7fy7WSgAAdVXrHPxeM/uQmZ1vZqsKH7FWBgBYkFqv4Hujz9tLtrmki+pbDgCgXk4b8Ga2RNKN7n7nItQDAKiT007RuPtJSR9ehFoAAHVU6xz8PjO7wczWMQcPAOlQ6xz8+6PPpVfyzMEDQILVFPDufmHchQAA6qvWv2Ttqbbd3ffUtxwAQL3UOkXzWyXLLZLeLukHkgh4AEioWqdoyu4FYGYvlfSVWCoCANTF2d5N8oSki+tZCACgvmqdg9+r/LtmpPw3hUsl3RVXUQCAhat1Dv6TJcszkg67+5EY6gEA1Emtc/DfjrsQAEB91TQHb2ZXmtnjZvYzM/u5mT1nZj+PuzgAwNmrdYpmp6R3uPujcRYDAKifWt9F8zThDgDpMu8VvJldGS2Om9mdkv5R0vOF/e5+d3ylAQAW4nRTNO+IPrvy732/vGSfS0pEwA8PD0vi2awAUGregHf390mSmY1Ius7dj0frKyV9KvbqajQ2NiaJgAeAUrXOwb+6EO6S5O7PSnptLBUBAOqi1oBfEl21S5Kih33U+g4cAEAD1BrSn5L0XTP7mvJz7++RdFNsVQEAFqzWv2TdY2bjkt4mySRd6e7/GWtlAIAFqXmaJQp0Qh0AUuJsbxcMAEg4Ah4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAJFwANAoAh4AAgUAQ8AgSLgASBQBDwABCqIx+6dOHGi0SUAQOIEEfDu3ugSACBxmKIBgEAR8AAQKAIeAAJFwANAoAh4AAgUAQ8AgSLgASBQBDwABIqAB4BAEfAAECgCHgACRcADQKAIeAAIFAEPAIEi4AEgUAQ8AASKgAeAQMUW8Gb2ZTN7xswejutrVFq/fr0OHjwoScrlctq6datyuVzZcqX59iVJWuoEkBxxXsHfJqkrxv6rGhgYkCSNjIzooYce0p49e8qWK823L0nSUieA5Igt4N39O5J+Glf/Bbt37y5bn5qa0v3336+xsTG5u0ZHRzU6Oip319jYWNkVcC6XK7ar3JckaakTQLKkfg7+7rvvPmXbTTfdpJMnT0qSpqenNTMzI0manZ0tuwIeGRkptqvclyRpqRNAsjQ84M3sWjMbN7PxY8eO1aXPmZmZYqi7u9y9uH3fvn3Fdvv37y+2q9yXJGmpE0CyNDzg3f1Wd+90987Vq1fXpc9MJqNMJiNJMjOZWXH7xo0bi+02bNhQbFe5L0nSUieAZGl4wC/UlVdeecq2vr4+LVmSH1pzc3MxHJuamtTT01Ns19vbW2xXuS9J0lIngGSJ822Sd0j6nqRfM7MjZvaBOL7O1q1by9ZbW1v11re+VV1dXTIzdXd3q7u7W2amrq4utbW1Fdu2tbUV21XuS5K01AkgWTJxdezu742r7/kMDQ1Jyl/1Tk5OFq92S5dLVbZLqrTUCSA5rPALyCTo7Oz08fHxMz5u/fr1kqQDBw7UtyAASDgzO+jundX2pX4OHgBQHQEPAIEi4AEgUAQ8AASKgAeAQBHwABAoAh4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAIV2wM/FlPhmasAgF8KIuCXL1/e6BIAIHGYogGAQBHwABAoAh4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgEAR8AAQKAIeAAJFwANAoAh4AAgUAQ8AgSLgASBQBDwABIqAB4BAEfAAECgCHgACRcADQKCCeOh2V1dXo0sAgMQJIuC3bNnS6BIAIHGYogGAQBHwABAoAh4AAkXAA0CgCHgACBQBDwCBIuABIFAEPAAEioAHgECZuze6hiIzOybp8Fkefp6kn9SxnKRinGFhnGFpxDgvcPfV1XYkKuAXwszG3b2z0XXEjXGGhXGGJWnjZIoGAAJFwANAoEIK+FsbXcAiYZxhYZxhSdQ4g5mDBwCUC+kKHgBQgoAHgEClPuDNrMvMHjOzCTO7sdH1LJSZTZrZQ2b2gJmNR9tWmdk+M3s8+ryypP1fRGN/zMz+oHGVz8/Mvmxmz5jZwyXbznhcZvb66PWZMLPdZmaLPZbTmWOsg2b2v9F5fcDMrijZl7qxmtk6M7vfzB41s0fM7Lpoe1DndJ5xpuN8untqPyQ1SfqRpIskLZV0SNKlja5rgWOalHRexbadkm6Mlm+U9Ilo+dJozC+RdGH0WjQ1egxzjOstkl4n6eGFjEvSf0h6kySTNCqpu9Fjq3Gsg5JuqNI2lWOVdL6k10XL50j6YTSWoM7pPONMxflM+xX8GyRNuPuP3f0FSV+VtLnBNcVhs6SRaHlE0h+VbP+quz/v7v8taUL51yRx3P07kn5asfmMxmVm50s6192/5/n/MXtKjkmMOcY6l1SO1d2fdPcfRMvPSXpU0loFdk7nGedcEjXOtAf8WklPlKwf0fwvfhq4pG+Z2UEzuzba9jJ3f1LK/4OT9KvR9rSP/0zHtTZartyeFh8xswejKZzC1EXqx2pmWUmvlfTvCvicVoxTSsH5THvAV5vDSvv7Pn/X3V8nqVvSh83sLfO0DXH80tzjSvN4PyfplZJeI+lJSZ+Ktqd6rGbWKukfJF3v7j+fr2mVbWkeZyrOZ9oD/oikdSXrL5d0tEG11IW7H40+PyPpHuWnXJ6OfsRT9PmZqHnax3+m4zoSLVduTzx3f9rdZ939pKQv6JdTaakdq5k1Kx96t7v73dHm4M5ptXGm5XymPeC/L+liM7vQzJZKulrSvQ2u6ayZ2QozO6ewLOlySQ8rP6beqFmvpK9Hy/dKutrMXmJmF0q6WPlf5KTFGY0r+pH/OTP77egdCD0lxyRaIfQif6z8eZVSOtaopi9JetTdd5XsCuqczjXO1JzPRv+WeqEfkq5Q/jfbP5LU1+h6FjiWi5T/DfwhSY8UxiOpTdI/SXo8+ryq5Ji+aOyPKUHvPqgytjuU/1F2WvmrmQ+czbgkdSr/n+lHkj6r6K+xk/Qxx1i/IukhSQ8qHwLnp3mskt6s/BTDg5IeiD6uCO2czjPOVJxPblUAAIFK+xQNAGAOBDwABIqAB4BAEfAAECgCHgACRcADQKAIeKAGZnabmb17gX18ycwORfcv+Vr05+9AbAh4YPFsc/ffdPdXS/ofSR9pdEEIGwGPVIpu63BfdEX8sJldFT1Q4dvRnTi/WXJPlANm9hkz+27Utuotlc1sieUfuPIrJdsmzOxl0eoGM/sXM/uhmf1htL/JzD4ZPcjhQTPbMlfNHt2MK/pT9WWKbjZlZqst/3CMH5jZ35jZYTM7rx6vE17cCHikVZeko9EV8askjUkalvRud3+9pC9Luqmk/Qp3/x1JH4r2ncLzN476uvL3FpGZvVHSpLs/HTXJSvp9SZskfd7MWiRdq/yDHV4bXZnfPl/RZva3kp6S9OtRvZI0IOmfPX8X0XskvaLWFwGYDwGPtHpI+SvqT5jZ7yl/B79XSdpnZg9I6lf53fvukIoP4zi39Cq9wp2SroqWr47WC+5y95Pu/rikHysf0hskfd7dZ6L+533Qh7u/T9Ia5R8cUfg6b1b+YTVy9zFJz847cqBGBDxSyd1/KOn1ygf9zZLeJekRd39N9HGZu19eekhlF3N0/T1J7Wa2Wvkn7txdsq9aHzZPX3PVPqv8N453RZsS8wxShIWARyqZ2RpJJ9z97yR9UtIbJa02szdF+5vN7DdKDrkq2v5mST9z959V69fzd9+7R9Iu5W8RmyvZ/SfRPP0rlb/z52OSviXpg2aWifpfNUe9ZmbthWVJ75D0X9Huf5X0nmjf5ZJWVusDOFOZRhcAnKXLJP2VmZ1U/ra8fyZpRtJuM3up8v+2P6P8bZcl6Vkz+66kcyW9/zR936n8swauqdj+mKRvS3qZpA+6+/+Z2RcldUh60MymlX/4w2er9GmSRszs3Gj5UFSzJA1JusPMror6f1LSc6d7AYDT4XbBCJ6ZHZB0g7uPN7qWaszsJZJm3X0m+gnkc+7+mgaXhQBwBQ803isk3WVmSyS9IOlPG1wPAsEVPF6UzOx9kq6r2Pxv7v7hOvR9j/JvnSz15+7+zYX2DZwJAh4AAsW7aAAgUAQ8AASKgAeAQBHwABCo/wdRsoGRfqvfGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the relationship between the 3G volume based cost in Sept (when no specific scheme is not purchased and paid as per usage) and whether the customer churned or not\n",
    "bivariate(churn_filtered.sep_vbc_3g, churn_filtered.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "J9hjNJSDd81l"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the relationship between the minutes of usage of special outgoing calls in the month of August and whether the customer churned or not\n",
    "bivariate(churn_filtered.spl_og_mou_8, churn_filtered.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "PGPuEp33d81l"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>night_pck_user_8</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.89123</td>\n",
       "      <td>97.117602</td>\n",
       "      <td>97.360704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.10877</td>\n",
       "      <td>2.882398</td>\n",
       "      <td>2.639296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "night_pck_user_8      -1.0        0.0        1.0\n",
       "churn                                           \n",
       "0                 85.89123  97.117602  97.360704\n",
       "1                 14.10877   2.882398   2.639296"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the relationship between whether the customer churned or not and the night package used by users in August\n",
    "pd.crosstab(churn_filtered.churn, churn_filtered.night_pck_user_8, normalize='columns')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "A97BvB99d81l"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sachet_3g_8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>11.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>13.0</th>\n",
       "      <th>14.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>17.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>19.0</th>\n",
       "      <th>20.0</th>\n",
       "      <th>21.0</th>\n",
       "      <th>23.0</th>\n",
       "      <th>25.0</th>\n",
       "      <th>27.0</th>\n",
       "      <th>29.0</th>\n",
       "      <th>30.0</th>\n",
       "      <th>38.0</th>\n",
       "      <th>41.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24972</td>\n",
       "      <td>1609</td>\n",
       "      <td>399</td>\n",
       "      <td>184</td>\n",
       "      <td>106</td>\n",
       "      <td>86</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2369</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sachet_3g_8   0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0   \\\n",
       "churn                                                                      \n",
       "0            24972  1609   399   184   106    86    43    35    28    19   \n",
       "1             2369    48     5     8     4     2     1     0     2     0   \n",
       "\n",
       "sachet_3g_8  10.0  11.0  12.0  13.0  14.0  15.0  16.0  17.0  18.0  19.0  20.0  \\\n",
       "churn                                                                           \n",
       "0              15     8    11    10     6     6     2     2     3     1     3   \n",
       "1               0     0     0     1     0     0     1     0     0     0     0   \n",
       "\n",
       "sachet_3g_8  21.0  23.0  25.0  27.0  29.0  30.0  38.0  41.0  \n",
       "churn                                                        \n",
       "0               3     2     1     1     1     2     1     1  \n",
       "1               0     0     0     0     0     0     0     0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the relationship between whether the customer churned or not and the 3G service schemes with validity smaller than a month for the month of August\n",
    "pd.crosstab(churn_filtered.churn, churn_filtered.sachet_3g_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqT4kIOt14xS"
   },
   "source": [
    "# Task 5: Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrPWcHw-d81m"
   },
   "source": [
    "### Cap outliers in all numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Hka8SDhOd81m"
   },
   "outputs": [],
   "source": [
    "# function for capping outliers\n",
    "def cap_outliers(array, k=3):\n",
    "    upper_limit = array.mean() + k*array.std()\n",
    "    lower_limit = array.mean() - k*array.std()\n",
    "    array[array<lower_limit] = lower_limit\n",
    "    array[array>upper_limit] = upper_limit\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "dJeeN7dNd81m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array after capping outliers: \n",
      " [-4194     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14    15    16    17    18    19    20    21    22    23\n",
      "    24    25    26    27    28    29    30    31    32    33    34    35\n",
      "    36    37    38    39    40    41    42    43    44    45    46    47\n",
      "    48    49    50    51    52    53    54    55    56    57    58    59\n",
      "    60    61    62    63    64    65    66    67    68    69    70    71\n",
      "    72    73    74    75    76    77    78    79    80    81    82    83\n",
      "    84    85    86    87    88    89    90    91    92    93    94    95\n",
      "    96    97    98  4291]\n"
     ]
    }
   ],
   "source": [
    "# example of capping\n",
    "sample_array = list(range(100))\n",
    "\n",
    "# add outliers to the data\n",
    "sample_array[0] = -9999\n",
    "sample_array[99] = 9999\n",
    "\n",
    "# cap outliers\n",
    "sample_array = np.array(sample_array)\n",
    "print(\"Array after capping outliers: \\n\", cap_outliers(sample_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "uT1T-mVEd81m"
   },
   "outputs": [],
   "source": [
    "# cap outliers in all the numeric columns using your outlier capping function\n",
    "\n",
    "churn_filtered[num_cols] = churn_filtered[num_cols].apply(cap_outliers, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLNDQNUEd81m"
   },
   "source": [
    "# Task 6: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTAIxBWMd81m"
   },
   "source": [
    "## i) Importing necessary libraries for machine learning and deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "l-io0IBMd81m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 20:23:48.133875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#algorithms for sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#baseline linear model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#modules for hyper parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#modules for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, r2_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# Import methods for building neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Import 'KerasClassifier' from 'keras' for connecting neural networks with 'sklearn' and 'GridSearchCV'\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC73FFRid81m"
   },
   "source": [
    "## ii) Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "sVJ6idKud81m"
   },
   "outputs": [],
   "source": [
    "# change churn to numeric\n",
    "\n",
    "churn_filtered['churn'] = pd.to_numeric(churn_filtered['churn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbkDXbbnd81m"
   },
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "NoLvPspud81m"
   },
   "outputs": [],
   "source": [
    "# Extract input and output data\n",
    "\n",
    "X = churn_filtered.drop(\"churn\", axis = 1)\n",
    "y = churn_filtered.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uPjpQffFvMvJ"
   },
   "outputs": [],
   "source": [
    "# Use dummy variables for categorical variables\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "g6y3azekd81m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 178)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(22500,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7501, 178)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7501,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide data into train and test\n",
    "# Note: Set the 'random_state' parameter to '4'\n",
    "# Note: Set the 'test_size' parameter to '0.25'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4)\n",
    "\n",
    "\n",
    "# print shapes of train and test sets\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "4dv9lxqeL_dE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    13780\n",
       "1     1221\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X.to_numpy()\n",
    "\n",
    "#train-test split using stratified K fold\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X_new,y)\n",
    "\n",
    "for train_index, test_index in skf.split(X_new,y):\n",
    "  X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('\\n')\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KByoUELzKEaj"
   },
   "source": [
    "Observe that the dataset is imbalanced. You should get the number of entries with output '1' approximately 1/10th of the number of entries with output '0'. This means that if we run a simple machine learning model, it should already show 90% accuracy.\n",
    "\n",
    "But in this case study, it is the most important for the model to predict which customer will churn as this will decide how their business is performing. We have to create a model that will predict the output '1' accurately. But its corresponding number of entries are very less.\n",
    "\n",
    "Hence, we will be doing some sampling methods to make the data set balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM8MHjnJKu2p"
   },
   "source": [
    "1) **Random Under-Sampling**: This method basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n",
    "\n",
    "We have seen how imbalanced the data set is. With random under-sampling, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. This means that if there are 1221 '0' class data entries, then there will be 1221 '1' class data entries by removing the rest. \n",
    "\n",
    "Note: The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "3Hpgqwib_0_c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1221\n",
       "1    1221\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random under sampling using imblearn\n",
    "# Use the RandomUnderSampler (RUS) function to produce new X and y from X_train and y_train\n",
    "# Use random_state as 1 for reproducibility\n",
    "rus = RandomUnderSampler(random_state = 1)\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "y_rus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7NHvHUCT_1nt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    855\n",
       "0    854\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rus, X_test_rus, y_train_rus, y_test_rus = train_test_split(X_rus, y_rus, test_size=0.3, random_state=42, stratify=y_rus)\n",
    "y_train_rus.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5sjXpfDMrYM"
   },
   "source": [
    "1) **Random Over-Sampling**: This method basically consists of adding data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n",
    "\n",
    "We have seen how imbalanced the data set is. With random over-sampling, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. This means that if there are 13780 '1' class data entries, then there will be 13780 '0' class data entries by removing the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "JH3RugzjCwfE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13780\n",
       "0    13780\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random over sampling with imblearn\n",
    "# Use the RandomOverSampler (ROS) function to produce new X and y from X_train and y_train\n",
    "# Use random_state as 1 for reproducibility\n",
    "ros = RandomOverSampler(random_state = 1)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "y_ros.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "b_lyHu2WCmhM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11024\n",
       "1    11024\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train Test split\n",
    "X_train_ros, X_test_ros, y_train_ros, y_test_ros = train_test_split(X_ros, y_ros, test_size=0.2, stratify=y_ros, random_state=42)\n",
    "y_train_ros.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcfKik7VM7Dv"
   },
   "source": [
    "Now, let's test different machine learning models over the three data sets, namely, the original cleaned data set, the under-sampled data set and the over-sampled data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWkCTv8E4Qv"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "u0owJrjCsX1o"
   },
   "outputs": [],
   "source": [
    "# Defining the logistic regression model and fit it on the normal X_train and y_train\n",
    "# 'penalty' is set to 'none'\n",
    "# 'solver' is set to 'lbfgs'\n",
    "# 'random_state' is set to 0\n",
    "# 'max_iter' is set to 100\n",
    "# You can change these values or use GridSearchCV to perform hyperparameter tuning to find the optimal performing model\n",
    "model_name = 'Logistic Regression - without balancing'\n",
    "\n",
    "logreg_model = LogisticRegression(penalty = 'none', solver = 'lbfgs',  max_iter = 100, random_state = 0)\n",
    "logreg_model = logreg_model.fit(X_train, y_train)\n",
    "train_y_pred = logreg_model.predict(X_train)\n",
    "test_y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "log_train_acc = accuracy_score(y_train, train_y_pred)\n",
    "log_val_acc = accuracy_score(y_test, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test, test_y_pred)\n",
    "precision = precision_score(y_test, test_y_pred)\n",
    "recall = recall_score(y_test, test_y_pred)\n",
    "# creating a dataframe to compare the performance of different models\n",
    "model_eval_data = [[model_name, log_train_acc, log_val_acc, f_score, precision, recall]]\n",
    "evaluate_df = pd.DataFrame(model_eval_data, columns=['Model Name', 'Training Score', 'Testing Score',\n",
    "                                          'F1 Score', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "lvz959sUD_vp"
   },
   "outputs": [],
   "source": [
    "# Defining the logistic regression model and fit it on the random under sampled X_train_rus and y_train_rus\n",
    "# 'penalty' is set to 'none'\n",
    "# 'solver' is set to 'lbfgs'\n",
    "# 'random_state' is set to 0\n",
    "# 'max_iter' is set to 100\n",
    "model_name = 'Logistic Regression - Random Undersampling'\n",
    "\n",
    "logreg_model_1 = LogisticRegression(penalty = 'none', solver = 'lbfgs',  max_iter = 100, random_state = 0)\n",
    "logreg_model_1 = logreg_model_1.fit(X_train_rus, y_train_rus)\n",
    "train_y_pred = logreg_model_1.predict(X_train_rus)\n",
    "test_y_pred = logreg_model_1.predict(X_test_rus)\n",
    "\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "log_train_acc = accuracy_score(y_train_rus, train_y_pred)\n",
    "log_val_acc = accuracy_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_rus, test_y_pred)\n",
    "precision = precision_score(y_test_rus, test_y_pred)\n",
    "recall = recall_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, log_train_acc, log_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "TCv-6xFgEoka"
   },
   "outputs": [],
   "source": [
    "# Defining the logistic regression model and fit it on the random over sampled X_train_ros and y_train_ros\n",
    "# 'penalty' is set to 'none'\n",
    "# 'solver' is set to 'lbfgs'\n",
    "# 'random_state' is set to 0\n",
    "# 'max_iter' is set to 100\n",
    "model_name = 'Logistic Regression - Random Oversampling'\n",
    "\n",
    "logreg_model_2 = LogisticRegression(penalty = 'none', solver = 'lbfgs',  max_iter = 100, random_state = 0)\n",
    "logreg_model_2 = logreg_model_2.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = logreg_model_2.predict(X_train_ros)\n",
    "test_y_pred = logreg_model_2.predict(X_test_ros)\n",
    "\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "log_train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "log_val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, log_train_acc, log_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoQgEN5IE-Rn"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "46sqkVkOrk-n"
   },
   "outputs": [],
   "source": [
    "# Defining the decision tree model and fit it on the normal X_train and y_train \n",
    "# 'max_depth' is set to 50\n",
    "# 'random_state' is set to 0\n",
    "# You can change these values or use GridSearchCV to perform hyperparameter tuning to find the optimal performing model\n",
    "model_name = 'Decision Tree - without balancing'\n",
    "\n",
    "DT = DecisionTreeClassifier(max_depth = 50, random_state = 0)\n",
    "DT = DT.fit(X_train, y_train)\n",
    "train_y_pred = DT.predict(X_train)\n",
    "test_y_pred = DT.predict(X_test)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "tree_train_acc = accuracy_score(y_train, train_y_pred)\n",
    "tree_val_acc = accuracy_score(y_test, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test, test_y_pred)\n",
    "precision = precision_score(y_test, test_y_pred)\n",
    "recall = recall_score(y_test, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, tree_train_acc, tree_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "v71WidfMFwob"
   },
   "outputs": [],
   "source": [
    "# Defining the decision tree model and fit it on the random under sampled X_train_rus and X_train_rus\n",
    "# 'max_depth' is set to 50\n",
    "# 'random_state' is set to 0\n",
    "model_name = 'Decision Tree - Random Undersampling'\n",
    "DT1 = DecisionTreeClassifier(max_depth = 50, random_state = 0)\n",
    "DT1 = DT1.fit(X_train_rus, y_train_rus)\n",
    "train_y_pred = DT1.predict(X_train_rus)\n",
    "test_y_pred = DT1.predict(X_test_rus)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "tree_train_acc = accuracy_score(y_train_rus, train_y_pred)\n",
    "tree_val_acc = accuracy_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_rus, test_y_pred)\n",
    "precision = precision_score(y_test_rus, test_y_pred)\n",
    "recall = recall_score(y_test_rus, test_y_pred)\n",
    "\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, tree_train_acc, tree_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ZYzQ7QSGFw9j"
   },
   "outputs": [],
   "source": [
    "# Defining the decision tree model and fit it on the random over sampled X_train_ros and y_train_ros\n",
    "# 'max_depth' is set to 50\n",
    "# 'random_state' is set to 0\n",
    "model_name = 'Decision Tree - Random Oversampling'\n",
    "\n",
    "DT2 = DecisionTreeClassifier(max_depth = 50, random_state = 0)\n",
    "DT2 = DT2.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = DT2.predict(X_train_ros)\n",
    "test_y_pred = DT2.predict(X_test_ros)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "tree_train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "tree_val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, tree_train_acc, tree_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh_R_oUOFBie"
   },
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "LcsoLa3frlZ_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=14)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=14)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=14)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the kNN model and fit it on the normal X_train and y_train \n",
    "# 'n_neighbors' is set to 14\n",
    "# You can change these values or use GridSearchCV to perform hyperparameter tuning to find the optimal performing model\n",
    "model_name = 'kNN - without balancing'\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 14)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "train_y_pred = knn_clf.predict(X_train)\n",
    "test_y_pred = knn_clf.predict(X_test)\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "knn_train_acc = accuracy_score(y_train, train_y_pred)\n",
    "knn_val_acc = accuracy_score(y_test, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test, test_y_pred)\n",
    "precision = precision_score(y_test, test_y_pred)\n",
    "recall = recall_score(y_test, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, knn_train_acc, knn_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "rQeM6F8WGsUH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=14)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=14)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=14)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the kNN model and fit it on the random under sampled X_train_rus and y_train_rus\n",
    "# 'n_neighbors' is set to 14\n",
    "model_name = 'kNN - Random Undersampling'\n",
    "\n",
    "knn_clf1 = KNeighborsClassifier(n_neighbors = 14)\n",
    "knn_clf1.fit(X_train_rus, y_train_rus)\n",
    "train_y_pred = knn_clf1.predict(X_train_rus)\n",
    "test_y_pred = knn_clf1.predict(X_test_rus)\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "knn_train_acc = accuracy_score(y_train_rus, train_y_pred)\n",
    "knn_val_acc = accuracy_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_rus, test_y_pred)\n",
    "precision = precision_score(y_test_rus, test_y_pred)\n",
    "recall = recall_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, knn_train_acc, knn_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "zr7T95KCG2wV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=14)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=14)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=14)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the kNN model and fit it on the random over sampled X_train_ros and y_train_ros\n",
    "# 'n_neighbors' is set to 14\n",
    "model_name = 'kNN - Random Oversampling'\n",
    "\n",
    "knn_clf2 = KNeighborsClassifier(n_neighbors = 14)\n",
    "knn_clf2.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = knn_clf2.predict(X_train_ros)\n",
    "test_y_pred = knn_clf2.predict(X_test_ros)\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "knn_train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "knn_val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, knn_train_acc, knn_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx4zM-tJFEkR"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "zxXdLPWl3z0d"
   },
   "outputs": [],
   "source": [
    "# Defining the Random Forest Classifier model and fit it on the normal X_train and y_train \n",
    "# 'n_estimators' is set to 200\n",
    "# 'max_depth' is set to 5\n",
    "# 'class_weight' is set to 'balanced'\n",
    "# 'random_state' is set to 123\n",
    "# You can change these values or use GridSearchCV to perform hyperparameter tuning to find the optimal performing model\n",
    "model_name = 'Random Forest - without balancing'\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 200, max_depth = 5, class_weight = 'balanced', random_state = 123)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "train_y_pred = rf.predict(X_train)\n",
    "test_y_pred = rf.predict(X_test)\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train, train_y_pred)\n",
    "rf_val_acc = accuracy_score(y_test, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test, test_y_pred)\n",
    "precision = precision_score(y_test, test_y_pred)\n",
    "recall = recall_score(y_test, test_y_pred)\n",
    "\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, rf_train_acc, rf_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "lNBgzLvEHqgs"
   },
   "outputs": [],
   "source": [
    "# Defining the Random Forest Classifier model and fit it on the random under sampled X_train_rus and y_train_rus\n",
    "# 'n_estimators' is set to 200\n",
    "# 'max_depth' is set to 5\n",
    "# 'class_weight' is set to 'balanced'\n",
    "# 'random_state' is set to 123\n",
    "model_name = 'Random Forest - Random Undersampling'\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators = 200, max_depth = 5, class_weight = 'balanced', random_state = 123)\n",
    "rf1 = rf1.fit(X_train_rus, y_train_rus)\n",
    "train_y_pred = rf1.predict(X_train_rus)\n",
    "test_y_pred = rf1.predict(X_test_rus)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train_rus, train_y_pred)\n",
    "rf_val_acc = accuracy_score(y_test_rus, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_rus, test_y_pred)\n",
    "precision = precision_score(y_test_rus, test_y_pred)\n",
    "recall = recall_score(y_test_rus, test_y_pred)\n",
    "\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, rf_train_acc, rf_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "fbQVjASQHq-o"
   },
   "outputs": [],
   "source": [
    "# Defining the Random Forest Classifier model and fit it on the random over sampled X_train_ros and y_train_ros\n",
    "# 'n_estimators' is set to 200\n",
    "# 'max_depth' is set to 5\n",
    "# 'class_weight' is set to 'balanced'\n",
    "# 'random_state' is set to 123\n",
    "model_name = 'Random Forest - Random Oversampling'\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators = 200, max_depth = 5, class_weight = 'balanced', random_state = 123)\n",
    "rf2 = rf2.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = rf2.predict(X_train_ros)\n",
    "test_y_pred = rf2.predict(X_test_ros)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "rf_val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# adding calculations to dataframe\n",
    "model_eval_data = [model_name, rf_train_acc, rf_val_acc, f_score, precision, recall]\n",
    "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
    "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "JuRWiDJTIHc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression - without balancing</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.518594</td>\n",
       "      <td>0.685061</td>\n",
       "      <td>0.417213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression - Random Undersampling</td>\n",
       "      <td>0.851960</td>\n",
       "      <td>0.845839</td>\n",
       "      <td>0.847503</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression - Random Oversampling</td>\n",
       "      <td>0.848603</td>\n",
       "      <td>0.853048</td>\n",
       "      <td>0.852781</td>\n",
       "      <td>0.854334</td>\n",
       "      <td>0.851234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree - without balancing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>0.486637</td>\n",
       "      <td>0.473970</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree - Random Undersampling</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791269</td>\n",
       "      <td>0.788966</td>\n",
       "      <td>0.796657</td>\n",
       "      <td>0.781421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree - Random Oversampling</td>\n",
       "      <td>0.999546</td>\n",
       "      <td>0.972424</td>\n",
       "      <td>0.973164</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kNN - without balancing</td>\n",
       "      <td>0.939471</td>\n",
       "      <td>0.935800</td>\n",
       "      <td>0.444957</td>\n",
       "      <td>0.749515</td>\n",
       "      <td>0.316393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kNN - Random Undersampling</td>\n",
       "      <td>0.824459</td>\n",
       "      <td>0.806276</td>\n",
       "      <td>0.788690</td>\n",
       "      <td>0.866013</td>\n",
       "      <td>0.724044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kNN - Random Oversampling</td>\n",
       "      <td>0.895183</td>\n",
       "      <td>0.875544</td>\n",
       "      <td>0.882935</td>\n",
       "      <td>0.833441</td>\n",
       "      <td>0.938679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest - without balancing</td>\n",
       "      <td>0.913939</td>\n",
       "      <td>0.900333</td>\n",
       "      <td>0.552261</td>\n",
       "      <td>0.435111</td>\n",
       "      <td>0.755738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest - Random Undersampling</td>\n",
       "      <td>0.899941</td>\n",
       "      <td>0.855389</td>\n",
       "      <td>0.847262</td>\n",
       "      <td>0.896341</td>\n",
       "      <td>0.803279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest - Random Oversampling</td>\n",
       "      <td>0.876270</td>\n",
       "      <td>0.871734</td>\n",
       "      <td>0.867379</td>\n",
       "      <td>0.897864</td>\n",
       "      <td>0.838897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model Name  Training Score  Testing Score  \\\n",
       "0      Logistic Regression - without balancing        0.938737       0.937000   \n",
       "1   Logistic Regression - Random Undersampling        0.851960       0.845839   \n",
       "2    Logistic Regression - Random Oversampling        0.848603       0.853048   \n",
       "3            Decision Tree - without balancing        1.000000       0.914200   \n",
       "4         Decision Tree - Random Undersampling        1.000000       0.791269   \n",
       "5          Decision Tree - Random Oversampling        0.999546       0.972424   \n",
       "6                      kNN - without balancing        0.939471       0.935800   \n",
       "7                   kNN - Random Undersampling        0.824459       0.806276   \n",
       "8                    kNN - Random Oversampling        0.895183       0.875544   \n",
       "9            Random Forest - without balancing        0.913939       0.900333   \n",
       "10        Random Forest - Random Undersampling        0.899941       0.855389   \n",
       "11         Random Forest - Random Oversampling        0.876270       0.871734   \n",
       "\n",
       "    F1 Score  Precision    Recall  \n",
       "0   0.518594   0.685061  0.417213  \n",
       "1   0.847503   0.837333  0.857923  \n",
       "2   0.852781   0.854334  0.851234  \n",
       "3   0.486637   0.473970  0.500000  \n",
       "4   0.788966   0.796657  0.781421  \n",
       "5   0.973164   0.947730  1.000000  \n",
       "6   0.444957   0.749515  0.316393  \n",
       "7   0.788690   0.866013  0.724044  \n",
       "8   0.882935   0.833441  0.938679  \n",
       "9   0.552261   0.435111  0.755738  \n",
       "10  0.847262   0.896341  0.803279  \n",
       "11  0.867379   0.897864  0.838897  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** - As we can see in the above result that **Decision Tree - Random Oversampling** is the best model with the best F1 score, Precision and recall value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHf4ErJAwNtC"
   },
   "source": [
    "In this case study, the most important factor in the prediction performance of a machine learning model is that it should be able to predict the positive class as accurately as possible. This means that the false negatives and false positives are supposed to be as minimal as possible. This further means that precision and recall should be as high as possible. \n",
    "\n",
    "There is another factor to consider. The most important factor which can lead to a company loss is the false negatives. This is because if we predict that a customer did not churn but in reality, the customer did, the company will miss out on the data of churned customers. Hence, observing the recall factor is much more important than precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YI4KLGtIQZc"
   },
   "source": [
    "## Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "MLnOBgQ180S7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of max_depth is 24\n"
     ]
    }
   ],
   "source": [
    "# Choose the model that performs in a robust manner with good accuracy, precision and recall. \n",
    "# Especially look out for the recall value because a good recall value means that it is able to accurately classify the data examples of the customers who churned\n",
    "\n",
    "# Define your model and parameter grid\n",
    "# Make sure to use random_state value as 0\n",
    "\n",
    "base_tree_model = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "# Define the range of the 'max_depth' parameter and store it in a parameter grid dictionary\n",
    "parameters_grid = {'max_depth': np.arange(2, 26, 1)}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator = base_tree_model,\n",
    "                    param_grid = parameters_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5)\n",
    "# Perform GridSearchCV\n",
    "grid_model = grid.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "# Display the best combination of parameters obtained from GridSearchCV\n",
    "\n",
    "best_max_depth = grid_model.best_params_['max_depth']\n",
    "print('The optimal value of max_depth is', best_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "9gmbyuJIl8kB"
   },
   "outputs": [],
   "source": [
    "# Re-fit your model with the combination of parameters obtained from GridSearchCV\n",
    "# Make sure to use random_state value as 0\n",
    "\n",
    "tree_model_best = DecisionTreeClassifier(max_depth = best_max_depth, random_state = 0)\n",
    "tree_model_best = tree_model_best.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = tree_model_best.predict(X_train_ros)\n",
    "test_y_pred = tree_model_best.predict(X_test_ros)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "tree_train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "tree_val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "c5D6vcAVp-ZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2586  170]\n",
      " [   0 2756]]\n",
      "ROC:    \t 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and validation sets using accuracy, confusion metrics and AUC of ROC\n",
    "\n",
    "# predict churn on test data\n",
    "y_pred = tree_model_best.predict(X_test_ros)\n",
    "\n",
    "# create confusion matrix\n",
    "cf = confusion_matrix(y_test_ros, y_pred)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = tree_model_best.predict_proba(X_test_ros)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test_ros, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELCx6CvkIc8K"
   },
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "r7K_6w5161bs"
   },
   "outputs": [],
   "source": [
    "# Define a function to create a neural network model and specify default values for variable hyperparameters\n",
    "# Note: The number of hidden layers is fixed at 2\n",
    "# Note: The number of neurons in the second hidden layer is fixed at 64\n",
    "# Note: The output layer activation function is fixed as 'sigmoid'\n",
    "\n",
    "# You can change the hyperparameters mentioned as arguments in the create_nn function\n",
    "# So that you can use them in GridSearchCV hyperparameter tuning\n",
    "# Feel free to modify the model too and test the model performance\n",
    "# You can add more types of layers like Dropout, Batch normalization etc.\n",
    "\n",
    "# Note: The variable hyperparameters list is the activation functions of the hidden layers and number of neurons in the first hidden layer\n",
    "def create_nn(activation_function = 'relu',\n",
    "              hidden1_neurons = 256,\n",
    "             learning_rate_value = 0.001):\n",
    "\n",
    "    # Declare an instance of an artificial neural network model using the 'Sequential()' method\n",
    "    nn = Sequential()\n",
    "\n",
    "    # keras.Input is the input layer of the neural network\n",
    "    nn.add(Input(shape= (None, 178)))\n",
    "    \n",
    "\n",
    "\n",
    "    # Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 'hidden1_neurons'  - This specifies the number of neurons in the hidden layer\n",
    "    # Note: Set the 'activation' parameter to 'activation_function' - This specifies the activation function parameter defined in the custom function\n",
    "    nn.add(Dense(units = hidden1_neurons,\n",
    "                 activation = activation_function))\n",
    "    \n",
    "\n",
    "    # Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 64  - This specifies the number of neurons in the hidden layer\n",
    "    # Note: Set the 'activation' parameter to 'activation_function' - This specifies the activation function parameter defined in the custom function\n",
    "    nn.add(Dense(units = 64,\n",
    "                 activation = activation_function))\n",
    "\n",
    "    # Add the output layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 1 - Binary classification\n",
    "    # Note: Set the 'activation' parameter to 'sigmoid' - The sigmoid activation function is used for output layer neurons in binary classification tasks\n",
    "    nn.add(Dense(units = 1,\n",
    "                 activation = 'sigmoid'))\n",
    "    \n",
    "    # Compile the model using the 'compile()' method\n",
    "    # Note: Set the 'loss' parameter to 'binary_crossentropy' - The binary crossentropy loss function is commonly used for binary classification tasks\n",
    "    # Note: Set the 'metrics' parameter to 'accuracy' - This records the accuracy of the model along with the loss during training\n",
    "    # Note: Set the 'optimizer' parameter to 'RMSprop' and set its 'learning_rate' parameter to 'learning_rate_value' - This specifies the learning rate value defined in the custom function\n",
    "    nn.compile(loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'],\n",
    "               optimizer = RMSprop(learning_rate = learning_rate_value))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "zhFTIqPT6-2M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, None, 256)         45824     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, None, 64)          16448     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, None, 1)           65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,337\n",
      "Trainable params: 62,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_11'), name='input_11', description=\"created by layer 'input_11'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_11'), name='input_11', description=\"created by layer 'input_11'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "469/469 [==============================] - ETA: 0s - loss: 10.4227 - accuracy: 0.9115WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_11'), name='input_11', description=\"created by layer 'input_11'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "469/469 [==============================] - 5s 6ms/step - loss: 10.4227 - accuracy: 0.9115 - val_loss: 1.6724 - val_accuracy: 0.9314\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.6633 - accuracy: 0.9225 - val_loss: 0.2789 - val_accuracy: 0.9189\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2906 - accuracy: 0.9182 - val_loss: 0.2046 - val_accuracy: 0.9183\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2687 - accuracy: 0.9183 - val_loss: 0.2505 - val_accuracy: 0.9174\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2634 - accuracy: 0.9185 - val_loss: 0.3530 - val_accuracy: 0.9139\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2514 - accuracy: 0.9191 - val_loss: 0.4256 - val_accuracy: 0.9206\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2488 - accuracy: 0.9193 - val_loss: 0.2139 - val_accuracy: 0.9195\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2558 - accuracy: 0.9197 - val_loss: 0.3694 - val_accuracy: 0.9258\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2939 - accuracy: 0.9197 - val_loss: 0.2339 - val_accuracy: 0.9184\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3040 - accuracy: 0.9192 - val_loss: 0.2038 - val_accuracy: 0.9189\n"
     ]
    }
   ],
   "source": [
    "# Create a default neural network using the 'create_nn' function and train it on the training data\n",
    "nn1 = create_nn()\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_data' parameter to (X_val, y_val)\n",
    "# Note: Set the 'epochs' parameter to 10 - This specifies the scope of loss computations and parameter updates\n",
    "nn1.summary()\n",
    "print('\\n')\n",
    "nn1_history = nn1.fit(X_train, y_train, validation_data =  (X_test, y_test), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "-wDL8mOz7i_g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.422724</td>\n",
       "      <td>0.911473</td>\n",
       "      <td>1.672431</td>\n",
       "      <td>0.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.663276</td>\n",
       "      <td>0.922539</td>\n",
       "      <td>0.278884</td>\n",
       "      <td>0.918867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.290627</td>\n",
       "      <td>0.918205</td>\n",
       "      <td>0.204584</td>\n",
       "      <td>0.918333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.268728</td>\n",
       "      <td>0.918339</td>\n",
       "      <td>0.250520</td>\n",
       "      <td>0.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.263361</td>\n",
       "      <td>0.918472</td>\n",
       "      <td>0.352974</td>\n",
       "      <td>0.913867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.251369</td>\n",
       "      <td>0.919072</td>\n",
       "      <td>0.425576</td>\n",
       "      <td>0.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.248788</td>\n",
       "      <td>0.919272</td>\n",
       "      <td>0.213888</td>\n",
       "      <td>0.919467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.255782</td>\n",
       "      <td>0.919672</td>\n",
       "      <td>0.369371</td>\n",
       "      <td>0.925800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.293946</td>\n",
       "      <td>0.919672</td>\n",
       "      <td>0.233862</td>\n",
       "      <td>0.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.303983</td>\n",
       "      <td>0.919205</td>\n",
       "      <td>0.203845</td>\n",
       "      <td>0.918867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  accuracy  val_loss  val_accuracy\n",
       "epoch                                             \n",
       "1      10.422724  0.911473  1.672431      0.931400\n",
       "2       0.663276  0.922539  0.278884      0.918867\n",
       "3       0.290627  0.918205  0.204584      0.918333\n",
       "4       0.268728  0.918339  0.250520      0.917400\n",
       "5       0.263361  0.918472  0.352974      0.913867\n",
       "6       0.251369  0.919072  0.425576      0.920600\n",
       "7       0.248788  0.919272  0.213888      0.919467\n",
       "8       0.255782  0.919672  0.369371      0.925800\n",
       "9       0.293946  0.919672  0.233862      0.918400\n",
       "10      0.303983  0.919205  0.203845      0.918867"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the neural network history object into a data frame to view its specifics\n",
    "hist = pd.DataFrame(nn1_history.history)\n",
    "hist['epoch'] = nn1_history.epoch\n",
    "hist['epoch'] = hist['epoch'].apply(lambda x: x + 1)\n",
    "hist.set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "OU3nlVwg7zyq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "9n2ZYiZ47sS-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2038 - accuracy: 0.9189\n",
      "The loss value of the model on the validation data is 0.20384520292282104\n",
      "The accuracy of the model on the validation data is 0.9188666939735413\n"
     ]
    }
   ],
   "source": [
    "# Compute the final accuracy of the model on the validation data set using the 'evaluate()' method\n",
    "performance_test = nn1.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the validation data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the validation data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "4SMFaeIuBhDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 22.5089 - accuracy: 0.9012\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 9.8799 - accuracy: 0.9152\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 6.1042 - accuracy: 0.9309\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 3.4716 - accuracy: 0.9325\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 2.4027 - accuracy: 0.9323\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 1.1017 - accuracy: 0.9323\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.4916 - accuracy: 0.9235\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.3572 - accuracy: 0.9204\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.3058 - accuracy: 0.9183\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.3277 - accuracy: 0.9253\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.3741 - accuracy: 0.9212\n",
      "[CV 1/2] END activation_function=relu, hidden1_neurons=256;, score=0.921 total time=  10.7s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_13'), name='input_13', description=\"created by layer 'input_13'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_13'), name='input_13', description=\"created by layer 'input_13'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 3ms/step - loss: 20.3360 - accuracy: 0.8976\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 8.7494 - accuracy: 0.9137\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 4.0009 - accuracy: 0.9307\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 2.2873 - accuracy: 0.9317\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6986 - accuracy: 0.9303\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2998 - accuracy: 0.9197\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2803 - accuracy: 0.9208\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2804 - accuracy: 0.9183\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2480 - accuracy: 0.9193\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2603 - accuracy: 0.9205\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_13'), name='input_13', description=\"created by layer 'input_13'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.3381 - accuracy: 0.9181\n",
      "[CV 2/2] END activation_function=relu, hidden1_neurons=256;, score=0.918 total time=  11.7s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 8.1461 - accuracy: 0.9121\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.3949 - accuracy: 0.9189\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3173 - accuracy: 0.9179\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4145 - accuracy: 0.9205\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2899 - accuracy: 0.9184\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.3890 - accuracy: 0.9173\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.2815 - accuracy: 0.9176\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.2601 - accuracy: 0.9181\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.2534 - accuracy: 0.9163\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2997 - accuracy: 0.9184\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.9192\n",
      "[CV 1/2] END activation_function=relu, hidden1_neurons=512;, score=0.919 total time=  16.3s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_15'), name='input_15', description=\"created by layer 'input_15'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_15'), name='input_15', description=\"created by layer 'input_15'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 13.8315 - accuracy: 0.9005\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5116 - accuracy: 0.9212\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.2597 - accuracy: 0.9188\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.9191\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.3076 - accuracy: 0.9188\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3589 - accuracy: 0.9189\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.9189\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.9185\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.9191\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.9189\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_15'), name='input_15', description=\"created by layer 'input_15'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.9183\n",
      "[CV 2/2] END activation_function=relu, hidden1_neurons=512;, score=0.918 total time=  22.5s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 4ms/step - loss: 0.2061 - accuracy: 0.9331\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1924 - accuracy: 0.9355\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1936 - accuracy: 0.9345\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1913 - accuracy: 0.9361\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1920 - accuracy: 0.9365\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1900 - accuracy: 0.9367\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1910 - accuracy: 0.9363\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1926 - accuracy: 0.9373\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1919 - accuracy: 0.9373\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1913 - accuracy: 0.9384\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1922 - accuracy: 0.9357\n",
      "[CV 1/2] END activation_function=sigmoid, hidden1_neurons=256;, score=0.936 total time=  12.9s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 5s 4ms/step - loss: 0.1993 - accuracy: 0.9305\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1834 - accuracy: 0.9369\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1834 - accuracy: 0.9352\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1837 - accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1845 - accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1853 - accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1841 - accuracy: 0.9379\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1850 - accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1842 - accuracy: 0.9377\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1855 - accuracy: 0.9373\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_17'), name='input_17', description=\"created by layer 'input_17'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1882 - accuracy: 0.9375\n",
      "[CV 2/2] END activation_function=sigmoid, hidden1_neurons=256;, score=0.937 total time=  14.2s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_18'), name='input_18', description=\"created by layer 'input_18'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_18'), name='input_18', description=\"created by layer 'input_18'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.2014 - accuracy: 0.9303\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1909 - accuracy: 0.9353\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1901 - accuracy: 0.9367\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1897 - accuracy: 0.9348\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1910 - accuracy: 0.9368\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.1911 - accuracy: 0.9369\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1916 - accuracy: 0.9371\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1897 - accuracy: 0.9380\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1893 - accuracy: 0.9353\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1884 - accuracy: 0.9380\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_18'), name='input_18', description=\"created by layer 'input_18'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1876 - accuracy: 0.9387\n",
      "[CV 1/2] END activation_function=sigmoid, hidden1_neurons=512;, score=0.939 total time=  22.8s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_19'), name='input_19', description=\"created by layer 'input_19'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_19'), name='input_19', description=\"created by layer 'input_19'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1929 - accuracy: 0.9327\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1824 - accuracy: 0.9347\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1822 - accuracy: 0.9345\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1837 - accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1836 - accuracy: 0.9375\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1819 - accuracy: 0.9377\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1799 - accuracy: 0.9369\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1797 - accuracy: 0.9387\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1812 - accuracy: 0.9376\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1841 - accuracy: 0.9373\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_19'), name='input_19', description=\"created by layer 'input_19'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1920 - accuracy: 0.9357\n",
      "[CV 2/2] END activation_function=sigmoid, hidden1_neurons=512;, score=0.936 total time=  23.2s\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_20'), name='input_20', description=\"created by layer 'input_20'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_20'), name='input_20', description=\"created by layer 'input_20'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 0.1951 - accuracy: 0.9329\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1868 - accuracy: 0.9363\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1880 - accuracy: 0.9364\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1893 - accuracy: 0.9370\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1888 - accuracy: 0.9388\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1890 - accuracy: 0.9372\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1904 - accuracy: 0.9385\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1904 - accuracy: 0.9378\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1884 - accuracy: 0.9385\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1878 - accuracy: 0.9392\n",
      "\n",
      " The optimal value of convolution filter size is sigmoid\n",
      "\n",
      " The optimal value of maxpooling filter size is 512\n",
      "\n",
      " The accuracy of the model with these optimal parameters is  0.9372040927410126\n"
     ]
    }
   ],
   "source": [
    "# Initialize a basic NN object using the 'KerasClassifier()' method\n",
    "# Note: Set the 'build_fn' parameter to 'create_nn' - This converts the 'create_nn' function into a 'KerasClassifier' object\n",
    "base_grid_model = KerasClassifier(build_fn = create_nn)\n",
    "\n",
    "# Define a list of 'activation_function' and 'hidden1_neurons' parameters and store it in a parameter grid dictionary\n",
    "parameters_grid = {'activation_function': ['relu','sigmoid'],\n",
    "                   'hidden1_neurons': [256, 512]}\n",
    "\n",
    "# Perform a grid search using the 'GridSearchCV()' method to obtain a grid on which to fit the training data\n",
    "# Note: Set the 'estimator' parameter to 'base_grid_model' - This specifies the estimator to be used by 'GridSearchCV()'\n",
    "# Note: Set the 'param_grid' parameter to 'parameters_grid' - This specifies the grid of parameters to search over\n",
    "# Note: Set the 'cv' parameter to 2 - This specifies the number of folds in the cross-validation process\n",
    "# Note: Set the 'verbose' parameter to 4 - This helps show more relevant information during training\n",
    "grid = GridSearchCV(estimator = base_grid_model,\n",
    "                    param_grid = parameters_grid,\n",
    "                    cv = 2,\n",
    "                    verbose = 4)\n",
    "\n",
    "# Train the model on the training data using the 'fit()' method\n",
    "# Note: Use the default batch size or set it to 32\n",
    "# Note: Set the 'epochs' parameter to 10\n",
    "# Note: The 'validation_split' parameter isn't particularly required since cross-validation is already in place\n",
    "grid_model = grid.fit(X_train, y_train, epochs = 10)\n",
    "\n",
    "# Print the optimal values of 'activation_function' and 'hidden1_neurons'\n",
    "best_activation_function = grid_model.best_params_['activation_function']\n",
    "best_hidden1_neurons = grid_model.best_params_['hidden1_neurons']\n",
    "best_accuracy = grid_model.best_score_\n",
    "\n",
    "print('\\n The optimal value of convolution filter size is', best_activation_function)\n",
    "print('\\n The optimal value of maxpooling filter size is', best_hidden1_neurons)\n",
    "print('\\n The accuracy of the model with these optimal parameters is ', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "838332c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_78 (Dense)            (None, None, 512)         91648     \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, None, 64)          32832     \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, None, 1)           65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,545\n",
      "Trainable params: 124,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9335WARNING:tensorflow:Model was constructed with shape (None, None, 178) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 178), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None, 178).\n",
      "469/469 [==============================] - 4s 6ms/step - loss: 0.1957 - accuracy: 0.9333 - val_loss: 0.1888 - val_accuracy: 0.9305\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1890 - accuracy: 0.9346 - val_loss: 0.1905 - val_accuracy: 0.9341\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1908 - accuracy: 0.9358 - val_loss: 0.1882 - val_accuracy: 0.9355\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1864 - accuracy: 0.9366 - val_loss: 0.2004 - val_accuracy: 0.9356\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1889 - accuracy: 0.9375 - val_loss: 0.1895 - val_accuracy: 0.9362\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1897 - accuracy: 0.9373 - val_loss: 0.1939 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1868 - accuracy: 0.9365 - val_loss: 0.2061 - val_accuracy: 0.9308\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1898 - accuracy: 0.9367 - val_loss: 0.1958 - val_accuracy: 0.9350\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1932 - accuracy: 0.9375 - val_loss: 0.1967 - val_accuracy: 0.9353\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1902 - accuracy: 0.9388 - val_loss: 0.1996 - val_accuracy: 0.9324\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with the optimal combination of hyperparameters and save its training history\n",
    "\n",
    "# Use the 'create_nn' function to create a NN with the optimal values of 'filter_size' and 'pool_filter_size'\n",
    "# Note: Set the 'activation_function' parameter to 'best_activation_function' - This specifies the optimal value for the 'activation_function' parameter\n",
    "# Note: Set the 'hidden1_neurons' parameter to 'best_hidden1_neurons' - This specifies the optimal value for the 'hidden1_neurons' parameter\n",
    "nn1 = create_nn(activation_function = best_activation_function, hidden1_neurons = best_hidden1_neurons)\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_data' parameter to (X_val, y_val)\n",
    "# Note: Use the default batch size or set it to 32\n",
    "# Note: Set the 'epochs' parameter to 10\n",
    "nn1.summary()\n",
    "print('\\n')\n",
    "nn1_history = nn1.fit(X_train, y_train,  validation_data =  (X_test, y_test), epochs = 10)\n",
    "hist = pd.DataFrame(nn1_history.history)\n",
    "hist['epoch'] = nn1_history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "776c3172"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "0565ad1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1996 - accuracy: 0.9324\n",
      "The loss value of the model on the test data is 0.199594184756279\n",
      "The accuracy of the model on the test data is 0.9323999881744385\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = nn1.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juV5XGj_d81q"
   },
   "source": [
    "## Business Insights: Misclassification Costs\n",
    "Our first step is to understand the current profitability of the telecomminucation service program, and then to is to estimate the impact of our model. We are going to use misclassification costs to study the impact. \n",
    "\n",
    "We are going to use \\$500 as an approximation company loss for the false negative cost, and \\$300 company loss for the false positive cost. Note: We are interested in finding the best cut-off that will maximize the benefit of our machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "shUswx0DDI5n"
   },
   "outputs": [],
   "source": [
    "# Define the false positive and false negative missclassification cost here\n",
    "\n",
    "fn_cost = 500\n",
    "fp_cost = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8L6GR6vDJNy"
   },
   "source": [
    "#### We will use the optimal model and its corresponding data set that was implemented in the GridSearchCV section. Let's first see the performance metrics of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "fgrZKCfs-8WO"
   },
   "outputs": [],
   "source": [
    "# Use the most optimal machine learning model that you obtained from the GridSearchCV section and the corresponding data set you used (normal, RUS or ROS)\n",
    "\n",
    "model_name = 'Decision Tree - Random Oversampling'\n",
    "tree_model_best = DecisionTreeClassifier(max_depth = best_max_depth, random_state = 0)\n",
    "tree_model_best = tree_model_best.fit(X_train_ros, y_train_ros)\n",
    "train_y_pred = tree_model_best.predict(X_train_ros)\n",
    "test_y_pred = tree_model_best.predict(X_test_ros)\n",
    "\n",
    "# Evaluating the accuracy of the training and validation sets\n",
    "\n",
    "train_acc = accuracy_score(y_train_ros, train_y_pred)\n",
    "val_acc = accuracy_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# Calculate the F1 score, Precision and Recall on the validation set\n",
    "\n",
    "f_score = f1_score(y_test_ros, test_y_pred)\n",
    "precision = precision_score(y_test_ros, test_y_pred)\n",
    "recall = recall_score(y_test_ros, test_y_pred)\n",
    "\n",
    "# creating a dataframe to compare the performance of different models\n",
    "new_model_eval_data = [[model_name, train_acc, val_acc, f_score, precision, recall]]\n",
    "new_evaluate_df = pd.DataFrame(new_model_eval_data, columns=['Model Name', 'Training Score', 'Testing Score',\n",
    "                                          'F1 Score', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "IXgdIhgRCRgR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree - Random Oversampling</td>\n",
       "      <td>0.991655</td>\n",
       "      <td>0.969158</td>\n",
       "      <td>0.970081</td>\n",
       "      <td>0.9419</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Name  Training Score  Testing Score  \\\n",
       "0  Decision Tree - Random Oversampling        0.991655       0.969158   \n",
       "\n",
       "   F1 Score  Precision  Recall  \n",
       "0  0.970081     0.9419     1.0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_evaluate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5RZ7kZgCd5L"
   },
   "source": [
    "#### We now calculate the current misclassification cost in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "xZybKG6aCh0T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of False Positives: 170\n",
      "Number of False Negatives: 0\n",
      "Prediction Misclassification Cost: 51000.00\n"
     ]
    }
   ],
   "source": [
    "# Obtain the count of false positive and false negative classifications from your model\n",
    "\n",
    "fp_count = cf[0,1]\n",
    "fn_count = cf[1,0]\n",
    "# Calculate the total misclassification cost using the FN and FP cost and FN and FP count\n",
    "\n",
    "misclassification_cost = fp_count * fp_cost + fn_count * fn_cost\n",
    "\n",
    "print('Number of False Positives: %d' % fp_count)\n",
    "print('Number of False Negatives: %d' % fn_count)\n",
    "print('Prediction Misclassification Cost: %.2f' % misclassification_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au5XGxGbDdVC"
   },
   "source": [
    "#### We now calculate the misclassification cost as we raise the cut-off value from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "jdzrfvyiDt1E"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for the training set and retain them for only positive outcomes\n",
    "lr_probs_train =tree_model_best.predict_proba(X_train_ros)[:, 1]\n",
    "\n",
    "# Predict probabilities for the validation set and retain them for only positive outcomes\n",
    "lr_probs_val = tree_model_best.predict_proba(X_test_ros)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "EOx1ry1MDfbN"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAERCAYAAAB2CKBkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjUElEQVR4nO3deZScdZ3v8fcnnc5CNkISDCSEJE5CWEIWOoAsTqI4F9ABiUHIoBDxgiyKwHVAuCrMeDwzo4wXkYHciIB4kIgKDHADjCAYVJYshCUEJGIcm7CETkh3tl6/94+qbppOp1NJ6nmqup/P65w6XfWs318H6tu/9VFEYGZm2dWr1AGYmVlpORGYmWWcE4GZWcY5EZiZZZwTgZlZxjkRmJllXLdMBJJulfSOpJcKPP6zkl6WtFLSz5KOz8ysO1F3nEcg6aPAJuCOiDhsJ8dOAO4GPhYRGyTtGxHvpBGnmVl30C1rBBGxGFjffpukD0t6WNIySU9KmpTfdR7wHxGxIX+uk4CZWTvdMhHswALgKxFxBPA14Kb89onAREm/l/S0pBNLFqGZWRnqXeoAikHSQOAY4BeSWjf3zf/sDUwAZgKjgSclHRYR76UcpplZWeoRiYBczea9iJjayb5q4OmIaAT+LOlVcolhSYrxmZmVrR7RNBQRteS+5E8HUM6U/O77gFn57cPJNRW9Xoo4zczKUWKJQNIBkh6XtCo/bPOrnRwzU9JGSSvyr28VeO27gKeAgyRVS/oicBbwRUnPAyuBU/OHPwLUSHoZeBz4x4ioKUYZzcx6gsSGj0raD9gvIpZLGgQsAz4dES+3O2Ym8LWI+FQiQZiZ2U4lViOIiDcjYnn+fR2wChiV1P3MzGz3pNJZLGksMA14ppPdH8k356wlVztY2dW1hg8fHmPHji16jGZmPdmyZcvejYgRne1LPBHkh3b+Crg036nb3nLgwIjYJOlkch27Ezq5xvnA+QBjxoxh6dKlyQZtZtbDSPrLjvYlOmpIUiW5JHBnRNzTcX9E1EbEpvz7RUBlfmRPx+MWRERVRFSNGNFpQjMzs92U5KghAT8GVkXE93dwzMj8cUg6Mh+PR/SYmaUoyaahY4HPAy9KWpHfdjUwBiAi5gNzgAslNQFbgTOjO66CZ2bWjSWWCCLid4B2csyNwI17eq/Gxkaqq6vZtm3bnl7K8vr168fo0aOprKwsdShmlrAescREdXU1gwYNYuzYsbRba8h2U0RQU1NDdXU148aNK3U4ZpawHrHExLZt2xg2bJiTQJFIYtiwYa5hmWVEj0gEgJNAkfn3aZYdPSYRmJn1ZNc/+kd+v/rdRK7tRFAENTU1TJ06lalTpzJy5EhGjRrV9rmhoaHLc5cuXcoll1yy03scc8wxxQrXzLqZjVsb+cFjr7F0zYZErt8jOotLbdiwYaxYsQKAa6+9loEDB/K1r32tbX9TUxO9e3f+q66qqqKqqmqn9/jDH/5QlFjNrPtZ/pcNRMCMcUMTub5rBAmZN28el19+ObNmzeLKK6/k2Wef5ZhjjmHatGkcc8wxvPrqqwA88cQTfOpTucVXr732Ws4991xmzpzJ+PHjueGGG9quN3DgwLbjZ86cyZw5c5g0aRJnnXUWrVMvFi1axKRJkzjuuOO45JJL2q5rZt3bs2vWU1khph2QTCLocTWCf3pgJS+v7bik0Z45ZP/BXPP3h+7yeX/84x959NFHqaiooLa2lsWLF9O7d28effRRrr76an71q19td84rr7zC448/Tl1dHQcddBAXXnjhdmP5n3vuOVauXMn+++/Psccey+9//3uqqqr40pe+xOLFixk3bhxz587d7fKaWXlZ8uf1HDZqCP37VCRy/R6XCMrJ6aefTkVF7h9u48aNnHPOObz22mtIorGxsdNzPvnJT9K3b1/69u3Lvvvuy9tvv83o0aM/cMyRRx7Ztm3q1KmsWbOGgQMHMn78+LZx/3PnzmXBggUJls7M0rCtsZkXqjcy79ixid2jxyWC3fnLPSkDBgxoe//Nb36TWbNmce+997JmzRpmzpzZ6Tl9+/Zte19RUUFTU1NBx3hlDrOe6YXqjTQ0tzBj7D6J3cN9BCnZuHEjo0blnstz++23F/36kyZN4vXXX2fNmjUA/PznPy/6PcwsfUvWrAeg6sBk+gfAiSA1V1xxBVdddRXHHnsszc3NRb9+//79uemmmzjxxBM57rjj+NCHPsSQIUOKfh8zS9ezf17PxA8NZOiAPondI7FnFielqqoqOj6YZtWqVRx88MEliqh8bNq0iYEDBxIRXHzxxUyYMIHLLrtst6/n36tZaTW3BFP/6b84Zer+fOe0yXt0LUnLIqLTsequEfQgP/rRj5g6dSqHHnooGzdu5Etf+lKpQzKzPbDqzVrq6psS7R+AHthZnGWXXXbZHtUAzKy8LM33D8wYl2wicI3AzKxMLVmzgVF792fU3v0TvY8TgZlZGYoInl2znhljkxst1MqJwMysDG3Y0si6unoOG5X86D8nAjOzMrSurh6AkUP6JX4vJ4IimDlzJo888sgHtl1//fVcdNFFOzy+dQjsySefzHvvvbfdMddeey3XXXddl/edP38+d9xxB5CbpLZ27drdiN7MylFrIhgxsO9OjtxzTgRFMHfuXBYuXPiBbQsXLixo4bdFixax995779Z9L7jgAs4++2zAicCsp1m3Kfeo2BGDnAi6hTlz5vDggw9SX5/L4GvWrGHt2rX87Gc/o6qqikMPPZRrrrmm03PHjh3Lu+/mnjr0ne98h4MOOogTTjihbZlqyM0PmDFjBlOmTOEzn/kMW7ZsAd6vNfzyl79k6dKlnHXWWUydOpWtW7fy2GOPMW3aNCZPnsy5557bFtvYsWO55pprmD59OpMnT+aVV15J8ldjZruprUaQQiLoefMIHvo6vPVica85cjKc9K873D1s2DCOPPJIHn74YU499VQWLlzIGWecwVVXXcU+++xDc3MzH//4x3nhhRc4/PDDO73GsmXLWLhwIc899xxNTU1Mnz6dI444AoDZs2dz3nnnAfCNb3yDH//4x3zlK19pO3fOnDnceOONXHfddVRVVbFt2zbmzZvHY489xsSJEzn77LO5+eabufTSSwEYPnw4y5cv56abbuK6667jlltuKdIvysyKZV1dPf0qezGwb/Jf064RFEn75qHWZqG7776b6dOnM23aNFauXMnLL7+8w/OffPJJTjvtNPbaay8GDx7MKaec0rbvpZde4vjjj2fy5MnceeedrFy5sstYXn31VcaNG8fEiRMBOOecc1i8eHHb/tmzZwNwxBFHtC1SZ2blZV1dPSMG9UVS4vfqeTWCLv5yT9KnP/1pLr/8cpYvX87WrVsZOnQo1113HUuWLGHo0KHMmzePbdu2dXmNHf2Dz5s3j/vuu48pU6Zw++2388QTT3R5nZ2tH9W6jPWOlrk2s9Jbt6k+lY5icI2gaAYOHMjMmTM599xzmTt3LrW1tQwYMIAhQ4bw9ttv89BDD3V5/kc/+lHuvfdetm7dSl1dHQ888EDbvrq6Ovbbbz8aGxu58847Oz1/0KBB1NXVAbklqdesWcPq1asB+OlPf8rf/u3fFqmkZpaG1hpBGnpejaCE5s6dy+zZs1m4cCGTJk1i2rRpHHrooYwfP55jjz22y3OnT5/OGWecwdSpUznwwAM5/vjj2/Z9+9vf5qijjuLAAw9k8uTJbV/47c2bN48LLriA/v3789RTT3Hbbbdx+umn09TUxIwZM7jggguKXl4zS866unqOTHiNoVZehtp2yL9Xs9JoaGph4jce4rITJvLVEyYU5ZpehtrMrBup2Zze0FFwIjAzKztpziGAHpQIulsTV7nz79OsdJwIdkO/fv2oqanxl1eRRAQ1NTX065f8Yldmtr20E0GPGDU0evRoqqurWbduXalD6TH69evH6NGjSx2GWSa1JoLhA5N7YH17PSIRVFZWMm7cuFKHYWZWFOs21TOkfyV9e1ekcr/EmoYkHSDpcUmrJK2U9NVOjpGkGyStlvSCpOlJxWNm1l2kOZkMkq0RNAH/KyKWSxoELJP064hov+DOScCE/Oso4Ob8TzOzzFpXl97yEpBgjSAi3oyI5fn3dcAqYFSHw04F7oicp4G9Je2XVExmZt3Buk3p1ghSGTUkaSwwDXimw65RwF/bfa5m+2SBpPMlLZW01B3CZtbTpd00lHgikDQQ+BVwaUTUdtzdySnbjQGNiAURURURVSNGjEgiTDOzsrC5voktDc09JxFIqiSXBO6MiHs6OaQaOKDd59GAn7doZpmV5rOKWyU5akjAj4FVEfH9HRx2P3B2fvTQ0cDGiHgzqZjMzMrduk3pTiaDZEcNHQt8HnhR0or8tquBMQARMR9YBJwMrAa2AF9IMB4zs7KX9qxiSDARRMTv6LwPoP0xAVycVAxmZt1NKRJBj1hryMysp1hXV09FLzF0r3SWlwAnAjOzsrKurp5hA/pQ0Sv5h9a3ciIwMysjaU8mAycCM7OykvZkMnAiMDMrK2mvMwROBGZmZaOlJXjXTUNmZtn13tZGmlrCicDMLKvqtjUCMKhfZar33WkikPTTQraZmdmeaWhqAaBP73T/Ri/kboe2/yCpAjgimXDMzLKrvjURVJRJIpB0laQ64HBJtflXHfAO8J+pRWhmlhENzblE0LdcagQR8S8RMQj4XkQMzr8GRcSwiLgqxRjNzDKhnJuGHpQ0AEDS5yR9X9KBCcdlZpY55ZwIbga2SJoCXAH8Bbgj0ajMzDKoodz6CNppyi8XfSrwg4j4ATAo2bDMzLKntY8g7RpBIc8jqJN0FbmHzByfHzWU7iBXM7MMKOemoTOAeuDciHgLGAV8L9GozMwyqGybhvJf/ncCQyR9CtgWEe4jMDMrsvpyGz7aStJngWeB04HPAs9ImpN0YGZmWVOqpqFC+gj+NzAjIt4BkDQCeBT4ZZKBmZllTTn3EfRqTQJ5NQWeZ2Zmu6BUfQSF1AgelvQIcFf+8xnAQ8mFZGaWTQ3NzfQS9C63RBAR/yhpNnAcIGBBRNybeGRmZhnT0NSSerMQdJEIJP0N8KGI+H1E3APck9/+UUkfjog/pRWkmVkWNDS1pN4sBF239V8P1HWyfUt+n5mZFVFDcwt9elekft+uEsHYiHih48aIWAqMTSwiM7OMqm9qSX0OAXSdCPp1sa9/sQMxM8u6UvURdHXHJZLO67hR0heBZcmFZGaWTaXqI+hq1NClwL2SzuL9L/4qoA9wWsJxmZllTq6PoIwSQUS8DRwjaRZwWH7z/4uI36QSmZlZxpTd8NFWEfE48HgKsZiZZVo5Dh81M7MUlappyInAzKxMlOOooT0i6VZJ70h6aQf7Z0raKGlF/vWtpGIxM+sOyjYRSJot6bX8l3atpDpJtQVc+3bgxJ0c82RETM2//rmQgM3Meqr6phb6ltnw0VbfBf4+IlbtyoUjYrGksbsVlZlZBpVzH8Hbu5oEdsFHJD0v6SFJh+7oIEnnS1oqaem6desSCsXMrLTKdvgosFTSz4H7yD3EHoD8iqR7YjlwYERsknRy/voTOjswIhYACwCqqqpiD+9rZlaWynFmcavB5FYc/bt224L8stS7KyJq271fJOkmScMj4t09ua6ZWXdVdjOLW0XEF5K4saSR5JqdQtKR5JqpapK4l5lZuWtuCZpbojwTgaTRwA+BY8nVBH4HfDUiqndy3l3ATGC4pGrgGqASICLmA3OACyU1AVuBMyPCzT5mlkmlenA9FNY0dBvwM+D0/OfP5bd9oquTImLuTvbfCNxYwP3NzHq8Uj24HgobNTQiIm6LiKb863ZgRMJxmZllSn1zM0DZPZim1buSPiepIv/6HG7LNzMrqlI2DRVyx3OBzwJvAW+Sa9s/N8mgzMyypqz7CCLiv4FTUojFzCyzGppb+wjSf3j9DhOBpCsi4ruSfkhutNAHRMQliUZmZpYh5VojaF1WYmkagZiZZVlZJoKIeCD/dktE/KL9Pkmnd3KKmZntpnIfPnpVgdvMzGw31TeXYY1A0knAycAoSTe02zUYaEo6MDOzLGmtEZRiHkFXfQRryfUPnAIsa7e9DrgsyaDMzLKmXPsIngeel/SziGhMMSYzs8wpZR9BIWsNjZX0L8AhQL/WjRExPrGozMwypqGEfQSF3PE24GZy/QKzgDuAnyYZlJlZ1pT7EhP9I+IxQBHxl4i4FvhYsmGZmWVLWfYRtLNNUi/gNUlfBt4A9k02LDOzbHl/iYnyrBFcCuwFXAIcQe55BOckGJOZWebUl3NncUQsyb/dBCTy2Eozs6xraGqhskL06qXU773T1CPp15L2bvd5qKRHEo3KzCxjGppaSlIbgMKahoZHxHutHyJiA+4jMDMrqobm5pJ0FENhiaBF0pjWD5IOpJNlqc3MbPc1NLWULBEUMmrofwO/k/Tb/OePAucnF5KZWfaUdSKIiIclTQeOBgRcFhHvJh6ZmVmGNDSXYR+BpEn5n9OBMeQWoXsDGJPfZmZmRZKrEaT/mEroukZwObkmoH/vZF/g2cVmZkVTX6ZNQ7/O//xiRLyeRjBmZlnV0NRC33JrGuL9p5D9Mo1AzMyyrKG5PGsENZIeB8ZJur/jzog4JbmwzMyypaGphaF7lV8i+CQwndyS0531E5iZWZGUcmZxV08oawCelnRMRKxLMSYzs8wpy6YhSddHxKXArZK2m0nspiEzs+Ip1wllrU8huy6NQMzMsqwsE0FELMv/bF1aAklDgQMi4oUUYjMzy4yyXn1U0hOSBkvaB3geuE3S95MPzcwsO+qbW+hbxquPDomIWmA2cFtEHAGcsLOTJN0q6R1JL+1gvyTdIGm1pBe8bIWZZVVElLRpqJC79pa0H/BZ4MFduPbtwIld7D8JmJB/nQ/cvAvXNjPrMRqbc+NxyrZpCPhn4BFgdUQskTQeeG1nJ0XEYmB9F4ecCtwROU8De+cTjplZprQ9uL7cOotbRcQvgF+0+/w68Jki3HsU8Nd2n6vz294swrXNzLqNhqbSJoJCOou/m+8srpT0mKR3JX2uCPfu7AnNnT75TNL5kpZKWrpunee2mVnPUvaJAPi7fGfxp8j91T4R+Mci3LsaOKDd59HknnmwnYhYEBFVEVE1YsSIItzazKx8tCWCMu4jqMz/PBm4KyK6avffFfcDZ+dHDx0NbIwINwuZWeY0NDcDZdxHADwg6RVgK3CRpBHAtp2dJOkuYCYwXFI1cA35pBIR84FF5JLLamAL8IXdKYCZWXdXn68RlGoeQSGdxV+X9G9AbUQ0S9pMbsTPzs6bu5P9AVxccKRmZj1U2/DRck0EeaOAT0jq127bHQnEY2aWOe/3EZTfM4sBkHQNuSaeQ8g155wE/A4nAjOzougOo4bmAB8H3oqILwBTgL6JRmVmliGl7iwu5K5bI6IFaJI0GHgHGJ9sWGZm2VHq4aOF9BEslbQ38CNgGbAJeDbJoMzMsqS+xE1DhYwauij/dr6kh4HBfh6BmVnxNJTr8NGuloWWND0ilicTkplZtpTzonP/3sW+AD5W5FjMzDKpbPsIImJWmoGYmWVV2Q8flXRxvrO49fNQSRd1cYqZme2Csk8EwHkR8V7rh4jYAJyXWERmZhnT0NyCBL17dbY6f/IKSQS9JLVFJ6kC6JNcSGZm2dLQ1EKfil60+6pNVSHzCB4B7pY0n1wn8QXAw4lGZWaWIfUlfHA9FJYIriT3cPkLyT1V7L+AW5IMyswsSxqaW0o2hwAKm1DWAswnN6FsH2B0RDQnHpmZWUa0Ng2VSiGjhp7IP7N4H2AFcJuk7ycemZlZRjSUuGmokDsPyT+zeDZwW0QcAZyQbFhmZtnRHRJBb0n7AZ8FHkw4HjOzzGloLv9E8M/kRg6tjoglksYDryUblplZdpS6j6CQzuJfAL9o9/l14DNJBmVmliWlbhrqavXRKyLiu5J+SG7+wAdExCWJRmZmlhH1zS0M6VNZsvt3VSNYlf+5NI1AzMyyqmybhiLigfzPn6QXjplZ9jQ0NZfnhDJJ93d1YkScUvxwzMyyp9SjhrpqGvoI8FfgLuAZcstLmJlZkRXUNBQBCS1K19WdRwJXA4cBPwA+AbwbEb+NiN8mEo2ZWQbtdNRQw2a4cw68dE8i99/hnSOiOSIejohzgKOB1cATkr6SSCRmZhnVZSLYugHu+DT86TfQuCWR+3c5j0BSX+CTwFxgLHADkExKMjPLqB32EdS9BT+dDTWvwek/gUOS6ZrtqrP4J+SahR4C/ikiXkokAjOzDGtpCRqbY/s+gsZtcNvJuWTwD3fDh5N7jHxXNYLPA5uBicAl7R9SBkREDE4sKjOzjGho3sHzilfeA+v/lHgSgK7nEZRuLJOZWUa0JoIPzCOIgGf+Lww/CCb8XeIx+MvezKyEGpo6qRG8sQzeXAFHnpfYkNH2nAjMzEqoLRG07yN4dgH0GQRTzkwlBicCM7MS2q5GsOmd3HyBqf8AfQelEkOiiUDSiZJelbRa0tc72T9T0kZJK/KvbyUZj5lZOYgInv/rezz1pxqWrFkPtEsEy34CLY0w43+mFs9On0ewuyRVAP9BbkZyNbBE0v0R8XKHQ5+MiE8lFYeZWblZ9pcNzJn/1Ae2Dd2rDzQ3wdJbYfwsGDExtXgSSwTAkeSeavY6gKSFwKlAx0RgZpYpb7y3FYD/c8YURg7uT/8+FRw+agjU/BHq1sLHv5lqPEkmglHkFq1rVQ0c1clxH5H0PLAW+FpErOx4gKTzgfMBxowZk0CoZmbpqdnUAMDMifsydECf93fUvpH7uXe633NJ9hF0Nuap45POlgMHRsQU4IfAfZ1dKCIWRERVRFSNGDGiuFGamaVs/eYGKnqJIf07PJWs9s3cz8H7pxpPkomgGjig3efR5P7qbxMRtRGxKf9+EVApaXiCMZmZlVzN5gaG7lVJr14d/l6uy39FDtov1XiSTARLgAmSxknqA5wJfOBhN5JGKr92haQj8/HUJBiTmVnJrd9czz7tm4Ra1b4J/YdCZf9U40msjyAimiR9GXgEqABujYiVki7I758PzAEulNQEbAXOjIiOzUdmZj3K+s0NO0gEa2FQus1CkGxncWtzz6IO2+a3e38jcGOSMZiZlZv1mxuYNLKTdTvr1qbePwCeWWxmlrr1mxsYOqBy+x21b8LgdPsHwInAzCxVzS3Be1sb2WdA3w47GmHzupI0DTkRmJmlaMOWBiJgWMc+grq3gHCNwMysp1u/OTeZbLvO4tr80NHBo1KOyInAzCxVrbOKt68RlGYOATgRmJmlqq1GMLBjjaA0s4rBicDMLFXrN9cDnTQN1a2Fir65CWUpcyIwM0vR+s2NQH7Z6fZq83MIUng0ZUdOBGZmKVq/uZ4h/SuprOjw9Vv7ZkmahcCJwMwsVTU7Wl6ibm1JOorBicDMLFWdrjMUUbJZxeBEYGaWqk4TwZb10FxfklnF4ERgZpaqms0NO55D4D4CM7OeLSLY0FmNoIRzCMCJwMwsNbVbm2hqic7nEIA7i83Merqa/GSyYdvNKl4LCAaNTD8onAjMzFKzYUvrgnMdlqCuXQsD94WKTp5RkAInAjOzlLQuOLdPx1nFdW+WrFkInAjMzFLT5YJzJeooBicCM7PU1GzewRLUtW+4RmBmlgXrNzewV58K+lVWvL+xcStse6+kNYLeJbuzmVnGtM0q3rQOHroilwSatuV2OhGYmfV8bbOKX1gIK++BkZMBwQFHwZiPlCwuJwIzs5Ss31zPiIF94eX7YeThcMGTpQ4JcB+BmVlqNmxuZGyfOqh+Fg4+pdThtHGNwMwsJTWb6zm6YWnuw8F/X9pg2nGNwMwsBVsamtjW2MKhG38LwyfCvpNKHVIbJwIzsxTUbGpgKLXsv3F5WdUGwInAzCwV6zc3cELFcnpFc1n1D4D7CMzMdmhLQxP3PbeWNTWb9/hab2zYyuxeS6gfOJq++00pQnTFk5lEsHFrI9UbtpQ6DDPrBpqag4deeou7n/kzldveZWhlA0J7dM0+NHJ8xYu0TDoPtGfXKrbMJIJXfncvQ5+8ttRhmFk30Ac4V5u5Qhvp1S+Ke/HDTyvu9YogM4ngb0bvR+OYw0odhpl1EwOGDKPX8ANyD4vpM6g4F+2/N4w5qjjXKqJEE4GkE4EfABXALRHxrx32K7//ZGALMC8ilicRy7CDj4eDj0/i0mZm3Vpio4YkVQD/AZwEHALMlXRIh8NOAibkX+cDNycVj5mZdS7J4aNHAqsj4vWIaAAWAqd2OOZU4I7IeRrYW1LpFuU2M8ugJBPBKOCv7T5X57ft6jFIOl/SUklL161bV/RAzcyyLMlE0Nn4qI7d74UcQ0QsiIiqiKgaMWJEUYIzM7OcJBNBNXBAu8+jgbW7cYyZmSUoyUSwBJggaZykPsCZwP0djrkfOFs5RwMbI+LNBGMyM7MOEhs+GhFNkr4MPEJu+OitEbFS0gX5/fOBReSGjq4mN3z0C0nFY2ZmnUt0HkFELCL3Zd9+2/x27wO4OMkYzMysa8p9F3cfktYBf9nN04cD7xYxnO4ii+XOYpkhm+XOYplh18t9YER0Otqm2yWCPSFpaURUlTqOtGWx3FksM2Sz3FksMxS33H4egZlZxjkRmJllXNYSwYJSB1AiWSx3FssM2Sx3FssMRSx3pvoIzMxse1mrEZiZWQdOBGZmGdcjE4GkEyW9Kmm1pK93sl+Sbsjvf0HS9FLEWWwFlPusfHlfkPQHSeX1BO3dsLMytztuhqRmSXPSjC8phZRb0kxJKyStlPTbtGMstgL++x4i6QFJz+fL3O1XKpB0q6R3JL20g/3F+S6LiB71IrecxZ+A8eQePfo8cEiHY04GHiK3+unRwDOljjulch8DDM2/P6m7l7uQMrc77jfkZrnPKXXcKf1b7w28DIzJf9631HGnUOargX/Lvx8BrAf6lDr2PSz3R4HpwEs72F+U77KeWCPI6gNxdlruiPhDRGzIf3ya3Gqv3Vkh/9YAXwF+BbyTZnAJKqTc/wDcExH/DRAR3b3shZQ5gEH5R+AOJJcImtINs7giYjG5cuxIUb7LemIiKNoDcbqZXS3TF8n9JdGd7bTMkkYBpwHz6TkK+beeCAyV9ISkZZLOTi26ZBRS5huBg8ktZf8i8NWIaEknvJIpyndZoovOlUjRHojTzRRcJkmzyCWC4xKNKHmFlPl64MqIaM79odgjFFLu3sARwMeB/sBTkp6OiD8mHVxCCinz/wBWAB8DPgz8WtKTEVGbcGylVJTvsp6YCLL6QJyCyiTpcOAW4KSIqEkptqQUUuYqYGE+CQwHTpbUFBH3pRJhMgr9b/zdiNgMbJa0GJgCdNdEUEiZvwD8a+Qaz1dL+jMwCXg2nRBLoijfZT2xaSirD8TZabkljQHuAT7fjf8ybG+nZY6IcRExNiLGAr8ELurmSQAK+2/8P4HjJfWWtBdwFLAq5TiLqZAy/ze5GhCSPgQcBLyeapTpK8p3WY+rEURGH4hTYLm/BQwDbsr/hdwU3XjVxgLL3OMUUu6IWCXpYeAFoAW4JSI6HYLYHRT4b/1t4HZJL5JrMrkyIrr18tSS7gJmAsMlVQPXAJVQ3O8yLzFhZpZxPbFpyMzMdoETgZlZxjkRmJllnBOBmVnGORGYmWWcE4FZO5JGSloo6U+SXpa0SNLELo6/ND9Of1fv8738CpnfkzRC0jOSnpN0/J6VwGzXefioWV5+sbI/AD9pnYMgaSowKCKe3ME5a4CqXR2vLqkWGBER9ZLOJDfT+5w9id9sd7lGYPa+WUBj+4loEbECqJD0YOs2STdKmifpEmB/4HFJj3e8WH625/ckvSTpRUln5LffDwwAnpF0JfBdcktfrJDUP9ESmnWix80sNtsDhwHLCj04Im6QdDkwawc1gtnAVHJr/AwHlkhaHBGnSNoUEVMBJL1Nrlbx5T0tgNnucI3ALDnHAXdFRHNEvA38FphR4pjMtuNEYPa+leSWbu6oiQ/+v9Kvs5MlnZZv3lkhqYrOlwg2KztOBGbv+w3QV9J5rRskzSC3yNkhkvpKGkJ+hcu8OmAQQETcGxFT86+lwGLgDEkVkkaQe+xgT14S2bopJwKzvPw69qcBn8gPH10JXEtuffe7ya3keSfwXLvTFgAPddZZDNybP+d5cknmioh4K7kSmO0eDx81M8s41wjMzDLOicDMLOOcCMzMMs6JwMws45wIzMwyzonAzCzjnAjMzDLu/wPqxzzto0sATAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Misclassification Cost on the training is 55200.00 at Cut-off 0.000\n",
      "Applying that cut-off to the validation data results in Misclassification Cost of 51000.00 \n"
     ]
    }
   ],
   "source": [
    "# Calculate and store the misclassification costs for different values of cut-off probability\n",
    "cost_train = []\n",
    "cost_val=[]\n",
    "\n",
    "for cutoff in np.arange(0, 1, 0.01):\n",
    "    # Get the classification predictions using the probabilities obtained for the training data set and the cutoff\n",
    "    # Get the false positive and false negative count from the predictions\n",
    "    # Calculate the training misclassification cost and append it to the cost_train array\n",
    "    curr_preds = np.where(lr_probs_train > cutoff, 1, 0)\n",
    "    curr_cf = confusion_matrix(y_train_ros, curr_preds)\n",
    "    curr_fp_count = curr_cf[0,1]\n",
    "    curr_fn_count = curr_cf[1,0]\n",
    "\n",
    "    curr_misclassification_cost = curr_fp_count * fp_cost + curr_fn_count * fn_cost\n",
    "    cost_train.append(curr_misclassification_cost)\n",
    "\n",
    "    # Get the classification predictions using the probabilities obtained for the validation data set and the cutoff\n",
    "    # Get the false positive and false negative count from the predictions\n",
    "    # Calculate the training misclassification cost and append it to the cost_val array\n",
    "    curr_preds = np.where(lr_probs_val > cutoff, 1, 0)\n",
    "    curr_cf = confusion_matrix(y_test_ros, curr_preds)\n",
    "    curr_fp_count = curr_cf[0,1]\n",
    "    curr_fn_count = curr_cf[1,0]\n",
    "\n",
    "    curr_misclassification_cost = curr_fp_count * fp_cost + curr_fn_count * fn_cost\n",
    "    cost_val.append(curr_misclassification_cost)\n",
    "\n",
    "# Get the X values (cut-off values)\n",
    "cutoffs = np.arange(0, 1, 0.01)\n",
    "\n",
    "# Plot misclassification cost against cut-off value\n",
    "plt.plot(cutoffs,cost_train, label='Training')\n",
    "plt.plot(cutoffs,cost_val, label='Validaiton')\n",
    "plt.xlabel('Cut-off')\n",
    "plt.ylabel('Misclassification Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum misclassification cost and its associated cut-off value based on the training data\n",
    "best_cost = min(cost_train)\n",
    "best_cutoff = cutoffs[cost_train.index(best_cost)]\n",
    "\n",
    "#apply the cut-off value to the validation data\n",
    "best_valcost = cost_val[cost_train.index(best_cost)]\n",
    "\n",
    "\n",
    "print('Best Misclassification Cost on the training is %.2f at Cut-off %.3f' % (best_cost, best_cutoff));\n",
    "print('Applying that cut-off to the validation data results in Misclassification Cost of %.2f ' % best_valcost);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nX4iEYxM14xL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
